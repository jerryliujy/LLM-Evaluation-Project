[
    {
        "title": "What is the difference between the 'COPY' and 'ADD' commands in a Dockerfile?",
        "url": "https://stackoverflow.com/questions/24958140/what-is-the-difference-between-the-copy-and-add-commands-in-a-dockerfile",
        "votes": "3132",
        "views": "1.1m",
        "author": "Steve",
        "issued_at": "2014-07-25 14:31",
        "tags": [
            "docker",
            "dockerfile"
        ],
        "answers": [
            {
                "answer": "You should check the ADD and COPY documentation for a more detailed description of their behaviors, but in a nutshell, the major difference is that ADD can do more than COPY:\nADD allows <src> to be a URL\nReferring to comments below, the ADD documentation states that:\nIf is a local tar archive in a recognized compression format (identity, gzip, bzip2 or xz) then it is unpacked as a directory. Resources from remote URLs are not decompressed.\nNote that the Best practices for writing Dockerfiles suggests using COPY where the magic of ADD is not required. Otherwise, you (since you had to look up this answer) are likely to get surprised someday when you mean to copy keep_this_archive_intact.tar.gz into your container, but instead, you spray the contents onto your filesystem.",
                "upvotes": 2996,
                "answered_by": "icecrime",
                "answered_at": "2014-07-25 14:52"
            },
            {
                "answer": "COPY is\nSame as 'ADD', but without the tar and remote URL handling.\nReference straight from the source code.",
                "upvotes": 890,
                "answered_by": "caike",
                "answered_at": "2014-09-30 16:13"
            },
            {
                "answer": "There is some official documentation on that point: Best Practices for Writing Dockerfiles\nBecause image size matters, using ADD to fetch packages from remote URLs is strongly discouraged; you should use curl or wget instead. That way you can delete the files you no longer need after they've been extracted and you won't have to add another layer in your image.\nRUN mkdir -p /usr/src/things \\\n  && curl -SL http://example.com/big.tar.gz \\\n    | tar -xJC /usr/src/things \\\n  && make -C /usr/src/things all\nFor other items (files, directories) that do not require ADD\u2019s tar auto-extraction capability, you should always use COPY.",
                "upvotes": 171,
                "answered_by": "Victor Laskin",
                "answered_at": "2014-10-02 08:21"
            },
            {
                "answer": "From Docker docs:\nADD or COPY\nAlthough ADD and COPY are functionally similar, generally speaking, COPY is preferred. That\u2019s because it\u2019s more transparent than ADD. COPY only supports the basic copying of local files into the container, while ADD has some features (like local-only tar extraction and remote URL support) that are not immediately obvious. Consequently, the best use for ADD is local tar file auto-extraction into the image, as in ADD rootfs.tar.xz /.\nMore: Best practices for writing Dockerfiles",
                "upvotes": 151,
                "answered_by": "eddd",
                "answered_at": "2015-08-10 15:19"
            }
        ]
    },
    {
        "title": "How do I pass environment variables to Docker containers?",
        "url": "https://stackoverflow.com/questions/30494050/how-do-i-pass-environment-variables-to-docker-containers",
        "votes": "1510",
        "views": "2.3m",
        "author": "AJcodez",
        "issued_at": "2015-05-27 22:17",
        "tags": [
            "docker",
            "environment-variables",
            "dockerfile"
        ],
        "answers": [
            {
                "answer": "You can pass environment variables to your containers with the -e (alias --env) flag.\ndocker run -e xx=yy\nAn example from a startup script:\nsudo docker run -d -t -i -e REDIS_NAMESPACE='staging' \\ \n-e POSTGRES_ENV_POSTGRES_PASSWORD='foo' \\\n-e POSTGRES_ENV_POSTGRES_USER='bar' \\\n-e POSTGRES_ENV_DB_NAME='mysite_staging' \\\n-e POSTGRES_PORT_5432_TCP_ADDR='docker-db-1.hidden.us-east-1.rds.amazonaws.com' \\\n-e SITE_URL='staging.mysite.com' \\\n-p 80:80 \\\n--link redis:redis \\  \n--name container_name dockerhub_id/image_name\nOr, if you don't want to have the value on the command-line where it will be displayed by ps, etc., -e can pull in the value from the current environment if you just give it without the =:\nsudo PASSWORD='foo' docker run  [...] -e PASSWORD [...]\nIf you have many environment variables and especially if they're meant to be secret, you can use an env-file:\n$ docker run --env-file ./env.list ubuntu bash\nThe --env-file flag takes a filename as an argument and expects each line to be in the VAR=VAL format, mimicking the argument passed to --env. Comment lines need only be prefixed with #",
                "upvotes": 2328,
                "answered_by": "errata",
                "answered_at": "2015-05-27 22:25"
            },
            {
                "answer": "You can pass using -e parameters with the docker run .. command as mentioned here and as mentioned by errata.\nHowever, the possible downside of this approach is that your credentials will be displayed in the process listing, where you run it.\nTo make it more secure, you may write your credentials in a configuration file and do docker run with --env-file as mentioned here. Then you can control the access of that configuration file so that others having access to that machine wouldn't see your credentials.",
                "upvotes": 150,
                "answered_by": "Sabin",
                "answered_at": "2015-05-27 22:37"
            },
            {
                "answer": "Use -e or --env value to set environment variables (default []).\nAn example from a startup script:\ndocker run  -e myhost='localhost' -it busybox sh\nIf you want to use multiple environments from the command line then before every environment variable use the -e flag.\nExample:\nsudo docker run -d -t -i -e NAMESPACE='staging' -e PASSWORD='foo' busybox sh\nNote: Make sure put the container name after the environment variable, not before that.\nIf you need to set up many variables, use the --env-file flag\nFor example,\n$ docker run --env-file ./my_env ubuntu bash\nFor any other help, look into the Docker help:\n$ docker run --help\nOfficial documentation: https://docs.docker.com/compose/environment-variables/",
                "upvotes": 114,
                "answered_by": "Vishnu Mishra",
                "answered_at": "2017-01-29 18:05"
            },
            {
                "answer": "If you are using 'docker-compose' as the method to spin up your container(s), there is actually a useful way to pass an environment variable defined on your server to the Docker container.\nIn your docker-compose.yml file, let's say you are spinning up a basic hapi-js container and the code looks like:\nhapi_server:\n  container_name: hapi_server\n  image: node_image\n  expose:\n    - \"3000\"\nLet's say that the local server that your docker project is on has an environment variable named 'NODE_DB_CONNECT' that you want to pass to your hapi-js container, and you want its new name to be 'HAPI_DB_CONNECT'. Then in the docker-compose.yml file, you would pass the local environment variable to the container and rename it like so:\nhapi_server:\n  container_name: hapi_server\n  image: node_image\n  environment:\n    - HAPI_DB_CONNECT=${NODE_DB_CONNECT}\n  expose:\n    - \"3000\"\nI hope this helps you to avoid hard-coding a database connect string in any file in your container!",
                "upvotes": 86,
                "answered_by": "Marquistador",
                "answered_at": "2016-10-21 15:10"
            }
        ]
    },
    {
        "title": "docker push error: denied: requested access to the resource is denied",
        "url": "https://stackoverflow.com/questions/41984399/docker-push-error-denied-requested-access-to-the-resource-is-denied",
        "votes": "848",
        "views": "982k",
        "author": "Keyur Shah",
        "issued_at": "2017-02-01 16:08",
        "tags": [
            "docker",
            "dockerfile"
        ],
        "answers": [
            {
                "answer": "You may need to switch your docker repo to private before docker push.\nThanks to the answer provided by Dean Wu and this comment by ses, before pushing, remember to log out, then log in from the command line to your docker hub account\n# you may need log out first `docker logout` ref. https://stackoverflow.com/a/53835882/248616\ndocker login\nAccording to the docs:\nYou need to include the namespace for Docker Hub to associate it with your account.\nThe namespace is the same as your Docker Hub account name.\nYou need to rename the image to YOUR_DOCKERHUB_NAME/docker-whale.\nSo, this means you have to tag your image before pushing:\ndocker tag firstimage YOUR_DOCKERHUB_NAME/firstimage\nand then you should be able to push it.\ndocker push YOUR_DOCKERHUB_NAME/firstimage",
                "upvotes": 1310,
                "answered_by": "Webert Lima",
                "answered_at": "2017-02-01 16:22"
            },
            {
                "answer": "Here are the steps worked for me:\nLogin to the docker.\ndocker login -u darlin\nTag your image build.\nMy image name here is mylocalimage and by default it has tag latest, My username is darlin as registered with docker cloud, and I created a public repository named dockerhub.\nSo my personal repository becomes now darlin/dockerhub and I want to push my image with tag myfirstimagepush.\nI tagged as below :\ndocker tag mylocalimage:latest darlin/dockerhub:myfirstimagepush\nPushed the image to my personal docker repository as below\ndocker push darlin/dockerhub:myfirstimagepush\nAnd it was successfully pushed to my personal docker repo.",
                "upvotes": 327,
                "answered_by": "Venu S",
                "answered_at": "2017-07-08 02:52"
            },
            {
                "answer": "I got the same issue while taking the docker beginner Course. I solved the issue by doing a:\ndocker login\nbefore the docker push call.",
                "upvotes": 250,
                "answered_by": "Dean Wu",
                "answered_at": "2017-02-22 22:23"
            }
        ]
    },
    {
        "title": "Difference between RUN and CMD in a Dockerfile",
        "url": "https://stackoverflow.com/questions/37461868/difference-between-run-and-cmd-in-a-dockerfile",
        "votes": "716",
        "views": "394k",
        "author": "TakeSoUp",
        "issued_at": "2016-05-26 13:11",
        "tags": [
            "docker",
            "dockerfile",
            "docker-run",
            "docker-cmd"
        ],
        "answers": [
            {
                "answer": "RUN is an image build step, the state of the container after a RUN command will be committed to the container image. A Dockerfile can have many RUN steps that layer on top of one another to build the image.\nCMD is the command the container executes by default when you launch the built image. A Dockerfile will only use the final CMD defined. The CMD can be overridden when starting a container with docker run $image $other_command.\nENTRYPOINT is also closely related to CMD and can modify the way a CMD is interpreted when a container is started from an image.",
                "upvotes": 1051,
                "answered_by": "Matt",
                "answered_at": "2016-05-26 13:25"
            },
            {
                "answer": "RUN - command triggers while we build the docker image.\nCMD - command triggers while we launch the created docker image.",
                "upvotes": 315,
                "answered_by": "Nisal Edu",
                "answered_at": "2017-07-17 06:56"
            },
            {
                "answer": "I found the Docker RUN vs CMD vs ENTRYPOINT article very helpful to understand the difference between them:\nRUN - RUN instruction allows you to install your application and packages required for it. It executes any commands on top of the current image and creates a new layer by committing the results. Often you will find multiple RUN instructions in a Dockerfile.\nCMD - CMD instruction allows you to set a default command, which will be executed only when you run container without specifying a command. If Docker container runs with a command, the default command will be ignored. If Dockerfile has more than one CMD instruction, all but last\nCMD instructions are ignored.",
                "upvotes": 133,
                "answered_by": "faigy langsam",
                "answered_at": "2017-01-01 13:14"
            },
            {
                "answer": "The existing answers cover most of what anyone looking at this question would need. So I'll just cover some niche areas for CMD and RUN.\nCMD: Duplicates are Allowed but Wasteful\nGingerBeer makes an important point: you won't get any errors if you put in more than one CMD - but it's wasteful to do so. I'd like to elaborate with an example:\nFROM busybox\nCMD echo \"Executing CMD\"\nCMD echo \"Executing CMD 2\"\nIf you build this into an image and run a container in this image, then as GingerBeer states, only the last CMD will be heeded. So the output of that container will be:\nExecuting CMD 2\nThe way I think of it is that \"CMD\" is setting a single global variable for the entire image that is being built, so successive \"CMD\" statements simply overwrite any previous writes to that global variable, and in the final image that's built the last one to write wins. Since a Dockerfile executes in order from top to bottom, we know that the bottom-most CMD is the one gets this final \"write\" (metaphorically speaking).\nRUN: Commands May not Execute if Images are Cached\nA subtle point to notice about RUN is that it's treated as a pure function even if there are side-effects, and is thus cached. What this means is that if RUN had some side effects that don't change the resultant image, and that image has already been cached, the RUN won't be executed again and so the side effects won't happen on subsequent builds. For example, take this Dockerfile:\nFROM busybox\nRUN echo \"Just echo while you work\"\nFirst time you run it, you'll get output such as this, with different alphanumeric IDs:\ndocker build -t example/run-echo .\nSending build context to Docker daemon  9.216kB\nStep 1/2 : FROM busybox\n ---> be5888e67be6\nStep 2/2 : RUN echo \"Just echo while you work\"\n ---> Running in ed37d558c505\nJust echo while you work\nRemoving intermediate container ed37d558c505\n ---> 6f46f7a393d8\nSuccessfully built 6f46f7a393d8\nSuccessfully tagged example/run-echo:latest\nNotice that the echo statement was executed in the above. Second time you run it, it uses the cache, and you won't see any echo in the output of the build:\ndocker build -t example/run-echo .\nSending build context to Docker daemon  9.216kB\nStep 1/2 : FROM busybox\n ---> be5888e67be6\nStep 2/2 : RUN echo \"Just echo while you work\"\n ---> Using cache\n ---> 6f46f7a393d8\nSuccessfully built 6f46f7a393d8\nSuccessfully tagged example/run-echo:latest",
                "upvotes": 41,
                "answered_by": "Colm Bhandal",
                "answered_at": "2020-04-19 10:45"
            }
        ]
    },
    {
        "title": "What's the difference between Docker Compose vs. Dockerfile",
        "url": "https://stackoverflow.com/questions/29480099/whats-the-difference-between-docker-compose-vs-dockerfile",
        "votes": "672",
        "views": "353k",
        "author": "Aaron Lelevier",
        "issued_at": "2015-04-06 21:36",
        "tags": [
            "docker",
            "docker-compose",
            "dockerfile",
            "development-environment",
            "boot2docker"
        ],
        "answers": [
            {
                "answer": "Dockerfile\nA Dockerfile is a simple text file that contains the commands a user could call to assemble an image.\nExample, Dockerfile\nFROM ubuntu:latest\nMAINTAINER john doe \n\nRUN apt-get update\nRUN apt-get install -y python python-pip wget\nRUN pip install Flask\n\nADD hello.py /home/hello.py\n\nWORKDIR /home\nDocker Compose\nDocker Compose\nis a tool for defining and running multi-container Docker applications.\ndefine the services that make up your app in docker-compose.yml so they can be run together in an isolated environment.\nget an app running in one command by just running docker-compose up\nExample, docker-compose.yml\nversion: \"3\"\nservices:\n  web:\n    build: .\n    ports:\n    - '5000:5000'\n    volumes:\n    - .:/code\n    - logvolume01:/var/log\n    links:\n    - redis\n  redis:\n    image: redis\n    volumes:\n      logvolume01: {}",
                "upvotes": 804,
                "answered_by": "code-8",
                "answered_at": "2017-08-07 14:28"
            },
            {
                "answer": "docker-compose exists to keep you from having to write a ton of commands you would have to with docker-cli.\ndocker-compose also makes it easy to startup multiple containers at the same time and automatically connect them together with some form of networking.\nThe purpose of docker-compose is to function as docker cli but to issue multiple commands much more quickly.\nTo make use of docker-compose, you need to encode the commands you were running before into a docker-compose.yml file.\nYou are not just going to copy paste them into the YAML file, there is a special syntax.\nOnce created, you have to feed it to the docker-compose cli and it will be up to the cli to parse the file and create all the different containers with the correct configuration we specify.\nSo you will have separate containers, let's say, one is redis-server and the second one is node-app, and you want that created using the Dockerfile in your current directory.\nAdditionally, after making that container, you would map some port from the container to the local machine to access everything running inside of it.\nSo for your docker-compose.yml file, you would want to start the first line like so:\nversion: '3'\nThat tells Docker the version of docker-compose you want to use. After that, you have to add:\nversion: '3'\nservices: \n  redis-server: \n    image: 'redis'\n  node-app:\n    build: .\nPlease notice the indentation, very important. Also, notice for one service I am grabbing an image, but for another service I am telling docker-compose to look inside the current directory to build the image that will be used for the second container.\nThen you want to specify all the different ports that you want open on this container.\nversion: '3'\nservices: \n  redis-server: \n    image: 'redis'\n  node-app:\n    build: .\n    ports:\n      -\nPlease notice the dash (a dash in a YAML file is how we specify an array). In this example, I am mapping 8081 on my local machine to 8081 on the container like so:\nversion: '3'\nservices: \n  redis-server: \n    image: 'redis'\n  node-app:\n    build: .\n    ports:\n      - \"8081:8081\"\nSo the first port is your local machine, and the other is the port on the container, you could also distinguish between the two to avoid confusion like so:\nversion: '3'\nservices:\n  redis-server:\n    image: 'redis'\n  node-app:\n    build: .\n    ports:\n      - \"4001:8081\"\nBy developing your docker-compose.yml file like this, it will create these containers on essentially the same network and they will have free access to communicate with each other any way they please and exchange as much information as they want.\nWhen the two containers are created using docker-compose, we do not need any port declarations.\nNow in my example, we need to do some code configuration in the Node.js app that looks something like this:\nconst express = require('express');\nconst redis = require('redis');\n\nconst app = express();\nconst client = redis.createClient({\n  host: 'redis-server'\n});\nI use this example above to make you aware that there may be some specific configuration you would have to do in addition to the docker-compose.yml file that may be specific to your project.\nNow, if you ever find yourself working with a Node.js app and redis, you want to ensure you are aware of the default port Node.js uses, so I will add this:\nconst express = require('express');\nconst redis = require('redis');\n\nconst app = express();\nconst client = redis.createClient({\n  host: 'redis-server',\n  port: 6379\n});\nSo Docker is going to see that the Node.js app is looking for redis-server and redirect that connection over to this running container.\nThe whole time, the Dockerfile only contains this:\nFROM node:alpine\n\nWORKDIR '/app'\n\nCOPY /package.json ./\nRUN npm install\nCOPY . .\n\nCMD [\"npm\", \"start\"]\nSo, whereas before you would have to run docker run myimage to create an instance of all the containers or services inside the file, you can instead run docker-compose up and you don't have to specify an image because Docker will look in the current working directory and look for a docker-compose.yml file inside.\nBefore docker-compose.yml, we had to deal with two separate commands of docker build . and docker run myimage, but in the docker-compose world, if you want to rebuild your images, you write docker-compose up --build. That tells Docker to start up the containers again but rebuild it to get the latest changes.\nSo docker-compose makes it easier for working with multiple containers. The next time you need to start this group of containers in the background, you can do docker-compose up -d; and to stop them, you can do docker-compose down.",
                "upvotes": 263,
                "answered_by": "Daniel",
                "answered_at": "2018-12-21 14:37"
            },
            {
                "answer": "Docker compose file is a way for you to declaratively orchestrate the startup of multiple containers, rather than run each Dockerfile separately with a bash script, which would be much slower to write and harder to debug.",
                "upvotes": 132,
                "answered_by": "Peeter Kokk",
                "answered_at": "2016-07-12 18:32"
            }
        ]
    },
    {
        "title": "COPY with docker but with exclusion",
        "url": "https://stackoverflow.com/questions/43747776/copy-with-docker-but-with-exclusion",
        "votes": "670",
        "views": "487k",
        "author": "Alexander Mills",
        "issued_at": "2017-05-02 21:48",
        "tags": [
            "docker",
            "dockerfile",
            "docker-copy"
        ],
        "answers": [
            {
                "answer": "Create file .dockerignore in your docker build context directory (so in this case, most likely a directory that is a parent to node_modules) with one line in it:\n**/node_modules\nalthough you probably just want:\nnode_modules\nInfo about dockerignore: https://docs.docker.com/engine/reference/builder/#dockerignore-file",
                "upvotes": 985,
                "answered_by": "vith",
                "answered_at": "2017-05-02 21:54"
            },
            {
                "answer": "Dockerfile 1.7.0 (Q1 2024) now supports an --exclude flag for ADD and COPY.\nSee PR moby/buildkit 4561 and the documentation on COPY [--exclude=<path>.\nCOPY --exclude\nWill be included in docker/dockerfile:1.7-labs.\nCOPY [--exclude=<path> ...] <src> ... <dest>\nThe --exclude flag lets you specify a path expression for files to be excluded.\nThe path expression follows the same format as <src>, supporting wildcards and matching using Go's filepath.Match rules. For example, to add all files starting with \"hom\", excluding files with a .txt extension:\nCOPY --exclude=*.txt hom* /mydir/\nYou can specify the --exclude option multiple times for a COPY instruction. Multiple --excludes are files matching its patterns not to be copied, even if the files paths match the pattern specified in <src>. To add all files starting with \"hom\", excluding files with either .txt or .md extensions:\nCOPY --exclude=*.txt --exclude=*.md hom* /mydir/\nAs noted by BMitch in the comments\nThis needs a syntax line since it's experimental:\n# syntax=docker.io/docker/dockerfile:1.7-labs\nAs noted by rrauenza in the comments:\nThe # syntax=docker.io/docker/dockerfile:1.7-labs line must be the FIRST line!\nEven comments above will disable it.",
                "upvotes": 60,
                "answered_by": "VonC",
                "answered_at": "2024-03-05 22:20"
            }
        ]
    },
    {
        "title": "How to set image name in Dockerfile?",
        "url": "https://stackoverflow.com/questions/38986057/how-to-set-image-name-in-dockerfile",
        "votes": "667",
        "views": "887k",
        "author": "gvlasov",
        "issued_at": "2016-08-16 23:37",
        "tags": [
            "docker",
            "tags",
            "dockerfile"
        ],
        "answers": [
            {
                "answer": "Workaround using docker-compose\nTagging of the image isn't supported inside the Dockerfile. This needs to be done in your build command. As a workaround, you can do the build with a docker-compose.yml that identifies the target image name and then run a docker-compose build. A sample docker-compose.yml would look like\nversion: '2'\n\nservices:\n  man:\n    build: .\n    image: dude/man:v2\nThat said, there's a push against doing the build with compose since that doesn't work with swarm mode deploys. So you're back to running the command as you've given in your question:\ndocker build -t dude/man:v2 .\nPersonally, I tend to build with a small shell script in my folder (build.sh) which passes any args and includes the name of the image there to save typing. And for production, the build is handled by a ci/cd server that has the image name inside the pipeline script.",
                "upvotes": 575,
                "answered_by": "BMitch",
                "answered_at": "2016-08-17 01:08"
            },
            {
                "answer": "Workaround using docker-compose\nUpdated: \"container_name\" names the container that's ultimately spun up from the image. \"image\" names and tags the image created, from which the container is built. As others have mentioned, one cannot specify the image name from the Dockerfile, as the OP asked, so we use the docker-compose.yml file instead, and run it with \"docker-compose up -d --build\nHere is another version if you have to reference a specific docker file:\nversion: \"3\"\nservices:\n  nginx:\n    container_name: nginx\n    build:\n      context: ../..\n      dockerfile: ./docker/nginx/Dockerfile\n    image: my_nginx:latest\nThen you just run\ndocker-compose build",
                "upvotes": 100,
                "answered_by": "David Dehghan",
                "answered_at": "2018-01-31 04:37"
            }
        ]
    },
    {
        "title": "How do I make a comment in a Dockerfile?",
        "url": "https://stackoverflow.com/questions/36710459/how-do-i-make-a-comment-in-a-dockerfile",
        "votes": "662",
        "views": "328k",
        "author": "kpie",
        "issued_at": "2016-04-19 06:42",
        "tags": [
            "docker",
            "dockerfile"
        ],
        "answers": [
            {
                "answer": "You can use # at the beginning of a line to start a comment (whitespaces before # are allowed):\n# do some stuff\nRUN apt-get update \\\n    # install some packages\n    && apt-get install -y cron\n#'s in the middle of a string are passed to the command itself, e.g.:\nRUN echo 'we are running some # of cool things'",
                "upvotes": 785,
                "answered_by": "Ranjeet",
                "answered_at": "2016-04-19 06:45"
            },
            {
                "answer": "As others have mentioned, comments are referenced with a # and are documented here. However, unlike some languages, the # must be at the beginning of the line. If they occur part way through the line, they are interpreted as an argument and may result in unexpected behavior.\n# This is a comment\n\nCOPY test_dir target_dir # This is not a comment, it is an argument to COPY\n\nRUN echo hello world # This is an argument to RUN but the shell may ignore it\nIt should also be noted that parser directives have recently been added to the Dockerfile which have the same syntax as a comment. They need to appear at the top of the file, before any other comments or commands. Originally, this directive was added for changing the escape character to support Windows:\n# escape=`\n\nFROM microsoft/nanoserver\nCOPY testfile.txt c:\\\nRUN dir c:\\\nThe first line, while it appears to be a comment, is a parser directive to change the escape character to a backtick so that the COPY and RUN commands can use the backslash in the path. A parser directive is also used with BuildKit to change the frontend parser with a syntax line. See the experimental syntax for more details on how this is being used in practice.\nWith a multi-line command, the commented lines are ignored, but you need to comment out every line individually:\n$ cat Dockerfile\nFROM busybox:latest\nRUN echo first command \\\n# && echo second command disabled \\\n && echo third command\n\n$ docker build .\nSending build context to Docker daemon  23.04kB\nStep 1/2 : FROM busybox:latest\n ---> 59788edf1f3e\nStep 2/2 : RUN echo first command  && echo third command\n ---> Running in b1177e7b563d\nfirst command\nthird command\nRemoving intermediate container b1177e7b563d\n ---> 5442cfe321ac\nSuccessfully built 5442cfe321ac",
                "upvotes": 178,
                "answered_by": "BMitch",
                "answered_at": "2017-02-08 20:56"
            },
            {
                "answer": "Use the # syntax for comments\nFrom: https://docs.docker.com/engine/reference/builder/#format\n# My comment here\nRUN echo 'we are running some cool things'",
                "upvotes": 20,
                "answered_by": "edhurtig",
                "answered_at": "2016-04-19 06:46"
            },
            {
                "answer": "Dockerfile comments start with #, just like Python. kstaken has good examples:\n# Install a more-up-to date version of MongoDB than what is included in the default Ubuntu repositories.\n\nFROM ubuntu\nMAINTAINER Kimbro Staken\n\nRUN apt-key adv --keyserver keyserver.ubuntu.com --recv 7F0CEB10\nRUN echo \"deb http://downloads-distro.mongodb.org/repo/ubuntu-upstart dist 10gen\" | tee -a /etc/apt/sources.list.d/10gen.list\nRUN apt-get update\nRUN apt-get -y install apt-utils\nRUN apt-get -y install mongodb-10gen\n\n#RUN echo \"\" >> /etc/mongodb.conf\n\nCMD [\"/usr/bin/mongod\", \"--config\", \"/etc/mongodb.conf\"] ",
                "upvotes": 15,
                "answered_by": "DhruvPathak",
                "answered_at": "2016-04-19 06:45"
            }
        ]
    },
    {
        "title": "Dockerfile copy keep subdirectory structure",
        "url": "https://stackoverflow.com/questions/30215830/dockerfile-copy-keep-subdirectory-structure",
        "votes": "591",
        "views": "655k",
        "author": "user1220022",
        "issued_at": "2015-05-13 13:09",
        "tags": [
            "copy",
            "docker",
            "dockerfile"
        ],
        "answers": [
            {
                "answer": "Remove star from COPY, with this Dockerfile:\nFROM ubuntu\nCOPY files/ /files/\nRUN ls -la /files/*\nStructure is there:\n$ docker build .\nSending build context to Docker daemon 5.632 kB\nSending build context to Docker daemon \nStep 0 : FROM ubuntu\n ---> d0955f21bf24\nStep 1 : COPY files/ /files/\n ---> 5cc4ae8708a6\nRemoving intermediate container c6f7f7ec8ccf\nStep 2 : RUN ls -la /files/*\n ---> Running in 08ab9a1e042f\n/files/folder1:\ntotal 8\ndrwxr-xr-x 2 root root 4096 May 13 16:04 .\ndrwxr-xr-x 4 root root 4096 May 13 16:05 ..\n-rw-r--r-- 1 root root    0 May 13 16:04 file1\n-rw-r--r-- 1 root root    0 May 13 16:04 file2\n\n/files/folder2:\ntotal 8\ndrwxr-xr-x 2 root root 4096 May 13 16:04 .\ndrwxr-xr-x 4 root root 4096 May 13 16:05 ..\n-rw-r--r-- 1 root root    0 May 13 16:04 file1\n-rw-r--r-- 1 root root    0 May 13 16:04 file2\n ---> 03ff0a5d0e4b\nRemoving intermediate container 08ab9a1e042f\nSuccessfully built 03ff0a5d0e4b",
                "upvotes": 776,
                "answered_by": "ISanych",
                "answered_at": "2015-05-13 16:08"
            },
            {
                "answer": "To merge a local directory into a directory within an image, do this. It will not delete files already present within the image. It will only add files that are present locally, overwriting the files in the image if a file of the same name already exists.\nCOPY ./local-path/. /image-path/",
                "upvotes": 103,
                "answered_by": "Cameron Hudson",
                "answered_at": "2019-07-08 19:23"
            }
        ]
    },
    {
        "title": "How to add users to Docker container?",
        "url": "https://stackoverflow.com/questions/27701930/how-to-add-users-to-docker-container",
        "votes": "563",
        "views": "868k",
        "author": "rfj001",
        "issued_at": "2014-12-30 08:26",
        "tags": [
            "linux",
            "ubuntu",
            "dockerfile"
        ],
        "answers": [
            {
                "answer": "The trick is to use useradd instead of its interactive wrapper adduser. I usually create users with:\nRUN useradd -ms /bin/bash newuser\nwhich creates a home directory for the user and ensures that bash is the default shell.\nYou can then add:\nUSER newuser\nWORKDIR /home/newuser\nto your dockerfile. Every command afterwards as well as interactive sessions will be executed as user newuser:\ndocker run -t -i image\nnewuser@131b7ad86360:~$\nYou might have to give newuser the permissions to execute the programs you intend to run before invoking the user command.\nUsing non-privileged users inside containers is a good idea for security reasons. It also has a few drawbacks. Most importantly, people deriving images from your image will have to switch back to root before they can execute commands with superuser privileges.",
                "upvotes": 871,
                "answered_by": "Paul Staab",
                "answered_at": "2014-12-30 10:05"
            },
            {
                "answer": "Ubuntu\nTry the following lines in Dockerfile:\nRUN useradd -rm -d /home/ubuntu -s /bin/bash -g root -G sudo -u 1001 ubuntu\nUSER ubuntu\nWORKDIR /home/ubuntu\nuseradd options (see: man useradd):\n-r, --system Create a system account. see: Implications creating system accounts\n-m, --create-home Create the user's home directory.\n-d, --home-dir HOME_DIR Home directory of the new account.\n-s, --shell SHELL Login shell of the new account.\n-g, --gid GROUP Name or ID of the primary group.\n-G, --groups GROUPS List of supplementary groups.\n-u, --uid UID Specify user ID. see: Understanding how uid and gid work in Docker containers\n-p, --password PASSWORD Encrypted password of the new account (e.g. ubuntu).\nSetting default user's password\nTo set the user password, add -p \"$(openssl passwd -1 ubuntu)\" to useradd command.\nAlternatively add the following lines to your Dockerfile:\nSHELL [\"/bin/bash\", \"-o\", \"pipefail\", \"-c\"]\nRUN echo 'ubuntu:ubuntu' | chpasswd\nThe first shell instruction is to make sure that -o pipefail option is enabled before RUN with a pipe in it. Read more: Hadolint: Linting your Dockerfile.",
                "upvotes": 266,
                "answered_by": "kenorb",
                "answered_at": "2018-04-16 01:31"
            },
            {
                "answer": "To avoid the interactive questions by adduser, you can call it with these parameters:\nRUN adduser --disabled-password --gecos '' newuser\nThe --gecos parameter is used to set the additional information. In this case it is just empty.\nOn systems with busybox (like Alpine), use\nRUN adduser -D -g '' newuser\nSee busybox adduser",
                "upvotes": 124,
                "answered_by": "Raffael",
                "answered_at": "2015-04-22 12:15"
            }
        ]
    },
    {
        "title": "How to copy multiple files in one layer using a Dockerfile?",
        "url": "https://stackoverflow.com/questions/30256386/how-to-copy-multiple-files-in-one-layer-using-a-dockerfile",
        "votes": "535",
        "views": "496k",
        "author": "kazhuravlev",
        "issued_at": "2015-05-15 09:49",
        "tags": [
            "dockerfile"
        ],
        "answers": [
            {
                "answer": "COPY README.md package.json gulpfile.js __BUILD_NUMBER ./\nor\nCOPY [\"__BUILD_NUMBER\", \"README.md\", \"gulpfile\", \"another_file\", \"./\"]\nYou can also use wildcard characters in the sourcefile specification. See the docs for a little more detail.\nDirectories are special! If you write\nCOPY dir1 dir2 ./\nthat actually works like\nCOPY dir1/* dir2/* ./\nIf you want to copy multiple directories (not their contents) under a destination directory in a single command, you'll need to set up the build context so that your source directories are under a common parent and then COPY that parent.",
                "upvotes": 882,
                "answered_by": "Nathaniel Waisbrot",
                "answered_at": "2015-05-19 04:08"
            },
            {
                "answer": "COPY <all> <the> <things> <last-arg-is-destination>/\nNote: the trailing / is required when copying from multiple sources.\nBut here is an important excerpt from the docs:\nIf you have multiple Dockerfile steps that use different files from your context, COPY them individually, rather than all at once. This ensures that each step\u2019s build cache is only invalidated (forcing the step to be re-run) if the specifically required files change.\nhttps://docs.docker.com/develop/develop-images/dockerfile_best-practices/#add-or-copy",
                "upvotes": 165,
                "answered_by": "swapsCAPS",
                "answered_at": "2018-07-12 11:47"
            },
            {
                "answer": "It might be worth mentioning that you can also create a .dockerignore file, to exclude the files and directories that you don't want to copy:\nhttps://docs.docker.com/engine/reference/builder/#dockerignore-file\nBefore the docker CLI sends the context to the docker daemon, it looks for a file named .dockerignore in the root directory of the context. If this file exists, the CLI modifies the context to exclude files and directories that match patterns in it. This helps to avoid unnecessarily sending large or sensitive files and directories to the daemon and potentially adding them to images using ADD or COPY.",
                "upvotes": 29,
                "answered_by": "Aaron",
                "answered_at": "2019-08-20 14:04"
            }
        ]
    },
    {
        "title": "Docker: How to use bash with an Alpine based docker image?",
        "url": "https://stackoverflow.com/questions/40944479/docker-how-to-use-bash-with-an-alpine-based-docker-image",
        "votes": "476",
        "views": "478k",
        "author": "iamdeit",
        "issued_at": "2016-12-03 05:14",
        "tags": [
            "bash",
            "docker",
            "shell",
            "dockerfile",
            "alpine-linux"
        ],
        "answers": [
            {
                "answer": "Alpine docker image doesn't have bash installed by default. You will need to add the following commands to get bash:\nRUN apk update && apk add bash\nIf you're using Alpine 3.3+ then you can just do:\nRUN apk add --no-cache bash\nTo keep the docker image size small. (Thanks to comment from @sprkysnrky)\nIf you just want to connect to the container and don't need bash, you can use:\ndocker run --rm -i -t alpine /bin/sh --login",
                "upvotes": 734,
                "answered_by": "anubhava",
                "answered_at": "2016-12-03 05:18"
            },
            {
                "answer": "Option \ud83d\udc08: Start from Bash\nThe official bash image is based on Alpine and prevents you from needing to install bash every time. Simply use\ndocker pull bash\nThis was first published on Oct 19, 2016 at 6:43 pm.\nOption \ud83d\udc15: Use your Existing Base Image\nIf you want to use your existing base image, while avoiding the need to install bash on every container boot, then you can add this to your Dockerfile.\n# Use openjdk:8-jdk-alpine as the base image\nFROM openjdk:8-jdk-alpine\n\n# Install bash package\nRUN apk add --no-cache bash",
                "upvotes": 23,
                "answered_by": "James Geddes",
                "answered_at": "2022-07-05 09:56"
            }
        ]
    },
    {
        "title": "Difference between links and depends_on in docker_compose.yml",
        "url": "https://stackoverflow.com/questions/35832095/difference-between-links-and-depends-on-in-docker-compose-yml",
        "votes": "457",
        "views": "383k",
        "author": "itsjef",
        "issued_at": "2016-03-06 20:24",
        "tags": [
            "docker",
            "docker-compose",
            "dockerfile"
        ],
        "answers": [
            {
                "answer": "This answer is for docker-compose version 2 and it also works on version 3\nYou can still access the data when you use depends_on.\nIf you look at docker docs Docker Compose and Django, you still can access the database like this:\nversion: '2'\nservices:\n  db:\n    image: postgres\n  web:\n    build: .\n    command: python manage.py runserver 0.0.0.0:8000\n    volumes:\n      - .:/code\n    ports:\n      - \"8000:8000\"\n    depends_on:\n      - db\nWhat is the difference between links and depends_on?\nlinks:\nWhen you create a container for a database, for example:\ndocker run -d --name=test-mysql --env=\"MYSQL_ROOT_PASSWORD=mypassword\" -P mysql\n\ndocker inspect d54cf8a0fb98 |grep HostPort\nAnd you may find\n\"HostPort\": \"32777\"\nThis means you can connect the database from your localhost port 32777 (3306 in container) but this port will change every time you restart or remove the container. So you can use links to make sure you will always connect to the database and don't have to know which port it is.\nweb:\n  links:\n   - db\ndepends_on:\nI found a nice blog from Giorgio Ferraris Docker-compose.yml: from V1 to V2\nWhen docker-compose executes V2 files, it will automatically build a network between all of the containers defined in the file, and every container will be immediately able to refer to the others just using the names defined in the docker-compose.yml file.\nAnd\nSo we don\u2019t need links anymore; links were used to start a network communication between our db container and our web-server container, but this is already done by docker-compose\nUpdate\ndepends_on\nExpress dependency between services, which has two effects:\ndocker-compose up will start services in dependency order. In the following example, db and redis will be started before web.\ndocker-compose up SERVICE will automatically include SERVICE\u2019s dependencies. In the following example, docker-compose up web will also create and start db and redis.\nSimple example:\nversion: '2'\nservices:\n  web:\n    build: .\n    depends_on:\n      - db\n      - redis\n  redis:\n    image: redis\n  db:\n    image: postgres\nNote: depends_on will not wait for db and redis to be \u201cready\u201d before starting web - only until they have been started. If you need to wait for a service to be ready, see Controlling startup order for more on this problem and strategies for solving it.",
                "upvotes": 271,
                "answered_by": "Windsooon",
                "answered_at": "2016-09-23 10:19"
            },
            {
                "answer": "[Update Sep 2016]: This answer was intended for docker compose file v1 (as shown by the sample compose file below). For v2, see the other answer by @Windsooon.\n[Original answer]:\nIt is pretty clear in the documentation. depends_on decides the dependency and the order of container creation and links not only does these, but also\nContainers for the linked service will be reachable at a hostname identical to the alias, or the service name if no alias was specified.\nFor example, assuming the following docker-compose.yml file:\nweb:\n  image: example/my_web_app:latest\n  links:\n    - db\n    - cache\n\ndb:\n  image: postgres:latest\n\ncache:\n  image: redis:latest\nWith links, code inside web will be able to access the database using db:5432, assuming port 5432 is exposed in the db image. If depends_on were used, this wouldn't be possible, but the startup order of the containers would be correct.",
                "upvotes": 58,
                "answered_by": "Xiongbing Jin",
                "answered_at": "2016-03-06 20:46"
            }
        ]
    },
    {
        "title": "Change directory command in Docker?",
        "url": "https://stackoverflow.com/questions/20632258/change-directory-command-in-docker",
        "votes": "441",
        "views": "607k",
        "author": "RParadox",
        "issued_at": "2013-12-17 10:55",
        "tags": [
            "docker",
            "dockerfile",
            "cd"
        ],
        "answers": [
            {
                "answer": "To change into another directory use WORKDIR. All the RUN, CMD and ENTRYPOINT commands after WORKDIR will be executed from that directory.\nRUN git clone XYZ \nWORKDIR \"/XYZ\"\nRUN make",
                "upvotes": 868,
                "answered_by": "Javier Castellanos",
                "answered_at": "2014-11-04 03:20"
            },
            {
                "answer": "You can run a script, or a more complex RUN. Here is an adaptation of your example case:\nRUN git clone XYZ && \\\n    cd XYZ && \\\n    make XYZ\nHere is an example from a Dockerfile I've downloaded to look at previously:\nRUN cd /opt && unzip treeio.zip && mv treeio-master treeio && \\\n    rm -f treeio.zip && cd treeio && pip install -r requirements.pip\nBecause of the use of '&&', it will only get to the final 'pip install' command if all the previous commands have succeeded.\nIn fact, since every RUN creates a new commit & (currently) an AUFS layer, if you have too many commands in the Dockerfile, you will use up the limits, so merging the RUNs (when the file is stable) can be a very useful thing to do.",
                "upvotes": 268,
                "answered_by": "Alister Bulman",
                "answered_at": "2013-12-17 11:13"
            },
            {
                "answer": "You can use single RUN command for all of them\nRUN git clone XYZ && \\\n    cd XYZ && \\\n    make XYZ",
                "upvotes": 7,
                "answered_by": "aditya Damera",
                "answered_at": "2021-09-25 07:26"
            }
        ]
    },
    {
        "title": "Add a volume to Docker, but exclude a sub-folder",
        "url": "https://stackoverflow.com/questions/29181032/add-a-volume-to-docker-but-exclude-a-sub-folder",
        "votes": "422",
        "views": "224k",
        "author": "Golo Roden",
        "issued_at": "2015-03-21 09:13",
        "tags": [
            "docker",
            "dockerfile"
        ],
        "answers": [
            {
                "answer": "If you want to have subdirectories ignored by docker-compose but persistent, you can do the following in docker-compose.yml:\nvolumes:\n  node_modules:\nservices:\n  server:\n    volumes:\n      - .:/app\n      - node_modules:/app/node_modules\nThis will mount your current directory as a shared volume, but mount a persistent docker volume in place of your local node_modules directory. This is similar to the answer by @kernix, but this will allow node_modules to persist between docker-compose up runs, which is likely the desired behavior.",
                "upvotes": 203,
                "answered_by": "Nate T",
                "answered_at": "2016-07-26 22:54"
            }
        ]
    },
    {
        "title": "Dockerfile if else condition with external arguments",
        "url": "https://stackoverflow.com/questions/43654656/dockerfile-if-else-condition-with-external-arguments",
        "votes": "419",
        "views": "554k",
        "author": "nick_gabpe",
        "issued_at": "2017-04-27 10:05",
        "tags": [
            "docker",
            "dockerfile"
        ],
        "answers": [
            {
                "answer": "It might not look that clean but you can have your Dockerfile (conditional) as follow:\nFROM centos:7\nARG arg\nRUN if [[ -z \"$arg\" ]] ; then echo Argument not provided ; else echo Argument is $arg ; fi\nand then build the image as:\ndocker build -t my_docker .  --build-arg arg=45\nor\ndocker build -t my_docker . ",
                "upvotes": 441,
                "answered_by": "Qasim Sarfraz",
                "answered_at": "2017-04-27 11:35"
            },
            {
                "answer": "There is an interesting alternative to the proposed solutions, that works with a single Dockerfile, require only a single call to docker build per conditional build and avoids bash.\nSolution:\nThe following Dockerfile solves that problem. Copy-paste it and try it yourself.\nARG my_arg\n\nFROM centos:7 AS base\nRUN echo \"do stuff with the centos image\"\n\nFROM base AS branch-version-1\nRUN echo \"this is the stage that sets VAR=TRUE\"\nENV VAR=TRUE\n\nFROM base AS branch-version-2\nRUN echo \"this is the stage that sets VAR=FALSE\"\nENV VAR=FALSE\n\nFROM branch-version-${my_arg} AS final\nRUN echo \"VAR is equal to ${VAR}\"\nExplanation of Dockerfile:\nWe first get a base image (centos:7 in your case) and put it into its own stage. The base stage should contain things that you want to do before the condition. After that, we have two more stages, representing the branches of our condition: branch-version-1 and branch-version-2. We build both of them. The final stage than chooses one of these stages, based on my_arg. Conditional Dockerfile. There you go.\nOutput when running:\n(I abbreviated this a little...)\nmy_arg==2\ndocker build --build-arg my_arg=2 .\nStep 1/12 : ARG my_arg\nStep 2/12 : ARG ENV\nStep 3/12 : FROM centos:7 AS base\nStep 4/12 : RUN echo \"do stuff with the centos image\"\ndo stuff with the centos image\nStep 5/12 : FROM base AS branch-version-1\nStep 6/12 : RUN echo \"this is the stage that sets VAR=TRUE\"\nthis is the stage that sets VAR=TRUE\nStep 7/12 : ENV VAR=TRUE\nStep 8/12 : FROM base AS branch-version-2\nStep 9/12 : RUN echo \"this is the stage that sets VAR=FALSE\"\nthis is the stage that sets VAR=FALSE\nStep 10/12 : ENV VAR=FALSE\nStep 11/12 : FROM branch-version-${my_arg}\nStep 12/12 : RUN echo \"VAR is equal to ${VAR}\"\nVAR is equal to FALSE\nmy_arg==1\ndocker build --build-arg my_arg=1 .\n...\nStep 11/12 : FROM branch-version-${my_arg}\nStep 12/12 : RUN echo \"VAR is equal to ${VAR}\"\nVAR is equal to TRUE\nThanks to T\u00f5nis for this amazing idea!",
                "upvotes": 320,
                "answered_by": "User12547645",
                "answered_at": "2020-03-23 19:14"
            },
            {
                "answer": "Do not use build args described in other answers where at all possible. This is an old messy solution. Docker's target property solves for this issue.\nTarget Example\nDockerfile\nFROM foo as base\n\nRUN ...\n\n# Build dev image\nFROM base as image-dev\n\nRUN ...\nCOPY ...\n\n# Build prod image\nFROM base as image-prod\n\nRUN ...\nCOPY ...\ndocker build --target image-dev -t foo .\nversion: '3.4'\n\nservices:\n\n  dev:\n    build:\n      context: .\n      dockerfile: Dockerfile\n      target: image-dev\nReal World\nDockerfiles get complex in the real world. Use buildkit & COPY --from for faster, more maintainable Dockerfiles:\nDocker builds every stage above the target, regardless of whether it is inherited or not. Use buildkit to build only inherited stages. Docker must by v19+. Hopefully this will be a default feature soon.\nTargets may share build stages. Use COPY --from to simplify inheritance.\nFROM foo as base\nRUN ...\nWORKDIR /opt/my-proj\n\nFROM base as npm-ci-dev\n# invalidate cache\nCOPY --chown=www-data:www-data ./package.json /opt/my-proj/package.json\nCOPY --chown=www-data:www-data ./package-lock.json /opt/my-proj/package-lock.json\nRUN npm ci\n\nFROM base as npm-ci-prod\n# invalidate cache\nCOPY --chown=www-data:www-data ./package.json /opt/my-proj/package.json\nCOPY --chown=www-data:www-data ./package-lock.json /opt/my-proj/package-lock.json\nRUN npm ci --only=prod\n\nFROM base as proj-files\nCOPY --chown=www-data:www-data ./ /opt/my-proj\n\nFROM base as image-dev\n# Will mount, not copy in dev environment\nRUN ...\n\nFROM base as image-ci\nCOPY --from=npm-ci-dev /opt/my-proj .\nCOPY --from=proj-files /opt/my-proj .\nRUN ...\n\nFROM base as image-stage\nCOPY --from=npm-ci-prod /opt/my-proj .\nCOPY --from=proj-files /opt/my-proj .\nRUN ...\n\nFROM base as image-prod\nCOPY --from=npm-ci-prod /opt/my-proj .\nCOPY --from=proj-files /opt/my-proj .\nRUN ...\nEnable experimental mode.\nsudo echo '{\"experimental\": true}' | sudo tee /etc/docker/daemon.json\nBuild with buildkit enabled. Buildkit builds without cache by default - enable with --build-arg BUILDKIT_INLINE_CACHE=1\nCI build job.\nDOCKER_BUILDKIT=1 \\\n    docker build \\\n    --build-arg BUILDKIT_INLINE_CACHE=1 \\\n    --target image-ci\\\n    -t foo:ci\n    .\nUse cache from a pulled image with --cache-from\nProd build job\ndocker pull foo:ci\ndocker pull foo:stage\n\nDOCKER_BUILDKIT=1 \\\n    docker build \\\n    --cache-from foo:ci,foo:stage \\\n    --target image-prod \\\n    -t prod\n    .",
                "upvotes": 120,
                "answered_by": "Dave",
                "answered_at": "2021-01-08 06:04"
            },
            {
                "answer": "From some reason most of the answers here didn't help me (maybe it's related to my FROM image in the Dockerfile)\nSo I preferred to create a bash script in my workspace combined with --build-arg in order to handle if statement while Docker build by checking if the argument is empty or not\nBash script:\n#!/bin/bash -x\n\nif test -z $1 ; then \n    echo \"The arg is empty\"\n    ....do something....\nelse \n    echo \"The arg is not empty: $1\"\n    ....do something else....\nfi\nDockerfile:\nFROM ...\n....\nARG arg\nCOPY bash.sh /tmp/  \nRUN chmod u+x /tmp/bash.sh && /tmp/bash.sh $arg\n....\nDocker Build:\ndocker build --pull -f \"Dockerfile\" -t $SERVICE_NAME --build-arg arg=\"yes\" .\nRemark: This will go to the else (false) in the bash script\ndocker build --pull -f \"Dockerfile\" -t $SERVICE_NAME .\nRemark: This will go to the if (true)\nEdit 1:\nAfter several tries I have found the following article and this one which helped me to understand 2 things:\n1) ARG before FROM is outside of the build\n2) The default shell is /bin/sh which means that the if else is working a little bit different in the docker build. for example you need only one \"=\" instead of \"==\" to compare strings.\nSo you can do this inside the Dockerfile\nARG argname=false   #default argument when not provided in the --build-arg\nRUN if [ \"$argname\" = \"false\" ] ; then echo 'false'; else echo 'true'; fi\nand in the docker build:\ndocker build --pull -f \"Dockerfile\" --label \"service_name=${SERVICE_NAME}\" -t $SERVICE_NAME --build-arg argname=true .",
                "upvotes": 32,
                "answered_by": "dsaydon",
                "answered_at": "2018-04-04 15:21"
            }
        ]
    },
    {
        "title": "How to get an environment variable value into Dockerfile during \"docker build\"?",
        "url": "https://stackoverflow.com/questions/19537645/how-to-get-an-environment-variable-value-into-dockerfile-during-docker-build",
        "votes": "406",
        "views": "540k",
        "author": "Damien MATHIEU",
        "issued_at": "2013-10-23 09:16",
        "tags": [
            "docker",
            "dockerfile",
            "docker-build"
        ],
        "answers": [
            {
                "answer": "You should use the ARG directive in your Dockerfile which is meant for this purpose.\nThe ARG instruction defines a variable that users can pass at build-time to the builder with the docker build command using the --build-arg <varname>=<value> flag.\nSo your Dockerfile will have this line:\nARG request_domain\nor if you'd prefer a default value:\nARG request_domain=127.0.0.1\nNow you can reference this variable inside your Dockerfile:\nENV request_domain=$request_domain\nthen you will build your container like so:\n$ docker build --build-arg request_domain=mydomain Dockerfile\n\nNote 1: Your image will not build if you have referenced an ARG in your Dockerfile but excluded it in --build-arg.\nNote 2: If a user specifies a build argument that was not defined in the Dockerfile, the build outputs a warning:\n[Warning] One or more build-args [foo] were not consumed.",
                "upvotes": 606,
                "answered_by": "Daniel van Flymen",
                "answered_at": "2016-01-04 21:23"
            },
            {
                "answer": "This is for those looking to pass env variable from docker-compose using .env file to dockerfile during build and then pass those args as env variable to container.\nTypical docker-compose.yaml file:\nservices:\n  web:\n    build:\n      context: ./api\n      dockerfile: Dockerfile\n      args:\n        - SECRET_KEY=$SECRET_KEY\n        - DATABASE_URL=$DATABASE_URL\n        - AWS_ACCESS_KEY_ID=$AWS_ACCESS_KEY_ID\nPass the env variable present in .env file to args in build command.\nTypical .env file:\nSECRET_KEY=blahblah\nDATABASE_URL=dburl\nNow when you run docker-compose up -d command, docker-compose file takes values from .env file then pass it to docker-compose file. Now Dockerfile of web contains all those variables through args during build.\nNow typical Dockerfile of web:\nFROM python:3.6-alpine\n\nARG SECRET_KEY\nARG DATABASE_URL\nARG AWS_ACCESS_KEY_ID\nARG AWS_SECRET_ACCESS_KEY\nARG AWS_BUCKET\nARG AWS_REGION\nARG CLOUDFRONT_DOMAIN\n\nENV CELERY_BROKER_URL redis://redis:6379/0\nENV CELERY_RESULT_BACKEND redis://redis:6379/0\nENV C_FORCE_ROOT true\nENV SECRET_KEY  ${SECRET_KEY?secretkeynotset}\nENV DATABASE_URL ${DATABASE_URL?envdberror}\nNow we recieved those secret_key and db url as arg in Dockerfile. Now let's use those in ENV as ENV SECRET_KEY ${SECRET_KEY?secretkeynotset}. Now even docker container has those variables in its environment.\nRemember not to use ARG $SECRET_KEY (which I did). It should be ARG SECRET_KEY.",
                "upvotes": 95,
                "answered_by": "Zincfan",
                "answered_at": "2021-09-14 07:34"
            },
            {
                "answer": "An alternative using envsubst without losing the ability to use commands like COPY or ADD, and without using intermediate files would be to use Bash's Process Substitution:\ndocker build -f <(envsubst < Dockerfile) -t my-target .",
                "upvotes": 25,
                "answered_by": "L. Alberto Gim\u00e9nez",
                "answered_at": "2017-03-13 18:56"
            }
        ]
    },
    {
        "title": "Build and run Dockerfile with one command",
        "url": "https://stackoverflow.com/questions/45141402/build-and-run-dockerfile-with-one-command",
        "votes": "395",
        "views": "364k",
        "author": "Twinkle",
        "issued_at": "2017-07-17 10:08",
        "tags": [
            "docker",
            "dockerfile"
        ],
        "answers": [
            {
                "answer": "If you want to avoid tagging, docker build -q outputs nothing but the final image hash, which you can use as the argument to docker run:\ndocker run -it $(docker build -q .)\nAnd add --rm to docker run if you want the container removed automatically when it exits.\ndocker run --rm -it $(docker build -q .)",
                "upvotes": 652,
                "answered_by": "starthal",
                "answered_at": "2018-07-12 20:37"
            },
            {
                "answer": "I use docker-compose for this convenience since most of the apps I'm building are talking to external services sooner or later, so if I'm going to use it anyway, why not use it from the start. Just have docker-compose.yml as:\nversion: \"3\"\nservices:\n  app:\n    build: .\nand then just run the app with:\ndocker-compose up --build app\nIt will rebuild the image or reuse the container depending on whether there were changes made to the image definition.",
                "upvotes": 99,
                "answered_by": "Dejan Simic",
                "answered_at": "2020-01-04 02:13"
            },
            {
                "answer": "Recently I started getting a promo message about using docker scan after every build.\nUse 'docker scan' to run Snyk tests against images to find vulnerabilities and learn how to fix them\nHere's what I used to do:\ndocker build --quiet .\nand here's what is working now:\ndocker build --quiet . | head -n1",
                "upvotes": 8,
                "answered_by": "bozdoz",
                "answered_at": "2021-04-16 19:25"
            }
        ]
    },
    {
        "title": "What is the point of WORKDIR on Dockerfile?",
        "url": "https://stackoverflow.com/questions/51066146/what-is-the-point-of-workdir-on-dockerfile",
        "votes": "391",
        "views": "494k",
        "author": "Le garcon",
        "issued_at": "2018-06-27 15:07",
        "tags": [
            "docker",
            "dockerfile"
        ],
        "answers": [
            {
                "answer": "According to the documentation:\nThe WORKDIR instruction sets the working directory for any RUN, CMD, ENTRYPOINT, COPY and ADD instructions that follow it in the Dockerfile. If the WORKDIR doesn\u2019t exist, it will be created even if it\u2019s not used in any subsequent Dockerfile instruction.\nAlso, in the Docker best practices it recommends you to use it:\n... you should use WORKDIR instead of proliferating instructions like RUN cd \u2026 && do-something, which are hard to read, troubleshoot, and maintain.\nI would suggest to keep it.\nI think you can refactor your Dockerfile to something like:\nFROM node:latest\nWORKDIR /usr/src/app\nCOPY package.json .\nRUN npm install\nCOPY . ./\nEXPOSE 3000\nCMD [ \"npm\", \"start\" ] ",
                "upvotes": 387,
                "answered_by": "juanlumn",
                "answered_at": "2018-06-27 15:19"
            },
            {
                "answer": "Before applying WORKDIR. Here the WORKDIR is at the wrong place and is not used wisely.\nFROM microsoft/aspnetcore:2\nCOPY --from=build-env /publish /publish\nWORKDIR /publish\nENTRYPOINT [\"dotnet\", \"/publish/api.dll\"]\nWe corrected the above code to put WORKDIR at the right location and optimised the following statements by removing /Publish\nFROM microsoft/aspnetcore:2\nWORKDIR /publish\nCOPY --from=build-env /publish .\nENTRYPOINT [\"dotnet\", \"api.dll\"]\nSo it acts like a cd and sets the tone for the upcoming statements.",
                "upvotes": 12,
                "answered_by": "Blue Clouds",
                "answered_at": "2018-09-30 09:28"
            }
        ]
    },
    {
        "title": "An error, \"failed to solve with frontend dockerfile.v0\"",
        "url": "https://stackoverflow.com/questions/64221861/an-error-failed-to-solve-with-frontend-dockerfile-v0",
        "votes": "389",
        "views": "757k",
        "author": "Muhammad Yasir",
        "issued_at": "2020-10-06 08:14",
        "tags": [
            "docker",
            "docker-compose",
            "dockerfile",
            "gatsby"
        ],
        "answers": [
            {
                "answer": "I had experienced this issue after upgrading to the latest Docker Desktop version on Mac. Solved with the comment on this issue.\nSolution: Don't use buildkit and it works for me.\nexport DOCKER_BUILDKIT=0\nexport COMPOSE_DOCKER_CLI_BUILD=0",
                "upvotes": 465,
                "answered_by": "Esteban Gatjens",
                "answered_at": "2021-03-18 16:32"
            },
            {
                "answer": "I had the same issue and all I had to do was to capitalize the Docker configuration filename:\ndockerfile > didn't work\nDockerfile > did work",
                "upvotes": 276,
                "answered_by": "Luis Gouveia",
                "answered_at": "2020-10-21 17:34"
            },
            {
                "answer": "If you use Docker for Windows you need to disable buildkit from Docker Engine in Settings. It works for me and solved my error\nSet buildkit option to false.\n{\n  \"builder\": {\n    \"gc\": {\n      \"defaultKeepStorage\": \"20GB\",\n      \"enabled\": true\n    }\n  },\n  \"experimental\": false,\n  \"features\": {\n    \"buildkit\": false\n  }\n}",
                "upvotes": 138,
                "answered_by": "dima-hx",
                "answered_at": "2021-11-29 10:26"
            }
        ]
    },
    {
        "title": "Docker - unable to prepare context: unable to evaluate symlinks in Dockerfile path: GetFileAttributesEx",
        "url": "https://stackoverflow.com/questions/35511604/docker-unable-to-prepare-context-unable-to-evaluate-symlinks-in-dockerfile-pa",
        "votes": "377",
        "views": "638k",
        "author": "villanux",
        "issued_at": "2016-02-19 17:48",
        "tags": [
            "docker",
            "dockerfile",
            "docker-toolbox",
            "docker-build"
        ],
        "answers": [
            {
                "answer": "While executing the following command,\ndocker build -t docker-whale .\ncheck that Dockerfile is present in your current working directory.",
                "upvotes": 348,
                "answered_by": "kalyani chaudhari",
                "answered_at": "2016-07-14 10:00"
            },
            {
                "answer": "The error message is misleading\nThe problem has nothing to do with symbolic links really. Usually, the problem is only that Docker cannot find the Dockerfile describing the build.\nTypical reasons are these:\nDockerfile has wrong name. It must be called Dockerfile. If it is called, for instance, dockerfile, .Dockerfile, Dockerfile.txt, or other, it will not be found.\nDockerfile is not in context. If you say docker build contextdir, the Dockerfile must be at contextdir/Dockerfile. If you have it in, say, ./Dockerfile instead, it will not be found.\nDockerfile does not exist at all. Does it sound silly? Well, I got the above error message from my GitLab CI after I had written a nice Dockerfile, but I had forgotten to check it in. Silly? Sure. Unlikely? No.",
                "upvotes": 255,
                "answered_by": "Lutz Prechelt",
                "answered_at": "2019-08-30 09:47"
            },
            {
                "answer": "If you are working on Windows 8, you would be using Docker toolbox.\nFrom the mydockerbuild directory, run the below command as your Dockerfile is a textfile:\ndocker build -t docker-whale -f ./Dockerfile.txt .",
                "upvotes": 51,
                "answered_by": "Divyanshu sachdeva",
                "answered_at": "2017-03-02 05:53"
            }
        ]
    },
    {
        "title": "How to define a variable in a Dockerfile?",
        "url": "https://stackoverflow.com/questions/33935807/how-to-define-a-variable-in-a-dockerfile",
        "votes": "373",
        "views": "390k",
        "author": "Maxime",
        "issued_at": "2015-11-26 10:10",
        "tags": [
            "variables",
            "docker",
            "dockerfile"
        ],
        "answers": [
            {
                "answer": "You can use ARG - see https://docs.docker.com/engine/reference/builder/#arg\nThe ARG instruction defines a variable that users can pass at build-time to the builder with the docker build command using the --build-arg <varname>=<value> flag. If a user specifies a build argument that was not defined in the Dockerfile, the build outputs an error.\nCan be useful with COPY during build time (e.g. copying tag specific content like specific folders) For example:\nARG MODEL_TO_COPY\nCOPY application ./application\nCOPY $MODEL_TO_COPY ./application/$MODEL_TO_COPY\nWhile building the container:\ndocker build --build-arg MODEL_TO_COPY=model_name -t <container>:<model_name specific tag> .",
                "upvotes": 262,
                "answered_by": "lumos0815",
                "answered_at": "2016-06-05 08:33"
            },
            {
                "answer": "To answer your question:\nIn my Dockerfile, I would like to define variables that I can use later in the Dockerfile.\nYou can define a variable with:\nARG myvalue=3\nNote that spaces around the equal character are not allowed.\nAnd use it later with:\nRUN echo \"$myvalue\" > /test",
                "upvotes": 221,
                "answered_by": "Ortomala Lokni",
                "answered_at": "2018-08-09 14:46"
            },
            {
                "answer": "To my knowledge, only ENV allows that, as mentioned in \"Environment replacement\"\nEnvironment variables (declared with the ENV statement) can also be used in certain instructions as variables to be interpreted by the Dockerfile.\nThey have to be environment variables in order to be redeclared in each new containers created for each line of the Dockerfile by docker build.\nIn other words, those variables aren't interpreted directly in a Dockerfile, but in a container created for a Dockerfile line, hence the use of environment variable.\nThis day, I use both ARG (docker 1.10+, and docker build --build-arg var=value) and ENV.\nUsing ARG alone means your variable is visible at build time, not at runtime.\nMy Dockerfile usually has:\nARG var\nENV var=${var}\nIn your case, ARG is enough: I use it typically for setting http_proxy variable, that docker build needs for accessing internet at build time.\nChristopher King adds in the comments:\nWatch out!\nThe ARG variable is only in scope for the \"stage that it is used\" and needs to be redeclared for each stage.\nHe points out to Dockerfile / scope\nAn ARG variable definition comes into effect from the line on which it is defined in the Dockerfile not from the argument\u2019s use on the command-line or elsewhere.\nFor example, consider this Dockerfile:\nFROM busybox\nUSER ${user:-some_user}\nARG user\nUSER $user\n# ...\nA user builds this file by calling:\ndocker build --build-arg user=what_user .\nThe USER at line 2 evaluates to some_user as the user variable is defined on the subsequent line 3.\nThe USER at line 4 evaluates to what_user as user is defined and the what_user value was passed on the command line.\nPrior to its definition by an ARG instruction, any use of a variable results in an empty string.\nAn ARG instruction goes out of scope at the end of the build stage where it was defined.\nTo use an arg in multiple stages, each stage must include the ARG instruction.\nAs summarized by gratinierer in the comments:\nIn simple words in the most simple cases:\nPut your ARG-lines after your FROM-line to avoid the surprise that your ARG-variable seems to be empty all the time.",
                "upvotes": 101,
                "answered_by": "VonC",
                "answered_at": "2015-11-26 10:20"
            }
        ]
    },
    {
        "title": "Why doesn't Python app print anything when run in a detached docker container?",
        "url": "https://stackoverflow.com/questions/29663459/why-doesnt-python-app-print-anything-when-run-in-a-detached-docker-container",
        "votes": "365",
        "views": "248k",
        "author": "jpdus",
        "issued_at": "2015-04-16 00:47",
        "tags": [
            "python",
            "docker",
            "dockerfile",
            "docker-run"
        ],
        "answers": [
            {
                "answer": "Finally I found a solution to see Python output when running daemonized in Docker, thanks to @ahmetalpbalkan over at GitHub. Answering it here myself for further reference :\nUsing unbuffered output with\nCMD [\"python\",\"-u\",\"main.py\"]\ninstead of\nCMD [\"python\",\"main.py\"]\nsolves the problem; you can see the output now (both, stderr and stdout) via\ndocker logs myapp\nwhy -u ref\n- print is indeed buffered and docker logs will eventually give you that output, just after enough of it will have piled up\n- executing the same script with python -u gives instant output as said above\n- import logging + logging.warning(\"text\") gives the expected result even without -u\nwhat it means by python -u ref. > python --help | grep -- -u\n-u     : force the stdout and stderr streams to be unbuffered;",
                "upvotes": 649,
                "answered_by": "jpdus",
                "answered_at": "2015-04-20 10:37"
            },
            {
                "answer": "In my case, running Python with -u didn't change anything. What did the trick, however, was to set PYTHONUNBUFFERED=1 as environment variable:\ndocker run --name=myapp -e PYTHONUNBUFFERED=1 -d myappimage\n[Edit]: Updated PYTHONUNBUFFERED=0 to PYTHONUNBUFFERED=1 after Lars's comment. This doesn't change the behavior and adds clarity.",
                "upvotes": 179,
                "answered_by": "Victor",
                "answered_at": "2015-08-03 20:38"
            }
        ]
    },
    {
        "title": "Understanding \"VOLUME\" instruction in DockerFile",
        "url": "https://stackoverflow.com/questions/41935435/understanding-volume-instruction-in-dockerfile",
        "votes": "345",
        "views": "519k",
        "author": "refactor",
        "issued_at": "2017-01-30 12:01",
        "tags": [
            "docker",
            "dockerfile"
        ],
        "answers": [
            {
                "answer": "In short: No, your VOLUME instruction is not correct.\nDockerfile's VOLUME specify one or more volumes given container-side paths. But it does not allow the image author to specify a host path. On the host-side, the volumes are created with a very long ID-like name inside the Docker root. On my machine this is /var/lib/docker/volumes.\nNote: Because the autogenerated name is extremely long and makes no sense from a human's perspective, these volumes are often referred to as \"unnamed\" or \"anonymous\".\nYour example that uses a '.' character will not even run on my machine, no matter if I make the dot the first or second argument. I get this error message:\ndocker: Error response from daemon: oci runtime error: container_linux.go:265: starting container process caused \"process_linux.go:368: container init caused \"open /dev/ptmx: no such file or directory\"\".\nI know that what has been said to this point is probably not very valuable to someone trying to understand VOLUME and -v and it certainly does not provide a solution for what you try to accomplish. So, hopefully, the following examples will shed some more light on these issues.\nMinitutorial: Specifying volumes\nGiven this Dockerfile:\nFROM openjdk:8u131-jdk-alpine\nVOLUME vol1 vol2\n(For the outcome of this minitutorial, it makes no difference if we specify vol1 vol2 or /vol1 /vol2 \u2014 this is because the default working directory within a Dockerfile is /)\nBuild it:\ndocker build -t my-openjdk\nRun:\ndocker run --rm -it my-openjdk\nInside the container, run ls in the command line and you'll notice two directories exist; /vol1 and /vol2.\nRunning the container also creates two directories, or \"volumes\", on the host-side.\nWhile having the container running, execute docker volume ls on the host machine and you'll see something like this (I have replaced the middle part of the name with three dots for brevity):\nDRIVER    VOLUME NAME\nlocal     c984...e4fc\nlocal     f670...49f0\nBack in the container, execute touch /vol1/weird-ass-file (creates a blank file at said location).\nThis file is now available on the host machine, in one of the unnamed volumes lol. It took me two tries because I first tried the first listed volume, but eventually I did find my file in the second listed volume, using this command on the host machine:\nsudo ls /var/lib/docker/volumes/f670...49f0/_data\nSimilarly, you can try to delete this file on the host and it will be deleted in the container as well.\nNote: The _data folder is also referred to as a \"mount point\".\nExit out from the container and list the volumes on the host. They are gone. We used the --rm flag when running the container and this option effectively wipes out not just the container on exit, but also the volumes.\nRun a new container, but specify a volume using -v:\ndocker run --rm -it -v /vol3 my-openjdk\nThis adds a third volume and the whole system ends up having three unnamed volumes. The command would have crashed had we specified only -v vol3. The argument must be an absolute path inside the container. On the host-side, the new third volume is anonymous and resides together with the other two volumes in /var/lib/docker/volumes/.\nIt was stated earlier that the Dockerfile can not map to a host path which sort of pose a problem for us when trying to bring files in from the host to the container during runtime. A different -v syntax solves this problem.\nImagine I have a subfolder in my project directory ./src that I wish to sync to /src inside the container. This command does the trick:\ndocker run -it -v $(pwd)/src:/src my-openjdk\nBoth sides of the : character expects an absolute path. Left side being an absolute path on the host machine, right side being an absolute path inside the container. pwd is a command that \"print current/working directory\". Putting the command in $() takes the command within parenthesis, runs it in a subshell and yields back the absolute path to our project directory.\nPutting it all together, assume we have ./src/Hello.java in our project folder on the host machine with the following contents:\npublic class Hello {\n    public static void main(String... ignored) {\n        System.out.println(\"Hello, World!\");\n    }\n}\nWe build this Dockerfile:\nFROM openjdk:8u131-jdk-alpine\nWORKDIR /src\nENTRYPOINT javac Hello.java && java Hello\nWe run this command:\ndocker run -v $(pwd)/src:/src my-openjdk\nThis prints \"Hello, World!\".\nThe best part is that we're completely free to modify the .java file with a new message for another output on a second run - without having to rebuild the image =)\nFinal remarks\nI am quite new to Docker, and the aforementioned \"tutorial\" reflects information I gathered from a 3-day command line hackathon. I am almost ashamed I haven't been able to provide links to clear English-like documentation backing up my statements, but I honestly think this is due to a lack of documentation and not personal effort. I do know the examples work as advertised using my current setup which is \"Windows 10 -> Vagrant 2.0.0 -> Docker 17.09.0-ce\".\nThe tutorial does not solve the problem \"how do we specify the container's path in the Dockerfile and let the run command only specify the host path\". There might be a way, I just haven't found it.\nFinally, I have a gut feeling that specifying VOLUME in the Dockerfile is not just uncommon, but it's probably a best practice to never use VOLUME. For two reasons. The first reason we have already identified: We can not specify the host path - which is a good thing because Dockerfiles should be very agnostic to the specifics of a host machine. But the second reason is people might forget to use the --rm option when running the container. One might remember to remove the container but forget to remove the volume. Plus, even with the best of human memory, it might be a daunting task to figure out which of all anonymous volumes are safe to remove.",
                "upvotes": 602,
                "answered_by": "Martin Andersson",
                "answered_at": "2017-10-28 17:11"
            },
            {
                "answer": "The official docker tutorial says:\nA data volume is a specially-designated directory within one or more containers that bypasses the Union File System. Data volumes provide several useful features for persistent or shared data:\nVolumes are initialized when a container is created. If the container\u2019s base image contains data at the specified mount point,\nthat existing data is copied into the new volume upon volume\ninitialization. (Note that this does not apply when mounting a host\ndirectory.)\nData volumes can be shared and reused among containers.\nChanges to a data volume are made directly.\nChanges to a data volume will not be included when you update an image.\nData volumes persist even if the container itself is deleted.\nIn Dockerfile you can specify only the destination of a volume inside a container. e.g. /usr/src/app.\nWhen you run a container, e.g. docker run --volume=/opt:/usr/src/app my_image, you may but do not have to specify its mounting point (/opt) on the host machine. If you do not specify --volume argument then the mount point will be chosen automatically, usually under /var/lib/docker/volumes/.",
                "upvotes": 194,
                "answered_by": "Bukharov Sergey",
                "answered_at": "2017-01-30 12:15"
            },
            {
                "answer": "Specifying a VOLUME line in a Dockerfile configures a bit of metadata on your image, but how that metadata is used is important.\nFirst, what did these two lines do:\nWORKDIR /usr/src/app\nVOLUME . /usr/src/app\nThe WORKDIR line there creates the directory if it doesn't exist, and updates some image metadata to specify all relative paths, along with the current directory for commands like RUN will be in that location. The VOLUME line there specifies two volumes, one is the relative path ., and the other is /usr/src/app, both just happen to be the same directory. Most often the VOLUME line only contains a single directory, but it can contain multiple as you've done, or it can be a json formatted array.\nYou cannot specify a volume source in the Dockerfile: A common source of confusion when specifying volumes in a Dockerfile is trying to match the runtime syntax of a source and destination at image build time, this will not work. The Dockerfile can only specify the destination of the volume. It would be a trivial security exploit if someone could define the source of a volume since they could update a common image on the docker hub to mount the root directory into the container and then launch a background process inside the container as part of an entrypoint that adds logins to /etc/passwd, configures systemd to launch a bitcoin miner on next reboot, or searches the filesystem for credit cards, SSNs, and private keys to send off to a remote site.\nWhat does the VOLUME line do? As mentioned, it sets some image metadata to say a directory inside the image is a volume. How is this metadata used? Every time you create a container from this image, docker will force that directory to be a volume. If you do not provide a volume in your run command, or compose file, the only option for docker is to create an anonymous volume. This is a local named volume with a long unique id for the name and no other indication for why it was created or what data it contains (anonymous volumes are were data goes to get lost). If you override the volume, pointing to a named or host volume, your data will go there instead.\nVOLUME breaks things: You cannot disable a volume once defined in a Dockerfile. And more importantly, the RUN command in docker is implemented with temporary containers with the classic builder. Those temporary containers will get a temporary anonymous volume. That anonymous volume will be initialized with the contents of your image. Any writes inside the container from your RUN command will be made to that volume. When the RUN command finishes, changes to the image are saved, and changes to the anonymous volume are discarded. Because of this, I strongly recommend against defining a VOLUME inside the Dockerfile. It results in unexpected behavior for downstream users of your image that wish to extend the image with initial data in volume location.\nHow should you specify a volume? To specify where you want to include volumes with your image, provide a docker-compose.yml. Users can modify that to adjust the volume location to their local environment, and it captures other runtime settings like publishing ports and networking.\nSomeone should document this! They have. Docker includes warnings on the VOLUME usage in their documentation on the Dockerfile along with advice to specify the source at runtime:\nChanging the volume from within the Dockerfile: If any build steps change the data within the volume after it has been declared, those changes will be discarded.\n...\nThe host directory is declared at container run-time: The host directory (the mountpoint) is, by its nature, host-dependent. This is to preserve image portability, since a given host directory can\u2019t be guaranteed to be available on all hosts. For this reason, you can\u2019t mount a host directory from within the Dockerfile. The VOLUME instruction does not support specifying a host-dir parameter. You must specify the mountpoint when you create or run the container.\nThe behavior of defining a VOLUME followed by RUN steps in a Dockerfile has changed with the introduction of buildkit. Here are two examples. First the Dockerfile:\n$ cat df.vol-run \nFROM busybox\n\nWORKDIR /test\nVOLUME /test\nRUN echo \"hello\" >/test/hello.txt \\\n && chown -R nobody:nobody /test\nNext, building without buildkit. Note how the changes from the RUN step are lost:\n$ DOCKER_BUILDKIT=0 docker build -t test-vol-run -f df.vol-run .\nSending build context to Docker daemon  23.04kB\nStep 1/4 : FROM busybox\n ---> beae173ccac6\nStep 2/4 : WORKDIR /test\n ---> Running in aaf2c2920ebd\nRemoving intermediate container aaf2c2920ebd\n ---> 7960bec5b546\nStep 3/4 : VOLUME /test\n ---> Running in 9e2fbe3e594b\nRemoving intermediate container 9e2fbe3e594b\n ---> 5895ddaede1f\nStep 4/4 : RUN echo \"hello\" >/test/hello.txt  && chown -R nobody:nobody /test\n ---> Running in 2c6adff98c70\nRemoving intermediate container 2c6adff98c70\n ---> ef2c30f207b6\nSuccessfully built ef2c30f207b6\nSuccessfully tagged test-vol-run:latest\n\n$ docker run -it test-vol-run /bin/sh\n/test # ls -al \ntotal 8\ndrwxr-xr-x    2 root     root          4096 Mar  6 14:35 .\ndrwxr-xr-x    1 root     root          4096 Mar  6 14:35 ..\n/test # exit\nAnd then building with buildkit. Note how the changes from the RUN step are preserved:\n$ docker build -t test-vol-run -f df.vol-run .\n[+] Building 0.5s (7/7) FINISHED                                                                         \n => [internal] load build definition from df.vol-run                                                0.0s\n => => transferring dockerfile: 154B                                                                0.0s\n => [internal] load .dockerignore                                                                   0.0s\n => => transferring context: 34B                                                                    0.0s\n => [internal] load metadata for docker.io/library/busybox:latest                                   0.0s\n => CACHED [1/3] FROM docker.io/library/busybox                                                     0.0s\n => [2/3] WORKDIR /test                                                                             0.0s\n => [3/3] RUN echo \"hello\" >/test/hello.txt  && chown -R nobody:nobody /test                        0.4s\n => exporting to image                                                                              0.0s\n => => exporting layers                                                                             0.0s\n => => writing image sha256:8cb3220e3593b033778f47e7a3cb7581235e4c6fa921c5d8ce1ab329ebd446b6        0.0s\n => => naming to docker.io/library/test-vol-run                                                     0.0s\n\n$ docker run -it test-vol-run /bin/sh\n/test # ls -al\ntotal 12\ndrwxr-xr-x    2 nobody   nobody        4096 Mar  6 14:34 .\ndrwxr-xr-x    1 root     root          4096 Mar  6 14:34 ..\n-rw-r--r--    1 nobody   nobody           6 Mar  6 14:34 hello.txt\n/test # exit",
                "upvotes": 115,
                "answered_by": "BMitch",
                "answered_at": "2019-04-04 12:49"
            },
            {
                "answer": "To better understand the volume instruction in dockerfile, let us learn the typical volume usage in mysql official docker file implementation.\nVOLUME /var/lib/mysql\nReference: https://github.com/docker-library/mysql/blob/3362baccb4352bcf0022014f67c1ec7e6808b8c5/8.0/Dockerfile\nThe /var/lib/mysql is the default location of MySQL that store data files.\nWhen you run test container for test purpose only, you may not specify its mounting point,e.g.\ndocker run mysql:8\nthen the mysql container instance will use the default mount path which is specified by the volume instruction in dockerfile. the volumes is created with a very long ID-like name inside the Docker root, this is called \"unnamed\" or \"anonymous\" volume. In the folder of underlying host system /var/lib/docker/volumes.\n/var/lib/docker/volumes/320752e0e70d1590e905b02d484c22689e69adcbd764a69e39b17bc330b984e4\nThis is very convenient for quick test purposes without the need to specify the mounting point, but still can get best performance by using Volume for data store, not the container layer.\nFor a formal use, you will need to specify the mount path by using named volume or bind mount, e.g.\ndocker run  -v /my/own/datadir:/var/lib/mysql mysql:8\nThe command mounts the /my/own/datadir directory from the underlying host system as /var/lib/mysql inside the container.The data directory /my/own/datadir won't be automatically deleted, even the container is deleted.\nUsage of the mysql official image (Please check the \"Where to Store Data\" section):\nReference: https://hub.docker.com/_/mysql/",
                "upvotes": 76,
                "answered_by": "Li-Tian",
                "answered_at": "2019-06-14 13:28"
            }
        ]
    },
    {
        "title": "Integrating Python Poetry with Docker",
        "url": "https://stackoverflow.com/questions/53835198/integrating-python-poetry-with-docker",
        "votes": "342",
        "views": "329k",
        "author": "Alex Bodea",
        "issued_at": "2018-12-18 14:28",
        "tags": [
            "python",
            "docker",
            "dockerfile",
            "python-poetry"
        ],
        "answers": [
            {
                "answer": "There are several things to keep in mind when using Poetry together with Docker.\nInstallation\nOfficial way to install Poetry is via:\ncurl -sSL https://install.python-poetry.org | python3 -\nThis way allows Poetry and its dependencies to be isolated from your dependencies.\nYou can also use pip install 'poetry==$POETRY_VERSION'. But, this will install Poetry and its dependencies into your main site-packages/. It might not be ideal.\nAlso, pin this version in your pyproject.toml as well:\n[build-system]\n# Should be the same as `$POETRY_VERSION`:\nrequires = [\"poetry-core>=1.6\"]\nbuild-backend = \"poetry.core.masonry.api\"\nIt will protect you from version mismatch between your local and Docker environments.\nCaching dependencies\nWe want to cache our requirements and only reinstall them when pyproject.toml or poetry.lock files change. Otherwise builds will be slow. To achieve working cache layer we should put:\nCOPY poetry.lock pyproject.toml /code/\nafter Poetry is installed, but before any other files are added.\nVirtualenv\nThe next thing to keep in mind is virtualenv creation. We do not need it in Docker. It is already isolated. So, we use POETRY_VIRTUALENVS_CREATE=false or poetry config virtualenvs.create false setting to turn it off.\nDevelopment vs. Production\nIf you use the same Dockerfile for both development and production as I do, you will need to install different sets of dependencies based on some environment variable:\npoetry install $(test \"$YOUR_ENV\" == production && echo \"--only=main\")\nThis way $YOUR_ENV will control which dependencies set will be installed: all (default) or production only with --only=main flag.\nYou may also want to add some more options for better experience:\n--no-interaction not to ask any interactive questions\n--no-ansi flag to make your output more log friendly\nResult\nYou will end up with something similar to:\nFROM python:3.11.5-slim-bookworm\n\nARG YOUR_ENV\n\nENV YOUR_ENV=${YOUR_ENV} \\\n  PYTHONFAULTHANDLER=1 \\\n  PYTHONUNBUFFERED=1 \\\n  PYTHONHASHSEED=random \\\n  PIP_NO_CACHE_DIR=off \\\n  PIP_DISABLE_PIP_VERSION_CHECK=on \\\n  PIP_DEFAULT_TIMEOUT=100 \\\n  # Poetry's configuration:\n  POETRY_NO_INTERACTION=1 \\\n  POETRY_VIRTUALENVS_CREATE=false \\\n  POETRY_CACHE_DIR='/var/cache/pypoetry' \\\n  POETRY_HOME='/usr/local' \\\n  POETRY_VERSION=1.7.1\n  # ^^^\n  # Make sure to update it!\n\n# System deps:\nRUN curl -sSL https://install.python-poetry.org | python3 -\n\n# Copy only requirements to cache them in docker layer\nWORKDIR /code\nCOPY poetry.lock pyproject.toml /code/\n\n# Project initialization:\nRUN poetry install $(test \"$YOUR_ENV\" == production && echo \"--only=main\") --no-interaction --no-ansi\n\n# Creating folders, and files for a project:\nCOPY . /code\nYou can find a fully working real-life example here.",
                "upvotes": 487,
                "answered_by": "sobolevn",
                "answered_at": "2019-02-19 09:50"
            },
            {
                "answer": "Multi-stage Docker build with Poetry and venv\nUpdate (2024-03-16)\nThis has become much easier over the past years. These days I'd use Poetry's bundle plugin to install the application into a virtual environment, then copy the virtual environment into a distroless image. Install Poetry with pipx, which is packaged by Debian. (You likely want to pin Poetry to avoid breakage when your project isn't compatible with a new Poetry release.) Use the option --only=main when bundling to omit development dependencies.\nFROM debian:12-slim AS builder\nRUN apt-get update && \\\n    apt-get install --no-install-suggests --no-install-recommends --yes pipx\nENV PATH=\"/root/.local/bin:${PATH}\"\nRUN pipx install poetry\nRUN pipx inject poetry poetry-plugin-bundle\nWORKDIR /src\nCOPY . .\nRUN poetry bundle venv --python=/usr/bin/python3 --only=main /venv\n\nFROM gcr.io/distroless/python3-debian12\nCOPY --from=builder /venv /venv\nENTRYPOINT [\"/venv/bin/my-awesome-app\"]\nOriginal Answer\nDo not disable virtualenv creation. Virtualenvs serve a purpose in Docker builds, because they provide an elegant way to leverage multi-stage builds. In a nutshell, your build stage installs everything into the virtualenv, and the final stage just copies the virtualenv over into a small image.\nUse poetry export and install your pinned requirements first, before copying your code. This will allow you to use the Docker build cache, and never reinstall dependencies just because you changed a line in your code.\nDo not use poetry install to install your code, because it will perform an editable install. Instead, use poetry build to build a wheel, and then pip-install that into your virtualenv. (Thanks to PEP 517, this whole process could also be performed with a simple pip install ., but due to build isolation you would end up installing another copy of Poetry.)\nHere's an example Dockerfile installing a Flask app into an Alpine image, with a dependency on Postgres. This example uses an entrypoint script to activate the virtualenv. But generally, you should be fine without an entrypoint script because you can simply reference the Python binary at /venv/bin/python in your CMD instruction.\nDockerfile\nFROM python:3.7.6-alpine3.11 as base\n\nENV PYTHONFAULTHANDLER=1 \\\n    PYTHONHASHSEED=random \\\n    PYTHONUNBUFFERED=1\n\nWORKDIR /app\n\nFROM base as builder\n\nENV PIP_DEFAULT_TIMEOUT=100 \\\n    PIP_DISABLE_PIP_VERSION_CHECK=1 \\\n    PIP_NO_CACHE_DIR=1 \\\n    POETRY_VERSION=1.0.5\n\nRUN apk add --no-cache gcc libffi-dev musl-dev postgresql-dev\nRUN pip install \"poetry==$POETRY_VERSION\"\nRUN python -m venv /venv\n\nCOPY pyproject.toml poetry.lock ./\nRUN poetry export -f requirements.txt | /venv/bin/pip install -r /dev/stdin\n\nCOPY . .\nRUN poetry build && /venv/bin/pip install dist/*.whl\n\nFROM base as final\n\nRUN apk add --no-cache libffi libpq\nCOPY --from=builder /venv /venv\nCOPY docker-entrypoint.sh wsgi.py ./\nCMD [\"./docker-entrypoint.sh\"]\ndocker-entrypoint.sh\n#!/bin/sh\n\nset -e\n\n. /venv/bin/activate\n\nwhile ! flask db upgrade\ndo\n     echo \"Retry...\"\n     sleep 1\ndone\n\nexec gunicorn --bind 0.0.0.0:5000 --forwarded-allow-ips='*' wsgi:app\nwsgi.py\nimport your_app\n\napp = your_app.create_app()",
                "upvotes": 208,
                "answered_by": "Claudio",
                "answered_at": "2019-09-11 09:59"
            },
            {
                "answer": "This is a minor revision to the answer provided by @Claudio, which uses the new poetry install --no-root feature as described by @sobolevn in his answer.\nIn order to force poetry to install dependencies into a specific virtualenv, one needs to first enable it.\n. /path/to/virtualenv/bin/activate && poetry install\nTherefore adding these into @Claudio's answer we have\nFROM python:3.10-slim as base\n\nENV PYTHONFAULTHANDLER=1 \\\n    PYTHONHASHSEED=random \\\n    PYTHONUNBUFFERED=1\n\nWORKDIR /app\n\nFROM base as builder\n\nENV PIP_DEFAULT_TIMEOUT=100 \\\n    PIP_DISABLE_PIP_VERSION_CHECK=1 \\\n    PIP_NO_CACHE_DIR=1 \\\n    POETRY_VERSION=1.3.1\n\nRUN pip install \"poetry==$POETRY_VERSION\"\n\nCOPY pyproject.toml poetry.lock README.md ./\n# if your project is stored in src, uncomment line below\n# COPY src ./src\n# or this if your file is stored in $PROJECT_NAME, assuming `myproject`\n# COPY myproject ./myproject\nRUN poetry config virtualenvs.in-project true && \\\n    poetry install --only=main --no-root && \\\n    poetry build\n\nFROM base as final\n\nCOPY --from=builder /app/.venv ./.venv\nCOPY --from=builder /app/dist .\nCOPY docker-entrypoint.sh .\n\nRUN ./.venv/bin/pip install *.whl\nCMD [\"./docker-entrypoint.sh\"]\nIf you need to use this for development purpose, you add or remove the --no-dev by replacing this line\nRUN . /venv/bin/activate && poetry install --no-dev --no-root\nto something like this as shown in @sobolevn's answer\nRUN . /venv/bin/activate && poetry install --no-root $(test \"$YOUR_ENV\" == production && echo \"--no-dev\")\nafter adding the appropriate environment variable declaration.\nThe example uses debian-slim's as base, however, adapting this to alpine-based image should be a trivial task.",
                "upvotes": 47,
                "answered_by": "Jeffrey04",
                "answered_at": "2020-11-02 08:30"
            },
            {
                "answer": "TL;DR\nI have been able to set up poetry for a Django project using postgres. After doing some research, I ended up with the following Dockerfile:\nFROM python:slim\n\n# Keeps Python from generating .pyc files in the container\nENV PYTHONDONTWRITEBYTECODE 1\n# Turns off buffering for easier container logging\nENV PYTHONUNBUFFERED 1\n\n# Install and setup poetry\nRUN pip install -U pip \\\n    && apt-get update \\\n    && apt install -y curl netcat \\\n    && curl -sSL https://raw.githubusercontent.com/python-poetry/poetry/master/get-poetry.py | python -\nENV PATH=\"${PATH}:/root/.poetry/bin\"\n\nWORKDIR /usr/src/app\nCOPY . .\nRUN poetry config virtualenvs.create false \\\n  && poetry install --no-interaction --no-ansi\n\n# run entrypoint.sh\nENTRYPOINT [\"/usr/src/app/entrypoint.sh\"]\nThis is the content of entrypoint.sh:\n#!/bin/sh\n\nif [ \"$DATABASE\" = \"postgres\" ]\nthen\n    echo \"Waiting for postgres...\"\n\n    while ! nc -z $SQL_HOST $SQL_PORT; do\n      sleep 0.1\n    done\n\n    echo \"PostgreSQL started\"\nfi\n\npython manage.py migrate\n\nexec \"$@\"\nDetailed Explanation\nSome points to notice:\nI have decide to use slim instead of alpine as tag for the python image because even though alpine images are supposed to reduce the size of Docker images and speed up the build, with Python, you can actually end up with a bit larger image and that takes a while to build (read this article for more info).\nUsing this configuration builds containers faster than using the alpine image because I do not need to add some extra packages to install Python packages properly.\nI am installing poetry directly from the URL provided in the documentation. I am aware of the warnings provided by sobolevn. However, I consider that it is better in the long term to use the lates version of poetry by default than relying on an environment variable that I should update periodically.\nUpdating the environment variable PATH is crucial. Otherwise, you will get an error saying that poetry was not found.\nDependencies are installed directly in the python interpreter of the container. It does not create poetry to create a virtual environment before installing the dependencies.\nIn case you need the alpine version of this Dockerfile:\nFROM python:alpine\n\n# Keeps Python from generating .pyc files in the container\nENV PYTHONDONTWRITEBYTECODE 1\n# Turns off buffering for easier container logging\nENV PYTHONUNBUFFERED 1\n\n# Install dev dependencies\nRUN apk update \\\n    && apk add curl postgresql-dev gcc python3-dev musl-dev openssl-dev libffi-dev\n\n# Install poetry\nRUN pip install -U pip \\\n    && curl -sSL https://raw.githubusercontent.com/python-poetry/poetry/master/get-poetry.py | python -\nENV PATH=\"${PATH}:/root/.poetry/bin\"\n\nWORKDIR /usr/src/app\nCOPY . .\nRUN poetry config virtualenvs.create false \\\n  && poetry install --no-interaction --no-ansi\n\n# run entrypoint.sh\nENTRYPOINT [\"/usr/src/app/entrypoint.sh\"]\nNotice that the alpine version needs some dependencies postgresql-dev gcc python3-dev musl-dev openssl-dev libffi-dev to work properly.",
                "upvotes": 32,
                "answered_by": "lmiguelvargasf",
                "answered_at": "2020-05-12 12:28"
            }
        ]
    },
    {
        "title": "How to name Dockerfiles",
        "url": "https://stackoverflow.com/questions/26077543/how-to-name-dockerfiles",
        "votes": "333",
        "views": "273k",
        "author": "Lloyd R. Prentice",
        "issued_at": "2014-09-27 17:56",
        "tags": [
            "docker",
            "dockerfile",
            "naming-conventions"
        ],
        "answers": [
            {
                "answer": "[Please read the full answer]Don't change the name of the dockerfile if you want to use the autobuilder at hub.docker.com. Don't use an extension for docker files, leave it null. File name should just be: (no extension at all)\nDockerfile\nHowever, now you can name dockerfiles like,\ntest1.Dockerfile\n$ docker build -f dockerfiles/test1.Dockerfile  -t test1_app .\nor\nDockerfile.test1\n$ docker build -f dockerfiles/Dockerfile.test1  -t test1_app .\nThis will also work.\nIf you handle multiple files that live in the same context, you could use STDIN:\ntest1.Dockerfile\n$ docker build -t test1_app - < test1.Dockerfile",
                "upvotes": 340,
                "answered_by": "tk_",
                "answered_at": "2014-09-28 10:33"
            },
            {
                "answer": "dev.Dockerfile, test.Dockerfile, build.Dockerfile etc.\nOn VS Code I use <purpose>.Dockerfile and it gets recognized correctly.",
                "upvotes": 190,
                "answered_by": "Sahil Ahuja",
                "answered_at": "2018-02-20 06:36"
            },
            {
                "answer": "I know this is an old question, with quite a few answers, but I was surprised to find that no one was suggesting the naming convention used in the official documentation:\n$ docker build -f dockerfiles/Dockerfile.debug -t myapp_debug .\n$ docker build -f dockerfiles/Dockerfile.prod  -t myapp_prod .\nThe above commands will build the current build context (as specified by the .) twice, once using a debug version of a Dockerfile and once using a production version.\nIn summary, if you have a file called Dockerfile in the root of your build context it will be automatically picked up. If you need more than one Dockerfile for the same build context, the suggested naming convention is:\nDockerfile.<purpose>\nThese dockerfiles could be in the root of your build context or in a subdirectory to keep your root directory more tidy.",
                "upvotes": 96,
                "answered_by": "Erik B",
                "answered_at": "2020-09-21 15:50"
            }
        ]
    },
    {
        "title": "Why is Docker installed but not Docker Compose?",
        "url": "https://stackoverflow.com/questions/36685980/why-is-docker-installed-but-not-docker-compose",
        "votes": "330",
        "views": "711k",
        "author": "mahen3d",
        "issued_at": "2016-04-18 05:22",
        "tags": [
            "docker",
            "docker-compose",
            "dockerfile"
        ],
        "answers": [
            {
                "answer": "You also need to install Docker Compose.\nSee the manual. Here are the commands you need to execute:\nsudo curl -L \"https://github.com/docker/compose/releases/download/v2.33.1/docker-compose-$(uname -s)-$(uname -m)\"  -o /usr/local/bin/docker-compose\nsudo mv /usr/local/bin/docker-compose /usr/bin/docker-compose\nsudo chmod +x /usr/bin/docker-compose      \nNote:\nMake sure that the link pointing to the GitHub release is not outdated!. Check out the latest releases on GitHub.",
                "upvotes": 524,
                "answered_by": "Daniel Stefaniuk",
                "answered_at": "2016-04-18 08:54"
            },
            {
                "answer": "If you installed docker by adding their official repository to your repository list, like:\ncurl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -\n\nsudo add-apt-repository \\\n   \"deb [arch=amd64] https://download.docker.com/linux/ubuntu \\\n   $(lsb_release -cs) \\\n   stable\"\nJust do:\nsudo apt-get install docker-compose\nIn case on RHEL based distro / Fedora:\nsudo dnf install docker-compose",
                "upvotes": 57,
                "answered_by": "Gayan Weerakutti",
                "answered_at": "2018-01-02 07:36"
            },
            {
                "answer": "If you're using ubuntu and docker compose works but docker-compose doesn't, and you need the old docker-compose syntax to be available (maybe a 3rd party library uses it) you can fix it by following these steps:\nthe docker-compose plugin is probably installed under /usr/libexec/docker/cli-plugins/docker-compose (make sure it is)\ncreate a symlink to it:\nsudo ln -s /usr/libexec/docker/cli-plugins/docker-compose /usr/bin/docker-compose\nNow docker-compose should be available\nUpdate:\nIf docker-compose is no where to be found on the mentioned path, you can download it manually from release page for your operating system and then move the downloaded file and make it executable.\ncd ~/Downloads\nsudo mv ./docker-compose-* /usr/local/bin/docker-compose\nsudo chmod +x /usr/local/bin/docker-compose",
                "upvotes": 35,
                "answered_by": "Amin Abdolrezapoor",
                "answered_at": "2022-07-09 12:17"
            }
        ]
    },
    {
        "title": "What is the use of PYTHONUNBUFFERED in docker file?",
        "url": "https://stackoverflow.com/questions/59812009/what-is-the-use-of-pythonunbuffered-in-docker-file",
        "votes": "328",
        "views": "168k",
        "author": "MayankBudhiraja",
        "issued_at": "2020-01-19 16:23",
        "tags": [
            "django",
            "docker",
            "dockerfile"
        ],
        "answers": [
            {
                "answer": "Setting PYTHONUNBUFFERED to a non-empty value different from 0 ensures that the python output i.e. the stdout and stderr streams are sent straight to terminal (e.g. your container log) without being first buffered and that you can see the output of your application (e.g. django logs) in real time.\nThis also ensures that no partial output is held in a buffer somewhere and never written in case the python application crashes.\nSince this has been mentioned in several comments and supplementary answers, note that PYTHONUNBUFFERED has absolutely no influence on the input (i.e. the stdin stream).\nIn other words, turning off buffering to stdout/stderr in a docker container is mainly a concern of getting as much information from your running application as fast as possible in the container log and not loosing anything in case of a crash.\nNote that turning buffering off can have an impact on performance depending on your hardware/environment. Meanwhile it should be minor in most situations (unless you have slow disks or are writing a tremendous amount of logs or had the bad idea to configure your docker daemon to write your logs on a slow network drive...). If this is a concern, buffering can be left on and you can flush the buffer directly from your application when needed. See link [4] below on this subject.\nReferences:\n[1] https://docs.python.org/3/using/cmdline.html#envvar-PYTHONUNBUFFERED\n[2] https://alphacoder.xyz/dockerizing-django/\n[3] https://towardsdatascience.com/how-to-contain-your-first-django-in-docker-and-access-it-from-aws-fdb0081bdf1d\n[4] https://github.com/aws/amazon-sagemaker-examples/issues/319",
                "upvotes": 494,
                "answered_by": "Zeitounator",
                "answered_at": "2020-01-19 17:29"
            },
            {
                "answer": "A PYTHONUNBUFFERED non-empty value forces the stdout and stderr streams to be unbuffered. This option has no effect on the stdin stream.",
                "upvotes": 20,
                "answered_by": "Shreeyansh Jain",
                "answered_at": "2021-10-10 01:18"
            }
        ]
    },
    {
        "title": "Multiple RUN vs. single chained RUN in Dockerfile, which is better?",
        "url": "https://stackoverflow.com/questions/39223249/multiple-run-vs-single-chained-run-in-dockerfile-which-is-better",
        "votes": "325",
        "views": "220k",
        "author": "Yajo",
        "issued_at": "2016-08-30 09:09",
        "tags": [
            "docker",
            "dockerfile"
        ],
        "answers": [
            {
                "answer": "When possible, I always merge together commands that create files with commands that delete those same files into a single RUN line. This is because each RUN line adds a layer to the image, the output is quite literally the filesystem changes that you could view with docker diff on the temporary container it creates. If you delete a file that was created in a different layer, all the union filesystem does is register the filesystem change in a new layer, the file still exists in the previous layer and is shipped over the networked and stored on disk. So if you download source code, extract it, compile it into a binary, and then delete the tgz and source files at the end, you really want this all done in a single layer to reduce image size.\nNext, I personally split up layers based on their potential for reuse in other images and expected caching usage. If I have 4 images, all with the same base image (e.g. debian), I may pull a collection of common utilities to most of those images into the first run command so the other images benefit from caching.\nOrder in the Dockerfile is important when looking at image cache reuse. I look at any components that will update very rarely, possibly only when the base image updates and put those high up in the Dockerfile. Towards the end of the Dockerfile, I include any commands that will run quick and may change frequently, e.g. adding a user with a host specific UID or creating folders and changing permissions. If the container includes interpreted code (e.g. JavaScript) that is being actively developed, that gets added as late as possible so that a rebuild only runs that single change.\nIn each of these groups of changes, I consolidate as best I can to minimize layers. So if there are 4 different source code folders, those get placed inside a single folder so it can be added with a single command. Any package installs from something like apt-get are merged into a single RUN when possible to minimize the amount of package manager overhead (updating and cleaning up).\nUpdate for multi-stage builds:\nI worry much less about reducing image size in the non-final stages of a multi-stage build. When these stages aren't tagged and shipped to other nodes, you can maximize the likelihood of a cache reuse by splitting each command to a separate RUN line.\nHowever, this isn't a perfect solution to squashing layers since all you copy between stages are the files, and not the rest of the image meta-data like environment variable settings, entrypoint, and command. And when you install packages in a linux distribution, the libraries and other dependencies may be scattered throughout the filesystem, making a copy of all the dependencies difficult.\nBecause of this, I use multi-stage builds as a replacement for building binaries on a CI/CD server, so that my CI/CD server only needs to have the tooling to run docker build, and not have a jdk, nodejs, go, and any other compile tools installed.",
                "upvotes": 243,
                "answered_by": "BMitch",
                "answered_at": "2016-09-05 12:17"
            },
            {
                "answer": "Official answer listed in their best practices ( official images MUST adhere to these )\nMinimize the number of layers\nYou need to find the balance between readability (and thus long-term maintainability) of the Dockerfile and minimizing the number of layers it uses. Be strategic and cautious about the number of layers you use.\nSince docker 1.10 the COPY, ADD and RUN statements add a new layer to your image. Be cautious when using these statements. Try to combine commands into a single RUN statement. Separate this only if it's required for readability.\nMore info: https://docs.docker.com/develop/develop-images/dockerfile_best-practices/#minimize-the-number-of-layers\nUpdate: Multi stage in docker >17.05\nWith multi-stage builds you can use multiple FROM statements in your Dockerfile. Each FROM statement is a stage and can have its own base image. In the final stage you use a minimal base image like alpine, copy the build artifacts from previous stages and install runtime requirements. The end result of this stage is your image. So this is where you worry about the layers as described earlier.\nAs usual, docker has great docs on multi-stage builds. Here's a quick excerpt:\nWith multi-stage builds, you use multiple FROM statements in your Dockerfile. Each FROM instruction can use a different base, and each of them begins a new stage of the build. You can selectively copy artifacts from one stage to another, leaving behind everything you don\u2019t want in the final image.\nA great blog post about this can be found here: https://blog.alexellis.io/mutli-stage-docker-builds/\nTo answer your points:\nYes, layers are sort of like diffs. I don't think there are layers added if there's absolutely zero changes. The problem is that once you install / download something in layer #2, you can not remove it in layer #3. So once something is written in a layer, the image size can not be decreased anymore by removing that.\nAlthough layers can be pulled in parallel, making it potentially faster, each layer undoubtedly increases the image size, even if they're removing files.\nYes, caching is useful if you're updating your docker file. But it works in one direction. If you have 10 layers, and you change layer #6, you'll still have to rebuild everything from layer #6-#10. So it's not too often that it will speed the build process up, but it's guaranteed to unnecessarily increase the size of your image.\nThanks to @Mohan for reminding me to update this answer.",
                "upvotes": 87,
                "answered_by": "Menzo Wijmenga",
                "answered_at": "2016-08-30 18:27"
            },
            {
                "answer": "More recent docs note this:\nPrior to Docker 17.05, and even more, prior to Docker 1.10, it was important to minimize the number of layers in your image. The following improvements have mitigated this need:\n[...]\nDocker 17.05 and higher add support for multi-stage builds, which allow you to copy only the artifacts you need into the final image. This allows you to include tools and debug information in your intermediate build stages without increasing the size of the final image.\nand this:\nNotice that this example also artificially compresses two RUN commands together using the Bash && operator, to avoid creating an additional layer in the image. This is failure-prone and hard to maintain.\nThis guidance suggests using multistage builds and keeping the Dockerfiles readable.",
                "upvotes": 43,
                "answered_by": "Mohan",
                "answered_at": "2017-11-23 08:44"
            },
            {
                "answer": "It depends on what you include in your image layers. The key point is sharing as many layers as possible.\nBad Examples\nDockerfile.1\nRUN yum install big-package && yum install package1\nDockerfile.2\nRUN yum install big-package && yum install package2\nGood Examples\nDockerfile.1\nRUN yum install big-package\nRUN yum install package1\nDockerfile.2\nRUN yum install big-package\nRUN yum install package2\nAnother suggestion is deleting is not so useful only if it happens on the same layer as the adding/installing action.",
                "upvotes": 12,
                "answered_by": "xdays",
                "answered_at": "2016-08-30 09:35"
            }
        ]
    },
    {
        "title": "How to pass arguments to a Dockerfile?",
        "url": "https://stackoverflow.com/questions/34254200/how-to-pass-arguments-to-a-dockerfile",
        "votes": "309",
        "views": "364k",
        "author": "meallhour",
        "issued_at": "2015-12-13 17:42",
        "tags": [
            "docker",
            "dockerfile",
            "docker-compose",
            "docker-registry",
            "dockerhub"
        ],
        "answers": [
            {
                "answer": "As of Docker 1.9, You are looking for --build-arg and the ARG instruction.\nCheck out this document for reference. This will allow you to add ARG arg to the Dockerfile and then build with\ndocker build --build-arg arg=2.3 .",
                "upvotes": 442,
                "answered_by": "Andy Shinn",
                "answered_at": "2015-12-13 18:28"
            }
        ]
    },
    {
        "title": "How do I Docker COPY as non root?",
        "url": "https://stackoverflow.com/questions/44766665/how-do-i-docker-copy-as-non-root",
        "votes": "306",
        "views": "169k",
        "author": "FGreg",
        "issued_at": "2017-06-26 18:48",
        "tags": [
            "docker",
            "dockerfile"
        ],
        "answers": [
            {
                "answer": "For versions v17.09.0-ce and newer\nUse the optional flag --chown=<user>:<group> with either the ADD or COPY commands.\nFor example\nCOPY --chown=<user>:<group> <hostPath> <containerPath>\nThe documentation for the --chown flag is now live on the main Dockerfile Reference page.\nIssue 34263 has been merged and is available in release v17.09.0-ce.\nFor versions older than v17.09.0-ce\nDocker doesn't support COPY as a user other than root. You need to chown / chmod the file after the COPY command.\nExample Dockerfile:\nfrom centos:6\nRUN groupadd -r myuser && adduser -r -g myuser myuser\nUSER myuser\n#Install code, configure application, etc...\nUSER root\nCOPY run-my-app.sh /usr/local/bin/run-my-app.sh\nRUN chown myuser:myuser /usr/local/bin/run-my-app.sh && \\\n    chmod 744 /usr/local/bin/run-my-app.sh\nUSER myuser\nENTRYPOINT [\"/usr/local/bin/run-my-app.sh\"]\nPrevious to v17.09.0-ce, the Dockerfile Reference for the COPY command said:\nAll new files and directories are created with a UID and GID of 0.\nHistory This feature has been tracked through multiple GitHub issues: 6119, 9943, 13600, 27303, 28499, Issue 30110.\nIssue 34263 is the issue that implemented the optional flag functionality and Issue 467 updated the documentation.",
                "upvotes": 466,
                "answered_by": "FGreg",
                "answered_at": "2017-06-26 18:48"
            }
        ]
    },
    {
        "title": "Connect to mysql in a docker container from the host",
        "url": "https://stackoverflow.com/questions/33001750/connect-to-mysql-in-a-docker-container-from-the-host",
        "votes": "299",
        "views": "598k",
        "author": "gturri",
        "issued_at": "2015-10-07 20:21",
        "tags": [
            "mysql",
            "docker",
            "dockerfile"
        ],
        "answers": [
            {
                "answer": "If your Docker MySQL host is running correctly you can connect to it from local machine, but you should specify host, port and protocol like this:\nmysql -h localhost -P 3306 --protocol=tcp -u root\nChange 3306 to port number you have forwarded from Docker container (in your case it will be 12345).\nBecause you are running MySQL inside Docker container, socket is not available and you need to connect through TCP. Setting \"--protocol\" in the mysql command will change that.",
                "upvotes": 383,
                "answered_by": "jozala",
                "answered_at": "2015-12-28 08:11"
            }
        ]
    },
    {
        "title": "standard_init_linux.go:190: exec user process caused \"no such file or directory\" - Docker",
        "url": "https://stackoverflow.com/questions/51508150/standard-init-linux-go190-exec-user-process-caused-no-such-file-or-directory",
        "votes": "291",
        "views": "292k",
        "author": "gamechanger17",
        "issued_at": "2018-07-24 22:03",
        "tags": [
            "docker",
            "dockerfile",
            "docker-for-windows"
        ],
        "answers": [
            {
                "answer": "Use notepad++, go to edit -> EOL conversion -> change from CRLF to LF.\nupdate: For VScode users: you can change CRLF to LF by clicking on CRLF present on lower right side in the status bar",
                "upvotes": 507,
                "answered_by": "Vikas Rathore",
                "answered_at": "2018-10-05 12:30"
            },
            {
                "answer": "change entry point as below. It worked for me\nENTRYPOINT [\"sh\",\"/run.sh\"]\nAs tuomastik pointed out in the comments, the docs require the first parameter to be the executable:\nENTRYPOINT has two forms:\nENTRYPOINT [\"executable\", \"param1\", \"param2\"] (exec form, preferred)\nENTRYPOINT command param1 param2 (shell form)",
                "upvotes": 135,
                "answered_by": "prity",
                "answered_at": "2018-08-07 05:29"
            }
        ]
    },
    {
        "title": "/bin/sh: apt-get: not found",
        "url": "https://stackoverflow.com/questions/45142855/bin-sh-apt-get-not-found",
        "votes": "281",
        "views": "447k",
        "author": "H\u00fcseyin Orkun Elmas",
        "issued_at": "2017-07-17 11:18",
        "tags": [
            "bash",
            "docker",
            "dockerfile",
            "aspell"
        ],
        "answers": [
            {
                "answer": "The image you're using is Alpine based, so you can't use apt-get because it's Ubuntu's package manager.\nTo fix this just use:\napk update and apk add",
                "upvotes": 703,
                "answered_by": "Serey",
                "answered_at": "2017-07-17 11:29"
            },
            {
                "answer": "If you are looking inside dockerfile while creating image, add this line:\nRUN apk add --update yourPackageName",
                "upvotes": 89,
                "answered_by": "Venu Gopal Tewari",
                "answered_at": "2019-05-20 09:49"
            }
        ]
    },
    {
        "title": "How can I use a variable inside a Dockerfile CMD?",
        "url": "https://stackoverflow.com/questions/40454470/how-can-i-use-a-variable-inside-a-dockerfile-cmd",
        "votes": "280",
        "views": "196k",
        "author": "david",
        "issued_at": "2016-11-06 20:59",
        "tags": [
            "docker",
            "dockerfile",
            "docker-cmd"
        ],
        "answers": [
            {
                "answer": "When you use an execution list, as in...\nCMD [\"django-admin\", \"startproject\", \"$PROJECTNAME\"]\n...then Docker will execute the given command directly, without involving a shell. Since there is no shell involved, that means:\nNo variable expansion\nNo wildcard expansion\nNo i/o redirection with >, <, |, etc\nNo multiple commands via command1; command2\nAnd so forth.\nIf you want your CMD to expand variables, you need to arrange for a shell. You can do that like this:\nCMD [\"sh\", \"-c\", \"django-admin startproject $PROJECTNAME\"]\nOr you can use a simple string instead of an execution list, which gets you a result largely identical to the previous example:\nCMD django-admin startproject $PROJECTNAME",
                "upvotes": 387,
                "answered_by": "larsks",
                "answered_at": "2016-11-06 21:30"
            },
            {
                "answer": "If you want to use the value at runtime, set the ENV value in the Dockerfile. If you want to use it at build-time, then you should use ARG.\nExample :\nARG value\nENV envValue=$value\nCMD [\"sh\", \"-c\", \"java -jar ${envValue}.jar\"]\nPass the value in the build command:\ndocker build -t tagName --build-arg value=\"jarName\"",
                "upvotes": 60,
                "answered_by": "rex roy",
                "answered_at": "2019-02-03 15:16"
            },
            {
                "answer": "Lets say you want to start a java process inside a container:\nExample Dockerfile excerpt:\nENV JAVA_OPTS -XX +UnlockExperimentalVMOptions -XX:+UseCGroupMemoryLimitForHeap -XX:MaxRAMFraction=1 -XshowSettings:vm \n... \nENTRYPOINT [\"/sbin/tini\", \"--\", \"entrypoint.sh\"] \nCMD [\"java\", \"${JAVA_OPTS}\", \"-myargument=true\"]\nExample entrypoint.sh excerpt:\n#!/bin/sh \n... \necho \"*** Startup $0 suceeded now starting service using eval to expand CMD variables ***\"\nexec su-exec mytechuser $(eval echo \"$@\")",
                "upvotes": 13,
                "answered_by": "Flavio",
                "answered_at": "2017-09-08 18:06"
            }
        ]
    },
    {
        "title": "How do I set environment variables during the \"docker build\" process?",
        "url": "https://stackoverflow.com/questions/39597925/how-do-i-set-environment-variables-during-the-docker-build-process",
        "votes": "279",
        "views": "421k",
        "author": "Micha\u0142 Pietraszko",
        "issued_at": "2016-09-20 15:20",
        "tags": [
            "docker",
            "dockerfile",
            "environment-variables",
            "docker-build"
        ],
        "answers": [
            {
                "answer": "ARG is for setting environment variables which are used during the docker build process - they are not present in the final image, which is why you don't see them when you use docker run.\nYou use ARG for settings that are only relevant when the image is being built, and aren't needed by containers which you run from the image. You can use ENV for environment variables to use during the build and in containers.\nWith this Dockerfile:\nFROM ubuntu\nARG BUILD_TIME=abc\nENV RUN_TIME=123\nRUN touch /env.txt\nRUN printenv > /env.txt\nYou can override the build arg as you have done with docker build -t temp --build-arg BUILD_TIME=def .. Then you get what you expect:\n> docker run temp cat /env.txt                                                                                         \nHOSTNAME=b18b9cafe0e0                                                                                                  \nRUN_TIME=123                                                                                                           \nHOME=/root                                                                                                             \nPATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin                                                      \nBUILD_TIME=def                                                                                                         \nPWD=/ ",
                "upvotes": 289,
                "answered_by": "Elton Stoneman",
                "answered_at": "2016-09-20 15:57"
            }
        ]
    },
    {
        "title": "Multiple FROMs - what it means",
        "url": "https://stackoverflow.com/questions/33322103/multiple-froms-what-it-means",
        "votes": "278",
        "views": "300k",
        "author": "ekkis",
        "issued_at": "2015-10-24 18:58",
        "tags": [
            "docker",
            "dockerfile"
        ],
        "answers": [
            {
                "answer": "As of May 2017, multiple FROMs can be used in a single Dockerfile.\nSee \"Builder pattern vs. Multi-stage builds in Docker\" (by Alex Ellis) and PR 31257 by T\u00f5nis Tiigi.\nThe general syntax involves adding FROM additional times within your Dockerfile - whichever is the last FROM statement is the final base image. To copy artifacts and outputs from intermediate images use COPY --from=<base_image_number>.\nFROM golang:1.7.3 as builder\nWORKDIR /go/src/github.com/alexellis/href-counter/\nRUN go get -d -v golang.org/x/net/html  \nCOPY app.go    .\nRUN CGO_ENABLED=0 GOOS=linux go build -a -installsuffix cgo -o app .\n\nFROM alpine:latest  \nRUN apk --no-cache add ca-certificates\nWORKDIR /root/\nCOPY --from=builder /go/src/github.com/alexellis/href-counter/app    .\nCMD [\"./app\"]  \nThe result would be two images, one for building, one with just the resulting app (much, much smaller)\nREPOSITORY          TAG                 IMAGE ID            CREATED             SIZE\n\nmulti               latest              bcbbf69a9b59        6 minutes ago       10.3MB  \ngolang              1.7.3               ef15416724f6        4 months ago        672MB  \nwhat is a base image?\nA set of files, plus EXPOSE'd ports, ENTRYPOINT and CMD.\nYou can add files and build a new image based on that base image, with a new Dockerfile starting with a FROM directive: the image mentioned after FROM is \"the base image\" for your new image.\ndoes it mean that if I declare neo4j/neo4j in a FROM directive, that when my image is run the neo database will automatically run and be available within the container on port 7474?\nOnly if you don't overwrite CMD and ENTRYPOINT.\nBut the image in itself is enough: you would use a FROM neo4j/neo4j if you had to add files related to neo4j for your particular usage of neo4j.\n2018: With the introduction of the --target option in docker build, you gain even more control over multi-stage builds.\nThis feature enables you to select which FROM statement in your Dockerfile you wish to build, allowing for more modular and efficient Docker images. This is especially useful in scenarios where you might want to:\nBuild Only the Dependencies: Create an image that only contains the dependencies of your project. This can be useful for caching purposes or for environments where you only need to run tests or static analysis tools.\nSeparate Build and Runtime Environments: Compile or build your application in a full-featured build environment but create a smaller, more secure image for deployment that only includes the runtime environment and the compiled application.\nCreate Images for Different Environments: Have different stages for development, testing, and production environments, each tailored with the specific tools and configurations needed for those environments.\nExample Using --target\nGiven a Dockerfile with multiple stages named builder, tester, and deployer, you can build up to the tester stage using the --target option like so:\ndocker build --target tester -t myapp-test .\nThis command tells Docker to stop building after the tester stage has been completed, thus creating an image that includes everything from the base image up to the tester stage, but excluding anything from deployer stage and beyond.\nDockerfile Example with --target Usage\n# Builder stage\nFROM golang:1.7.3 as builder\nWORKDIR /go/src/github.com/example/project/\n# Assume app.go exists and has a function\nCOPY app.go .\nRUN CGO_ENABLED=0 GOOS=linux go build -a -installsuffix cgo -o app .\n\n# Tester stage\nFROM builder as tester\nCOPY . .\nRUN go test ./...\n\n# Deployer stage\nFROM alpine:latest as deployer\nCOPY --from=builder /go/src/github.com/example/project/app /app\nCMD [\"/app\"]\nUsing the --target option with this Dockerfile allows for flexibility in building images tailored for specific steps of the development lifecycle.\nAs illustrated in \"Building a multi-stage Dockerfile with --target flag builds all stages instead of just the specified one\", this works well with BuildKit, which is now (2023+) the default builder.\nFrom that page, you have Igor Kulebyakin's answer:\nIf one wants to make sure that the current target stage is force re-built even if it has already been cached without rebuilding the previous dependent stages, once can use the docker build --no-cache-filter flag.\nAn example, given you have a multi-stage Dockerfile with a 'test' stage, would be:\ndocker build --no-cache-filter test --target test --tag your-image-name:version .",
                "upvotes": 256,
                "answered_by": "VonC",
                "answered_at": "2015-10-24 19:25"
            },
            {
                "answer": "Let me summarize my understanding of the question and the answer, hoping that it will be useful to others.\nQuestion: Let\u2019s say I have three images, apple, banana and orange. Can I have a Dockerfile that has FROM apple, FROM banana and FROM orange that will tell docker to magically merge all three applications into a single image (containing the three individual applications) which I could call smoothie?\nAnswer: No, you can't. If you do that, you will end up with four images, the three fruit images you pulled, plus the new image based on the last FROM image. If, for example, FROM orange was the last statement in the Dockerfile without anything added, the smoothie image would just be a clone of the orange image.\nWhy Are They Not Merged? I Really Want It\nA typical docker image will contain almost everything the application needs to run (leaving out the kernel) which usually means that they\u2019re built from a base image for their chosen operating system and a particular version or distribution.\nMerging images successfully without considering all possible distributions, file systems, libraries and applications, is not something Docker, understandably, wants to do. Instead, developers are expected to embrace the microservices paradigm, running multiple containers that talk to each other as needed.\nWhat\u2019s the Alternative?\nOne possible use case for image merging would be to mix and match Linux distributions with our desired applications, for example, Ubuntu and Node.js. This is not the solution:\nFROM ubuntu\nFROM node\nIf we don\u2019t want to stick with the Linux distribution chosen by our application image, we can start with our chosen distribution and use the package manager to install the applications instead, e.g.\nFROM ubuntu\nRUN apt-get update &&\\\n    apt-get install package1 &&\\\n    apt-get install package2\nBut you probably knew that already. Often times there isn\u2019t a snap or package available in the chosen distribution, or it\u2019s not the desired version, or it doesn't work well in a docker container out of the box, which was the motivation for wanting to use an image. I\u2019m just confirming that, as far as I know, the only option is to do it the long way, if you really want to follow a monolithic approach.\nIn the case of Node.js for example, you might want to manually install the latest version, since apt provides an ancient one, and snap does not come with the Ubuntu image. For neo4j we might have to download the package and manually add it to the image, according to the documentation and the license.\nOne strategy, if size does not matter, is to start with the base image that would be hardest to install manually, and add the rest on top.\nWhen To Use Multiple FROM Directives\nThere is also the option to use multiple FROM statements and manually copy stuff between build stages or into your final one. In other words, you can manually merge images, if you know what you're doing. As per the documentation:\nOptionally a name can be given to a new build stage by adding AS name to the FROM instruction. The name can be used in subsequent FROM and COPY --from=<name> instructions to refer to the image built in this stage.\nPersonally, I\u2019d only be comfortable using this merge approach with my own images or by following documentation from the application vendor, but it\u2019s there if you need it or you're just feeling lucky.\nA better application of this approach though, would be when we actually do want to use a temporary container from a different image, for building or doing something and discard it after copying the desired output.\nExample\nI wanted a lean image with gpgv only, and based on this Unix & Linux answer, I installed the whole gpg with yum and then copied only the binaries required, to the final image:\nFROM docker.io/photon:latest AS builder\nRUN yum install gnupg -y\n\nFROM docker.io/photon:latest\nCOPY --from=builder /usr/bin/gpgv /usr/bin/\nCOPY --from=builder /usr/lib/libgcrypt.so.20 /usr/lib/libgpg-error.so.0 /usr/lib/\nThe rest of the Dockerfile continues as usual.",
                "upvotes": 108,
                "answered_by": "Nagev",
                "answered_at": "2021-10-15 11:47"
            },
            {
                "answer": "The first answer is too complex, historic, and uninformative for my tastes.\nIt's actually rather simple. Docker provides for a functionality called multi-stage builds the basic idea here is to,\nFree you from having to manually remove what you don't want, by forcing you to allowlist what you do want,\nFree resources that would otherwise be taken up because of Docker's implementation.\nLet's start with the first. Very often with something like Debian you'll see.\nRUN apt-get update \\ \n  && apt-get dist-upgrade \\\n  && apt-get install <whatever> \\\n  && apt-get clean\nWe can explain all of this in terms of the above. The above command is chained together so it represents a single change with no intermediate Images required. If it was written like this,\nRUN apt-get update ;\nRUN apt-get dist-upgrade;\nRUN apt-get install <whatever>;\nRUN apt-get clean;\nIt would result in 3 more temporary intermediate Images. Having it reduced to one image, there is one remaining problem: apt-get clean doesn't clean up artifacts used in the install. If a Debian maintainer includes in his install a script that modifies the system that modification will also be present in the final solution (see something like pepperflashplugin-nonfree for an example of that).\nBy using a multi-stage build you get all the benefits of a single changed action, but it will require you to manually allowlist and copy over files that were introduced in the temporary image using the COPY --from syntax documented here. Moreover, it's a great solution where there is no alternative (like an apt-get clean), and you would otherwise have lots of un-needed files in your final image.\nSee also\nMulti-stage builds\nCOPY syntax",
                "upvotes": 33,
                "answered_by": "Evan Carroll",
                "answered_at": "2020-07-12 19:26"
            }
        ]
    },
    {
        "title": "what is docker run -it flag?",
        "url": "https://stackoverflow.com/questions/48368411/what-is-docker-run-it-flag",
        "votes": "258",
        "views": "228k",
        "author": "Alex",
        "issued_at": "2018-01-21 15:27",
        "tags": [
            "docker",
            "dockerfile",
            "command-line-arguments"
        ],
        "answers": [
            {
                "answer": "-it is short for --interactive + --tty. When you docker run with this command it takes you straight inside the container.\n-d is short for --detach, which means you just run the container and then detach from it. Essentially, you run container in the background.\nEdit: So if you run the Docker container with -itd, it runs both the -it options and detaches you from the container. As a result, your container will still be running in the background even without any default app to run.",
                "upvotes": 235,
                "answered_by": "Fendi jatmiko",
                "answered_at": "2018-01-21 15:47"
            },
            {
                "answer": "I want to add some intuition for newbies like me.\n-it are flags for command docker run or docker container run (they are aliases). Suggest you know what are flags and go forward:\n-i or --interactive:\nWhen you type docker run -i this means that your terminal will transfer your input to container (app in container) until you press ctrl-D (leave container). For example, if some app works in container that waits for user input you can type something and that will be forwarded to the app.\n-t or -tty (Pseudo-TTY):\nIf you add this flag, your container's output is attached to your terminal. Seems it mostly about formatting output (for bin/bash, for example - try ls with and without -t flag), but sometimes is more important because some apps change their behaviour depending on being launched via terminal or not (text editors, for example, or mechanism of masking password with * implemented by terminal).\nCombining two flags as -it gives you opportunity to make your container get your stdin and get nice formatted output from container like you are working with your nice native own terminal.\nFollow-ups:\nHow does docker understand which app will get my stdin (standard input)?\nWhen you type docker run -i <image> <app>, last argument (<app>) will get your input.\nWhat are practical applications of both flags alone?\n-i is useful when you want to interact with app in container but it's not important for you to get formatted output. For example, you want to send your data to container, get data from it and save it to file. In such case -t is not necessary.\necho \"my input\" | docker run -i <image> > output.txt\n-t is useful when you want terminal-like output but do not need your input to be transferred to container. Say you have script that launches inside container, outputs data in specific format and after that data is used by another script:\ndocker run -t my-image | my-processing-script.sh",
                "upvotes": 31,
                "answered_by": "Iskander14yo",
                "answered_at": "2023-10-24 20:18"
            }
        ]
    },
    {
        "title": "How do I use Docker environment variable in ENTRYPOINT array?",
        "url": "https://stackoverflow.com/questions/37904682/how-do-i-use-docker-environment-variable-in-entrypoint-array",
        "votes": "255",
        "views": "254k",
        "author": "Psycho Punch",
        "issued_at": "2016-06-19 06:49",
        "tags": [
            "docker",
            "dockerfile"
        ],
        "answers": [
            {
                "answer": "You're using the exec form of ENTRYPOINT. Unlike the shell form, the exec form does not invoke a command shell. This means that normal shell processing does not happen. For example, ENTRYPOINT [ \"echo\", \"$HOME\" ] will not do variable substitution on $HOME. If you want shell processing then either use the shell form or execute a shell directly, for example: ENTRYPOINT [ \"sh\", \"-c\", \"echo $HOME\" ].\nWhen using the exec form and executing a shell directly, as in the case for the shell form, it is the shell that is doing the environment variable expansion, not docker.(from Dockerfile reference)\nIn your case, I would use shell form\nENTRYPOINT ./greeting --message \"Hello, $ADDRESSEE\\!\"",
                "upvotes": 401,
                "answered_by": "vitr",
                "answered_at": "2016-06-19 07:13"
            },
            {
                "answer": "After much pain, and great assistance from @vitr et al above, i decided to try\nstandard bash substitution\nshell form of ENTRYPOINT (great tip from above)\nand that worked.\nENV LISTEN_PORT=\"\"\n\nENTRYPOINT java -cp \"app:app/lib/*\" hello.Application --server.port=${LISTEN_PORT:-80}\ne.g.\ndocker run --rm -p 8080:8080 -d --env LISTEN_PORT=8080 my-image\nand\ndocker run --rm -p 8080:80 -d my-image\nboth set the port correctly in my container\nRefs\nsee https://www.cyberciti.biz/tips/bash-shell-parameter-substitution-2.html",
                "upvotes": 38,
                "answered_by": "mlo55",
                "answered_at": "2019-05-27 10:18"
            },
            {
                "answer": "I tried to resolve with the suggested answer and still ran into some issues...\nThis was a solution to my problem:\nARG APP_EXE=\"AppName.exe\"\nENV _EXE=${APP_EXE}\n\n# Build a shell script because the ENTRYPOINT command doesn't like using ENV\nRUN echo \"#!/bin/bash \\n mono ${_EXE}\" > ./entrypoint.sh\nRUN chmod +x ./entrypoint.sh\n\n# Run the generated shell script.\nENTRYPOINT [\"./entrypoint.sh\"]\nSpecifically targeting your problem:\nRUN echo \"#!/bin/bash \\n ./greeting --message ${ADDRESSEE}\" > ./entrypoint.sh\nRUN chmod +x ./entrypoint.sh\nENTRYPOINT [\"./entrypoint.sh\"]",
                "upvotes": 33,
                "answered_by": "Ben Kauffman",
                "answered_at": "2018-04-17 23:57"
            },
            {
                "answer": "I SOLVED THIS VERY SIMPLY!\nIMPORTANT: The variable which you wish to use in the ENTRYPOINT MUST be ENV type (and not ARG type).\nEXAMPLE #1:\nARG APP_NAME=app.jar                    # $APP_NAME can be ARG or ENV type.\nENV APP_PATH=app-directory/$APP_NAME    # $APP_PATH must be ENV type.\nENTRYPOINT java -jar $APP_PATH\nThis will result with executing: java -jar app-directory/app.jar\nEXAMPLE #2 (YOUR QUESTION):\nARG ADDRESSEE=\"world\"                       # $ADDRESSEE can be ARG or ENV type.\nENV MESSAGE=\"Hello, $ADDRESSEE!\"            # $MESSAGE must be ENV type.\nENTRYPOINT ./greeting --message $MESSAGE\nThis will result with executing: ./greeting --message Hello, world!\nPlease verify to be sure, whether you need quotation-marks \"\" when assigning string variables.\nMY TIP: Use ENV instead of ARG whenever possible to avoid confusion on your part or the SHELL side.",
                "upvotes": 19,
                "answered_by": "Tal Jacob - Sir Jacques",
                "answered_at": "2021-09-23 14:00"
            }
        ]
    },
    {
        "title": "Conditional COPY/ADD in Dockerfile?",
        "url": "https://stackoverflow.com/questions/31528384/conditional-copy-add-in-dockerfile",
        "votes": "253",
        "views": "224k",
        "author": "derrend",
        "issued_at": "2015-07-21 00:16",
        "tags": [
            "docker",
            "dockerfile"
        ],
        "answers": [
            {
                "answer": "2025 Update:\nIn current versions of docker, simple use a wildcard like this\nCOPY file-which-may-or-may-not-exist* .\nDocker will copy the file if it exists, and will copy nothing if it doesn't exist.\nFor older versions where this fails, use this workaround:\nCOPY foo file-which-may-exist* /target\nMake sure foo exists, since COPY needs at least one valid source.\nIf file-which-may-exist is present, it will also be copied.\nNOTE: You should take care to ensure that your wildcard doesn't pick up other files which you don't intend to copy. To be more careful, you could use file-which-may-exist? instead (? matches just a single character).\nOr even better, use a character class like this to ensure that only one file can be matched:\nCOPY foo file-which-may-exis[t] /target",
                "upvotes": 198,
                "answered_by": "jdhildeb",
                "answered_at": "2017-10-18 02:36"
            },
            {
                "answer": "As stated by this comment, Santhosh Hirekerur's answer still copies the file, to achieve a true conditional copy, you can use this method.\nARG BUILD_ENV=copy\n\nFROM alpine as build_copy\nONBUILD COPY file /file\n\nFROM alpine as build_no_copy\nONBUILD RUN echo \"I don't copy\"\n\nFROM build_${BUILD_ENV}\n# other stuff\nThe ONBUILD instructions ensures that the file is only copied if the \"branch\" is selected by the BUILD_ENV. Set this var using a little script before calling docker build",
                "upvotes": 132,
                "answered_by": "Siyu",
                "answered_at": "2019-01-17 22:55"
            },
            {
                "answer": "2021+, from this answer, using glob pattern actually Go filepath patterns, Docker COPY will not fail if it won't locate any valid source\nCOPY requiements.tx[t] /destination\n2015: This isn't currently supported (as I suspect it would lead to a non-reproducible image, since the same Dockerfile would copy or not the file, depending on its existence).\nThis is still requested, in issue 13045, using wildcards: \"COPY foo/* bar/\" not work if no file in foo\" (May 2015).\nIt won't be implemented for now (July 2015) in Docker, but another build tool like bocker could support this.\n2021:\nCOPY source/. /source/ works for me (i.e. copies directory when empty or not, as in \"Copy directory into docker build no matter if empty or not - fails on \"COPY failed: no source files were specified\"\")\n2022\nHere is my suggestion:\n# syntax=docker/dockerfile:1.2\n\nRUN --mount=type=bind,source=jars,target=/build/jars \\\n find /build/jars -type f -name '*.jar' -maxdepth 1  -print0 \\\n | xargs -0 --no-run-if-empty --replace=source cp --force source >\"${INSTALL_PATH}/modules/\"\nThat works around:\nCOPY jars/*.jar \"${INSTALL_PATH}/modules/\"\nBut copies no *.jar if none is found, without throwing an error.\nQ2 2024, regarding the 2022 solution, x-yuri adds in the comments:\nThe idea is clear, but your 2022 solution doesn't work. b.sh.\nSolution that works. c.sh is like b.sh but w/o xargs.\nset -eux\nrm -rf a\nmkdir a\ncd a\n\nmkdir src\ntouch src/a\n# touch src/b.txt\n# touch src/c.txt\ncat <<\\EOF >Dockerfile\nFROM debian:bookworm-slim\nRUN mkdir dst\nRUN --mount=type=bind,target=/mnt \\\n  find /mnt/src -maxdepth 1 -type f -name '*.txt' -exec cp -t dst {} +\nEOF\ndocker build -t i --progress=plain .\ndocker run --rm i ls -A dst",
                "upvotes": 84,
                "answered_by": "VonC",
                "answered_at": "2015-07-21 07:20"
            },
            {
                "answer": "I think I came up with a valid workaround with this Dockerfile\nFROM alpine\nCOPy always_exist_on_host.txt .\nCOPY *sometimes_exist_on_host.txt .\nThe always_exist_on_host.txt file will always be copied to the image and the build won't fail to COPY the sometimes_exist_on_host.txt file when it doesn't exist. Furthermore, it will COPY the sometimes_exist_on_host.txt file when it does exist.\nFor example:\n.\n\u251c\u2500\u2500 Dockerfile\n\u2514\u2500\u2500 always_exist_on_host.txt\nbuild succeeds\ndocker build . -t copy-when-exists --no-cache\n[+] Building 1.0s (7/7) FINISHED                                                                                                                            \n => [internal] load .dockerignore                                                                                                                      0.0s\n => => transferring context: 2B                                                                                                                        0.0s\n => [internal] load build definition from Dockerfile                                                                                                   0.0s\n => => transferring dockerfile: 36B                                                                                                                    0.0s\n => [internal] load metadata for docker.io/library/alpine:latest                                                                                       1.0s\n => [internal] load build context                                                                                                                      0.0s\n => => transferring context: 43B                                                                                                                       0.0s\n => CACHED [1/2] FROM docker.io/library/alpine@sha256:c0e9560cda118f9ec63ddefb4a173a2b2a0347082d7dff7dc14272e7841a5b5a                                 0.0s\n => [2/2] COPY always_exist_on_host.txt *sometimes_exist_on_host.txt .                                                                                 0.0s\n => exporting to image                                                                                                                                 0.0s\n => => exporting layers                                                                                                                                0.0s\n => => writing image sha256:e7d02c6d977f43500dbc1c99d31e0a0100bb2a6e5301d8cd46a19390368f4899                                                           0.0s               \n.\n\u251c\u2500\u2500 Dockerfile\n\u251c\u2500\u2500 always_exist_on_host.txt\n\u2514\u2500\u2500 sometimes_exist_on_host.txt\nbuild still succeeds\ndocker build . -t copy-when-exists --no-cache\n[+] Building 1.0s (7/7) FINISHED                                                                                                                            \n => [internal] load build definition from Dockerfile                                                                                                   0.0s\n => => transferring dockerfile: 36B                                                                                                                    0.0s\n => [internal] load .dockerignore                                                                                                                      0.0s\n => => transferring context: 2B                                                                                                                        0.0s\n => [internal] load metadata for docker.io/library/alpine:latest                                                                                       0.9s\n => [internal] load build context                                                                                                                      0.0s\n => => transferring context: 91B                                                                                                                       0.0s\n => CACHED [1/2] FROM docker.io/library/alpine@sha256:c0e9560cda118f9ec63ddefb4a173a2b2a0347082d7dff7dc14272e7841a5b5a                                 0.0s\n => [2/2] COPY always_exist_on_host.txt *sometimes_exist_on_host.txt .                                                                                 0.0s\n => exporting to image                                                                                                                                 0.0s\n => => exporting layers                                                                                                                                0.0s\n => => writing image sha256:4c88e2ffa77ebf6869af3c7ca2a0cfb9461979461fc3ae133709080b5abee8ff                                                           0.0s\n => => naming to docker.io/library/copy-when-exists                                                                                                    0.0s",
                "upvotes": 45,
                "answered_by": "aidanmelen",
                "answered_at": "2020-12-04 04:49"
            }
        ]
    },
    {
        "title": "Alpine Dockerfile advantages of --no-cache vs. rm /var/cache/apk/*",
        "url": "https://stackoverflow.com/questions/49118579/alpine-dockerfile-advantages-of-no-cache-vs-rm-var-cache-apk",
        "votes": "244",
        "views": "172k",
        "author": "Angel S. Moreno",
        "issued_at": "2018-03-05 20:06",
        "tags": [
            "docker",
            "dockerfile",
            "alpine-linux",
            "alpine-package-keeper"
        ],
        "answers": [
            {
                "answer": "I think this is a design style. The essence of cache is to reuse, for example, multiple containers can mount the same cached file system without repeatedly downloading it from the network.\nCan view the Alpine wiki: https://wiki.alpinelinux.org/wiki/Alpine_Linux_package_management#Local_Cache",
                "upvotes": 7,
                "answered_by": "lupguo",
                "answered_at": "2019-10-28 06:53"
            }
        ]
    },
    {
        "title": "apt-get install tzdata noninteractive [closed]",
        "url": "https://stackoverflow.com/questions/44331836/apt-get-install-tzdata-noninteractive",
        "votes": "240",
        "views": "148k",
        "author": "PYA",
        "issued_at": "2017-06-02 14:54",
        "tags": [
            "bash",
            "ubuntu",
            "dockerfile",
            "apt-get"
        ],
        "answers": [
            {
                "answer": "This is the script I used\n(Updated Version with input from @elquimista from the comments)\n#!/bin/bash\n\nln -fs /usr/share/zoneinfo/America/New_York /etc/localtime\nDEBIAN_FRONTEND=noninteractive apt-get install -y tzdata\ndpkg-reconfigure --frontend noninteractive tzdata\nSeems to work fine.\nAs one liner:\nDEBIAN_FRONTEND=noninteractive apt-get install -y --no-install-recommends tzdata",
                "upvotes": 332,
                "answered_by": "PYA",
                "answered_at": "2017-06-02 16:44"
            },
            {
                "answer": "If someone wants to achieve it in Dockerfile, use as below.\nRUN DEBIAN_FRONTEND=noninteractive apt-get -y install tzdata",
                "upvotes": 168,
                "answered_by": "Youngjae",
                "answered_at": "2019-10-07 07:11"
            },
            {
                "answer": "I have recently found the following solution in a Dockerfile building the Cingulata FHE library:\nln -snf /usr/share/zoneinfo/$(curl https://ipapi.co/timezone) /etc/localtime\nIt basically uses the API provided by ipapi.co to retrieve the timezone information. This automatically configures the timezone properly instead of skipping the dialog and using the default (UTC).",
                "upvotes": 13,
                "answered_by": "Patrick",
                "answered_at": "2020-07-29 12:47"
            }
        ]
    },
    {
        "title": "What is .build-deps for apk add --virtual command?",
        "url": "https://stackoverflow.com/questions/46221063/what-is-build-deps-for-apk-add-virtual-command",
        "votes": "230",
        "views": "98k",
        "author": "gdbj",
        "issued_at": "2017-09-14 13:55",
        "tags": [
            "docker",
            "dockerfile",
            "alpine-linux",
            "alpine-package-keeper"
        ],
        "answers": [
            {
                "answer": "If you see the documentation\n-t, --virtual NAME    Instead of adding all the packages to 'world', create a new \n                      virtual package with the listed dependencies and add that \n                      to 'world'; the actions of the command are easily reverted \n                      by deleting the virtual package\nWhat that means is when you install packages, those packages are not added to global packages. And this change can be easily reverted. So if I need gcc to compile a program, but once the program is compiled I no more need gcc.\nI can install gcc, and other required packages in a virtual package and all of its dependencies and everything can be removed this virtual package name. Below is an example usage\nRUN apk add --virtual mypacks gcc vim \\\n && apk del mypacks\nThe next command will delete all 18 packages installed with the first command.\nIn docker these must be executed as a single RUN command (as shown above), otherwise it will not reduce the image size.",
                "upvotes": 375,
                "answered_by": "Tarun Lalwani",
                "answered_at": "2017-09-14 14:43"
            },
            {
                "answer": ".build-deps is an arbitrary name to call a \"virtual package\" in Alpine, where you will add packages.\nIt creates an extra 'world' of packages, that you will need for a limited period of time (e.g. compilers for building other things).\nIts main purpose is to keep your image as lean and light as possible, because you can easily get rid of it once those packages were used.\nPlease remember that it should be included in the same RUN if you want to achieve the main purpose of lightweight.",
                "upvotes": 26,
                "answered_by": "RicHincapie",
                "answered_at": "2021-01-14 00:05"
            }
        ]
    },
    {
        "title": "npm ERR! Tracker \"idealTree\" already exists while creating the Docker image for Node project",
        "url": "https://stackoverflow.com/questions/57534295/npm-err-tracker-idealtree-already-exists-while-creating-the-docker-image-for",
        "votes": "227",
        "views": "212k",
        "author": "Jaypal Sodha",
        "issued_at": "2019-08-17 07:41",
        "tags": [
            "docker",
            "dockerfile"
        ],
        "answers": [
            {
                "answer": "This issue is happening due to changes in NodeJS starting with version 15. When no WORKDIR is specified, npm install is executed in the root directory of the container, which is resulting in this error. Executing the npm install in a project directory of the container specified by WORKDIR resolves the issue.\nUse the following Dockerfile:\n# Specify a base image\nFROM node:alpine\n\n#Install some dependencies\n\nWORKDIR /usr/app\nCOPY ./ /usr/app\nRUN npm install\n\n# Set up a default command\nCMD [ \"npm\",\"start\" ]",
                "upvotes": 468,
                "answered_by": "Prince Arora",
                "answered_at": "2020-12-24 20:44"
            },
            {
                "answer": "Global install\nIn the event you're wanting to install a package globally outside of working directory with a package.json, you should use the -g flag.\nnpm install -g <pkg>\nThis error may trigger if the CI software you're using like semantic-release is built in node and you attempt to install it outside of a working directory.",
                "upvotes": 51,
                "answered_by": "Evan Carroll",
                "answered_at": "2021-08-10 19:04"
            },
            {
                "answer": "You should specify the WORKDIR prior to COPY instruction in order to ensure the execution of npm install inside the directory where all your application files are there. Here is how you can complete this:\nWORKDIR /usr/app\n\n# Install some dependencies\nCOPY ./ ./\nRUN npm install\nNote that you can simply \"COPY ./ (current local directory) ./ (container directory which is now /usr/app thanks to the WORKDIR instruction)\" instead of \"COPY ./ /usr/app\"\nNow the good reason to use WORKDIR instruction is that you avoid mixing your application files and directories with the root file system of the container (to avoid overriding file system directories in case you have similar directories labels on your application directories)\nOne more thing. It is a good practice to segment a bit your configuration so that when you make a change for example in your index.js (so then you need to rebuild your image), you will not need to run \"npm install\" while the package.json has not been modified.\nYour application is very basic, but think of a big applications where \"npm install\" should take several minutes.\nIn order to make use of caching process of Docker, you can segment your configuration as follows:\nWORKDIR /usr/app\n\n# Install some dependencies\nCOPY ./package.json ./\nRUN npm install\nCOPY ./ ./ \nThis instructs Docker to cache the first COPY and RUN commands when package.json is not touched. So when you change for instance the index.js, and you rebuild your image, Docker will use cache of the previous instructions (first COPY and RUN) and start executing the second COPY. This makes your rebuild much quicker.\nExample for image rebuild:\n => CACHED [2/5] WORKDIR /usr/app                                                                                                       0.0s\n => CACHED [3/5] COPY ./package.json ./                                                                                                 0.0s\n => CACHED [4/5] RUN npm install                                                                                                        0.0s\n => [5/5] COPY ./ ./",
                "upvotes": 7,
                "answered_by": "Youssef",
                "answered_at": "2021-04-09 04:41"
            }
        ]
    },
    {
        "title": "ARG or ENV, which one to use in this case?",
        "url": "https://stackoverflow.com/questions/41916386/arg-or-env-which-one-to-use-in-this-case",
        "votes": "220",
        "views": "199k",
        "author": "ReynierPM",
        "issued_at": "2017-01-29 00:24",
        "tags": [
            "docker",
            "arguments",
            "environment-variables",
            "dockerfile"
        ],
        "answers": [
            {
                "answer": "From Dockerfile reference:\nThe ARG instruction defines a variable that users can pass at build-time to the builder with the docker build command using the --build-arg <varname>=<value> flag.\nThe ENV instruction sets the environment variable <key> to the value <value>.\nThe environment variables set using ENV will persist when a container is run from the resulting image.\nSo if you need build-time customization, ARG is your best choice.\nIf you need run-time customization (to run the same image with different settings), ENV is well-suited.\nIf I want to add let's say 20 (a random number) of extensions or any other feature that can be enable|disable\nGiven the number of combinations involved, using ENV to set those features at runtime is best here.\nBut you can combine both by:\nbuilding an image with a specific ARG\nusing that ARG as an ENV\nThat is, with a Dockerfile including:\nARG var\nENV var=${var}\nYou can then either build an image with a specific var value at build-time (docker build --build-arg var=xxx), or run a container with a specific runtime value (docker run -e var=yyy)",
                "upvotes": 392,
                "answered_by": "VonC",
                "answered_at": "2017-01-29 08:50"
            },
            {
                "answer": "Why to use ARG or ENV ?\nLet's say we have a jar file and we want to make a docker image of it. So, we can ship it to any docker engine.\nWe can write a Dockerfile.\nDockerfile\nFROM eclipse-temurin:17-jdk-alpine\nVOLUME /tmp\nARG JAR_FILE\nCOPY ${JAR_FILE} app.jar\nENTRYPOINT [\"java\",\"-jar\",\"/app.jar\"]\nNow, if we want to build the docker image using Maven, we can pass the JAR_FILE using the --build-arg as target/*.jar\ndocker build --build-arg JAR_FILE=target/*.jar -t myorg/myapp \nHowever, if we are using Gradle; the above command doesn't work and we've to pass a different path: build/libs/\ndocker build --build-arg JAR_FILE=build/libs/*.jar -t myorg/myapp .\nOnce you have chosen a build system, we don\u2019t need the ARG. We can hard code the JAR location.\nFor Maven, that would be as follows:\nDockerfile\nFROM eclipse-temurin:17-jdk-alpine\nVOLUME /tmp\nCOPY target/*.jar app.jar\nENTRYPOINT [\"java\",\"-jar\",\"/app.jar\"]\nhere, we can build an image with the following command:\ndocker build -t image:tag .\nWhen to use `ENV`?\nIf we want to set some values at running containers and reflect that to the image like the Port Number that your application can run/listen on. We can set that using the ENV.\nBoth ARG and ENV seem very similar. Both can be accessed from within our Dockerfile commands in the same manner.\nExample:\nARG VAR_A 5\nENV VAR_B 6\nRUN echo $VAR_A\nRUN echo $VAR_B\nPersonal Option!\nThere is a tradeoff between choosing ARG over ENV. If you choose ARG you can't change it later during the run. However, if you chose ENV you can modify the value at the container.\nI personally prefer ARG over ENV wherever I can, like,\nIn the above Example:\nI have used ARG as the build system maven or Gradle impacts during build rather than runtime. It thus encapsulates a lot of details and provided a minimum set of arguments for the runtime.\nFor more details, you can refer to this.",
                "upvotes": 6,
                "answered_by": "Zahid Khan",
                "answered_at": "2022-12-27 09:21"
            }
        ]
    },
    {
        "title": "Dockerfile - set ENV to result of command",
        "url": "https://stackoverflow.com/questions/34911622/dockerfile-set-env-to-result-of-command",
        "votes": "219",
        "views": "148k",
        "author": "Sultanen",
        "issued_at": "2016-01-20 22:10",
        "tags": [
            "dockerfile",
            "env"
        ],
        "answers": [
            {
                "answer": "As an addition to DarkSideF answer.\nYou should be aware that each line/command in Dockerfile is ran in another container.\nYou can do something like this:\nRUN export bleah=$(hostname -f);echo $bleah;\nThis is run in a single container.",
                "upvotes": 47,
                "answered_by": "Dimitrie Mititelu",
                "answered_at": "2016-06-16 13:15"
            },
            {
                "answer": "If you run commands using sh as it seems to be the default in docker.\nYou can do something like this:\nRUN echo \"export VAR=`command`\" >> /envfile\nRUN . /envfile; echo $VAR\n....\nRUN . /envfile; echo $VAR # can be re-used\n....\nRUN rm /envfile # Remove the file at the end if it contains secrets so that they are not persisted in the container image\nIMPORTANT: if secrets are stored in that file, delete it before finishing the container image.\nThis way, you build a env file by redirecting output to the env file of your choice. It's more explicit than having to define profiles and so on.\nThen as the file will be available to other layers, it will be possible to source it and use the variables being exported. The way you create the env file isn't important.\nThen when you're done you could remove the file to make it unavailable to the running container.\nThe . is how the env file is loaded.",
                "upvotes": 33,
                "answered_by": "Lo\u00efc Faure-Lacroix",
                "answered_at": "2021-07-07 17:11"
            },
            {
                "answer": "I had same issue and found way to set environment variable as result of function by using RUN command in dockerfile.\nFor example i need to set SECRET_KEY_BASE for Rails app just once without changing as would when i run:\ndocker run  -e SECRET_KEY_BASE=\"$(openssl rand -hex 64)\"\nInstead it i write to Dockerfile string like:\nRUN bash -l -c 'echo export SECRET_KEY_BASE=\"$(openssl rand -hex 64)\" >> /etc/bash.bashrc'\nand my env variable available from root, even after bash login. or may be\nRUN /bin/bash -l -c 'echo export SECRET_KEY_BASE=\"$(openssl rand -hex 64)\" > /etc/profile.d/docker_init.sh'\nthen it variable available in CMD and ENTRYPOINT commands\nDocker cache it as layer and change only if you change some strings before it.\nYou also can try different ways to set environment variable.",
                "upvotes": 26,
                "answered_by": "DarkSideF",
                "answered_at": "2016-06-16 09:38"
            }
        ]
    },
    {
        "title": "Docker-compose check if mysql connection is ready",
        "url": "https://stackoverflow.com/questions/42567475/docker-compose-check-if-mysql-connection-is-ready",
        "votes": "218",
        "views": "241k",
        "author": "John Kariuki",
        "issued_at": "2017-03-02 22:48",
        "tags": [
            "mysql",
            "docker",
            "docker-compose",
            "dockerfile"
        ],
        "answers": [
            {
                "answer": "This should be enough\nversion: '3.4'\nservices:\n  api:\n    build: .\n    container_name: api\n    ports:\n        - \"8080:8080\"\n    depends_on:\n        db:\n          condition: service_healthy\n  db:\n    image: mysql\n    ports: ['3306:3306']\n    environment:\n      MYSQL_USER: myuser\n      MYSQL_PASSWORD: mypassword\n    healthcheck:\n      test: mysqladmin ping -h 127.0.0.1 -u $$MYSQL_USER --password=$$MYSQL_PASSWORD\n      start_period: 5s\n      interval: 5s\n      timeout: 5s\n      retries: 55",
                "upvotes": 58,
                "answered_by": "Maksim Kostromin",
                "answered_at": "2019-02-24 16:55"
            },
            {
                "answer": "condition was removed compose spec in versions 3.0 to 3.8 but is now back!\nUsing version of the compose spec v3.9+ (docker-compose v1.29), you can use condition as an option in long syntax form of depends_on.\nUse condition: service_completed_successfully to tell compose that service must be running before dependent service gets started.\nservices:\n  web:\n    build: .\n    depends_on:\n      db:\n        condition: service_completed_successfully\n      redis:\n        condition: service_completed_successfully\n  redis:\n    image: redis\n  db:\n    image: postgres\ncondition option can be:\nservice_started is equivalent to short syntax form\nservice_healthy is waiting for the service to be healthy. Define healthy with healthcheck option\nservice_completed_successfully specifies that a dependency is expected to run to successful completion before starting a dependent service (Added to docker-compose with PR#8122).\nIt is sadly pretty badly documented. I found references to it on Docker forums, Docker doc issues, Docker compose issue, in Docker Compose e2e fixtures. Not sure if it's supported by Docker Compose v2.",
                "upvotes": 52,
                "answered_by": "Capripot",
                "answered_at": "2019-01-18 07:56"
            },
            {
                "answer": "Hi for a simple healthcheck using docker-compose v2.1, I used:\n/usr/bin/mysql --user=root --password=rootpasswd --execute \\\"SHOW DATABASES;\\\"\nBasically it runs a simple mysql command SHOW DATABASES; using as an example the user root with the password rootpasswd in the database. (Don't expose credentials in Production, use environment variables to pass them)\nIf the command succeed the db is up and ready so the healthcheck path. You can use interval so it tests at interval.\nRemoving the other field for visibility, here is what it would look like in your docker-compose.yaml.\nversion: '2.1'\n\n  services:\n    db:\n      ... # Other db configuration (image, port, volumes, ...)\n      healthcheck:\n        test: \"/usr/bin/mysql --user=root --password=rootpasswd --execute \\\"SHOW DATABASES;\\\"\"\n        interval: 2s\n        timeout: 20s\n        retries: 10\n\n     app:\n       ... # Other app configuration\n       depends_on:\n         db:\n         condition: service_healthy",
                "upvotes": 35,
                "answered_by": "Sylhare",
                "answered_at": "2018-08-01 19:36"
            }
        ]
    },
    {
        "title": "How to update /etc/hosts file in Docker image during \"docker build\"",
        "url": "https://stackoverflow.com/questions/38302867/how-to-update-etc-hosts-file-in-docker-image-during-docker-build",
        "votes": "207",
        "views": "339k",
        "author": "Prakash",
        "issued_at": "2016-07-11 08:57",
        "tags": [
            "docker",
            "dockerfile",
            "hosts"
        ],
        "answers": [
            {
                "answer": "With a more recent version of docker, this could be done with docker-compose and its extra_hosts directive\nAdd hostname mappings.\nUse the same values as the docker run client --add-host parameter (which should already be available for docker 1.8).\nextra_hosts:\n  - \"somehost:162.242.195.82\"\n  - \"otherhost:50.31.209.229\"\nIn short: modify /etc/hosts of your container when running it, instead of when building it.\nWith Docker 17.x+, you have a docker build --add-host mentioned below, but, as commented in issue 34078 and in this answer:\nThe --add-host feature during build is designed to allow overriding a host during build, but not to persist that configuration in the image.\nThose links point to strategies for dealing with the problem at hand:\nRun an internal DNS; you can set the default DNS server to use in the daemon; that way every container started will automatically use the configured DNS by default\nUse docker compose and provide a docker-compose.yml to your developers.\nThe docker compose file allows you to specify all the options that should be used when starting a container, so developers could just docker compose up to start the container with all the options they need to set.\nThese solutions can take advantage of using of the docker-compose method that was suggested earlier in the answer, with its extra_hosts directive.",
                "upvotes": 265,
                "answered_by": "VonC",
                "answered_at": "2016-07-11 12:13"
            },
            {
                "answer": "You can not modify the host file in the image using echo in RUN step because docker daemon will maintain the file(/etc/hosts) and its content(hosts entry) when you start a container from the image.\nHowever following can be used to achieve the same:\nENTRYPOINT [\"/bin/sh\", \"-c\" , \"echo 192.168.254.10   database-server >> /etc/hosts && echo 192.168.239.62   redis-ms-server >> /etc/hosts && exec java -jar ./botblocker.jar \" ]\nKey to notice here is the use of exec command as docker documentation suggests. Use of exec will make the java command as PID 1 for the container. Docker interrupts will only respond to that.\nSee https://docs.docker.com/engine/reference/builder/#entrypoint",
                "upvotes": 42,
                "answered_by": "Shubham Singh",
                "answered_at": "2016-11-21 14:01"
            },
            {
                "answer": "I think docker recently added the --add-host flag to docker build which is really great.\n[Edit] So this feature was updated on 17.04.0-ce\nFor more detail on how to use docker build with the --add-host flag please visit: https://docs.docker.com/edge/engine/reference/commandline/build/",
                "upvotes": 29,
                "answered_by": "Bill Cheng",
                "answered_at": "2017-09-15 14:27"
            },
            {
                "answer": "Since this still comes up as a first answer in Google I'll contribute possible solution.\nCommand taken from here suprisingly worked for me (Docker 1.13.1, Ubuntu 16.04) :\ndocker exec -u 0 <container-name> /bin/sh -c \"echo '<ip> <name>' >> /etc/hosts\"",
                "upvotes": 15,
                "answered_by": "Remy",
                "answered_at": "2018-05-09 12:35"
            }
        ]
    },
    {
        "title": "Docker images - types. Slim vs slim-stretch vs stretch vs alpine",
        "url": "https://stackoverflow.com/questions/54954187/docker-images-types-slim-vs-slim-stretch-vs-stretch-vs-alpine",
        "votes": "197",
        "views": "135k",
        "author": "mailtobash",
        "issued_at": "2019-03-02 00:50",
        "tags": [
            "java",
            "docker",
            "dockerfile"
        ],
        "answers": [
            {
                "answer": "Per docker library docs (quote and links below), here's a summary:\nopenjdk:<version>\nThe defacto image. Use it if unsure.\nopenjdk:<version>-buster, openjdk:<version>-stretch and openjdk:<version>-jessie\nbuster, jessie or stretch are the suite code names for releases of Debian and indicate which release the image is based on.\nopenjdk:<version>-alpine\nSimilarly, this image is based on the Alpine Linux, thus being a very small base image. It is recommended if you need an image size is as small as possible. The caveat is that it uses some unusual libs, but shouldn't be a problem for most software. In doubt, check the official docs below.\nopenjdk:<version> (from 12 onwards), openjdk:<version>-oracle and openjdk:<version>-oraclelinux7\nStarting with openjdk:12 the default image as well as the -oracle and -oraclelinux7 variants are based on the official Oracle Linux 7 image. The OpenJDK binaries in the default image as well as the -oracle and -oraclelinux7 variants are built by Oracle and are sourced from the OpenJDK community.\nopenjdk:<version>-slim\nThis image only contains the minimal packages needed to run Java (and is missing many of the UI-related Java libraries, for instance). Unless you are working in an environment where only the openjdk image will be deployed and you have space constraints, the default image is recommended over this one.\nopenjdk:<version>-windowsservercore\nThis image is based on Windows Server Core (microsoft/windowsservercore).\n\nFull docs (version shown below here, latest version here):\nImage Variants\nThe openjdk images come in many flavors, each designed for a specific use case.\nopenjdk:<version>\nThis is the defacto image. If you are unsure about what your needs are, you probably want to use this one. It is designed to be used both as a throw away container (mount your source code and start the container to start your app), as well as the base to build other images off of.\nSome of these tags may have names like jessie or stretch in them. These are the suite code names for releases of Debian and indicate which release the image is based on.\nopenjdk:<version>-alpine\nThis image is based on the popular Alpine Linux project, available in the alpine official image. Alpine Linux is much smaller than most distribution base images (~5MB), and thus leads to much slimmer images in general.\nThis variant is highly recommended when final image size being as small as possible is desired. The main caveat to note is that it does use musl libc instead of glibc and friends, so certain software might run into issues depending on the depth of their libc requirements. However, most software doesn't have an issue with this, so this variant is usually a very safe choice. See this Hacker News comment thread for more discussion of the issues that might arise and some pro/con comparisons of using Alpine-based images.\nTo minimize image size, it's uncommon for additional related tools (such as git or bash) to be included in Alpine-based images. Using this image as a base, add the things you need in your own Dockerfile (see the alpine image description for examples of how to install packages if you are unfamiliar).\nopenjdk:<version>-windowsservercore\nThis image is based on Windows Server Core (microsoft/windowsservercore). As such, it only works in places which that image does, such as Windows 10 Professional/Enterprise (Anniversary Edition) or Windows Server 2016.\nFor information about how to get Docker running on Windows, please see the relevant \"Quick Start\" guide provided by Microsoft:\nWindows Server Quick Start\nWindows 10 Quick Start\nopenjdk:<version>-slim\nThis image installs the -headless package of OpenJDK and so is missing many of the UI-related Java libraries and some common packages contained in the default tag. It only contains the minimal packages needed to run Java. Unless you are working in an environment where only the openjdk image will be deployed and you have space constraints, we highly recommend using the default image of this repository.",
                "upvotes": 201,
                "answered_by": "acdcjunior",
                "answered_at": "2019-03-29 23:20"
            },
            {
                "answer": "Choose a base docker image that fits your needs and please keep in mind that Image size is an important aspect also.\nImage can be considered as a set of instructions on how to create the container. In Docker, one image could be inherited from (or based on) another image, adding additional instructions on top of base ones. Each image consists of multiple layers, which are effectively immutable.\nPlase read Crafting the perfect Java Docker build flow article.\nDocker image size is actually very important. The size has an impact on:\nnetwork latency: need to transfer Docker image over the web\nstorage: need to store all these bits somewhere\nservice availability and elasticity: when using a Docker scheduler, like Kubernetes, Swarm, Nomad, DC/OS or other (the scheduler can move containers between hosts)\nsecurity: do you really, I mean really need the libpng package with all its CVE vulnerabilities for your Java application?\ndevelopment agility: small Docker images == faster build time and faster deployment\n\nTo run a java application you need JRE at least. For example, for a spring project your image can be based on slim Alpine Linux with OpenJDK JRE:\n#simple dockerFile for java app:\n\n#here we are using Base Alpine Linux based image with OpenJDK JRE only\n#For Java 8, try this\nFROM openjdk:8-jre-alpine\n\n#For Java 11, try this\n#FROM adoptopenjdk/openjdk11:alpine-jre\n\n#copy application WAR/JAR (with libraries inside)\nCOPY target/spring-boot-*.war/jar yourName.war/jar\n# specify default command\nCMD [\"/usr/bin/java\", \"-jar\", \"/yourName.war/jar\"]\nAlso you can use docker history yourImageName to see all layers (and their size) that makes your image.",
                "upvotes": 7,
                "answered_by": "Ghasem Sadeghi",
                "answered_at": "2020-04-29 17:45"
            }
        ]
    },
    {
        "title": "docker-compose, run a script after container has started?",
        "url": "https://stackoverflow.com/questions/47615751/docker-compose-run-a-script-after-container-has-started",
        "votes": "195",
        "views": "435k",
        "author": "Blooze",
        "issued_at": "2017-12-03 06:03",
        "tags": [
            "docker",
            "docker-compose",
            "dockerfile",
            "rancher"
        ],
        "answers": [
            {
                "answer": "This is the way I use for calling a script after a container is started without overriding the entrypoint.\nIn my example, I used it for initializing the replicaset of my local MongoDB\nservices:\n  mongo:\n    image: mongo:4.2.8\n    hostname: mongo\n    container_name: mongodb\n    entrypoint: [\"/usr/bin/mongod\",\"--bind_ip_all\",\"--replSet\",\"rs0\"]\n    ports:\n      - 27017:27017\n  mongosetup:\n    image: mongo:4.2.8\n    depends_on:\n      - mongo\n    restart: \"no\"\n    entrypoint: [ \"bash\", \"-c\", \"sleep 10 && mongo --host mongo:27017 --eval 'rs.initiate()'\"]      \nIn the first part, I simply launch my service (mongo)\nThe second service use a \"bash\" entry point AND a restart: no <= important\nI also use a depends_on between service and setup service for manage the launch order.",
                "upvotes": 116,
                "answered_by": "Gibson Lunaziz",
                "answered_at": "2020-10-01 10:02"
            },
            {
                "answer": "The trick is to overwrite the compose COMMAND to perform whatever init action you need before calling the original command.\nAdd a script in your image that will perform the init work that you want like set password, change internal config files, etc. Let's call it init.sh. You add it to your image.\nDockerfile:\nFROM: sourceimage:tag\nCOPY init.sh /usr/local/bin/\nENTRYPOINT []\nThe above overrides whatever ENTRYPOINT is defined in the sourceimage. That's to make this example simpler. Make sure you understand what the ENTRYPOINT is doing in the Dockerfile from the sourceimage and call it in the command: of the docker-compose.yml file.\ndocker-compose.yml:\nservices:\n  myservice:\n    image: something:tag\n    ...\n    command: sh -c \"/usr/local/bin/init.sh && exec myexecutable\"\nIt's important to use exec before calling the main command. That will install the command as the first process (PID1) which will make it receive signals like STOP, KILL (Ctrl-C on keyboard) or HUP.",
                "upvotes": 75,
                "answered_by": "Bernard",
                "answered_at": "2017-12-04 09:17"
            },
            {
                "answer": "You can also use volumes to do this:\nservices:\n  example:\n    image: <whatever>\n    volume: ./init.sh:/init.sh\n    entrypoint: sh -c \"/init.sh\"\nNote that this will mount init.sh to the container, not copy it (if that matters, usually it doesn't). Basically processes within the container can modify init.sh and it would modify the file as it exists in your actual computer.",
                "upvotes": 33,
                "answered_by": "Asad-ullah Khan",
                "answered_at": "2021-04-07 22:13"
            }
        ]
    },
    {
        "title": "Deploying a minimal flask app in docker - server connection issues",
        "url": "https://stackoverflow.com/questions/30323224/deploying-a-minimal-flask-app-in-docker-server-connection-issues",
        "votes": "193",
        "views": "102k",
        "author": "Dreen",
        "issued_at": "2015-05-19 10:36",
        "tags": [
            "python",
            "deployment",
            "flask",
            "docker",
            "dockerfile"
        ],
        "answers": [
            {
                "answer": "The problem is you are only binding to the localhost interface, you should be binding to 0.0.0.0 if you want the container to be accessible from outside. If you change:\nif __name__ == '__main__':\n    app.run()\nto\nif __name__ == '__main__':\n    app.run(host='0.0.0.0')\nIt should work.\nNote that this will bind to all interfaces on the host, which may in some circumstances be a security risk - see https://stackoverflow.com/a/58138250/4332 for more information on binding to a specific interface.",
                "upvotes": 330,
                "answered_by": "Adrian Mouat",
                "answered_at": "2015-05-19 15:13"
            },
            {
                "answer": "When using the flask command instead of app.run, you can pass the --host option to change the host. The line in Docker would be:\nCMD [\"flask\", \"run\", \"--host\", \"0.0.0.0\"]\nor\nCMD flask run --host 0.0.0.0",
                "upvotes": 73,
                "answered_by": "Marku",
                "answered_at": "2017-03-25 10:00"
            }
        ]
    },
    {
        "title": "Run a script in Dockerfile",
        "url": "https://stackoverflow.com/questions/34549859/run-a-script-in-dockerfile",
        "votes": "186",
        "views": "580k",
        "author": "Kevin",
        "issued_at": "2015-12-31 17:45",
        "tags": [
            "docker",
            "dockerfile"
        ],
        "answers": [
            {
                "answer": "RUN and ENTRYPOINT are two different ways to execute a script.\nRUN means it creates an intermediate container, runs the script and freeze the new state of that container in a new intermediate image. The script won't be run after that: your final image is supposed to reflect the result of that script.\nENTRYPOINT means your image (which has not executed the script yet) will create a container, and runs that script.\nIn both cases, the script needs to be added, and a RUN chmod +x /bootstrap.sh is a good idea.\nIt should also start with a shebang (like #!/bin/sh)\nConsidering your script (bootstrap.sh: a couple of git config --global commands), it would be best to RUN that script once in your Dockerfile, but making sure to use the right user (the global git config file is %HOME%/.gitconfig, which by default is the /root one)\nAdd to your Dockerfile:\nRUN /bootstrap.sh\nThen, when running a container, check the content of /root/.gitconfig to confirm the script was run.",
                "upvotes": 223,
                "answered_by": "VonC",
                "answered_at": "2016-01-01 07:56"
            },
            {
                "answer": "In addition to the answers above:\nIf you created/edited your .sh script file in Windows, make sure it was saved with line ending in Unix format. By default many editors in Windows will convert Unix line endings to Windows format and Linux will not recognize shebang (#!/bin/sh) at the beginning of the file. So Linux will produce the error message like if there is no shebang.\nTips:\nIf you use Notepad++, you need to click \"Edit/EOL Conversion/UNIX (LF)\"\nIf you use Visual Studio, I would suggest installing \"End Of Line\" plugin. Then you can make line endings visible by pressing Ctrl-R, Ctrl-W. And to set Linux style endings you can press Ctrl-R, Ctrl-L. For Windows style, press Ctrl-R, Ctrl-C.",
                "upvotes": 62,
                "answered_by": "VeganHunter",
                "answered_at": "2018-07-12 06:11"
            },
            {
                "answer": "Try to create script with ADD command and specification of working directory Like this(\"script\" is the name of script and /root/script.sh is where you want it in the container, it can be different path:\nADD script.sh /root/script.sh\nIn this case ADD has to come before CMD, if you have one BTW it's cool way to import scripts to any location in container from host machine\nIn CMD place [./script]\nIt should automatically execute your script\nYou can also specify WORKDIR as /root, then you'l be automatically placed in root, upon starting a container",
                "upvotes": 28,
                "answered_by": "Michael",
                "answered_at": "2016-05-28 13:06"
            },
            {
                "answer": "It's best practice to use COPY instead of ADD when you're copying from the local file system to the image. Also, I'd recommend creating a sub-folder to place your content into. If nothing else, it keeps things tidy. Make sure you mark the script as executable using chmod.\nHere, I am creating a scripts sub-folder to place my script into and run it from:\nRUN mkdir -p /scripts\nCOPY script.sh /scripts\nWORKDIR /scripts\nRUN chmod +x script.sh\nRUN ./script.sh",
                "upvotes": 24,
                "answered_by": "ldobson",
                "answered_at": "2020-06-16 21:59"
            }
        ]
    },
    {
        "title": "How to pass environment variable to docker-compose up",
        "url": "https://stackoverflow.com/questions/49293967/how-to-pass-environment-variable-to-docker-compose-up",
        "votes": "183",
        "views": "248k",
        "author": "Abhi.G",
        "issued_at": "2018-03-15 07:44",
        "tags": [
            "docker",
            "docker-compose",
            "dockerfile",
            "environment-variables"
        ],
        "answers": [
            {
                "answer": "You have two options (option 2. overrides 1.):\nCreate the .env file as already suggested in another answer.\nPrepend KEY=VALUE pair(s) to your docker-compose command, e.g:\nKB_DB_TAG_VERSION=kb-1.3.20-v1.0.0 docker-compose up\nExporting it earlier in a script should also work, e.g.:\nexport KB_DB_TAG_VERSION=kb-1.3.20-v1.0.0\ndocker-compose up\nKeep in mind that these options just pass an environment varible to the docker-compose.yml file, not to a container. For an environment variable to be actually passed to a container you always need something like this in your docker-compose.yml:\n  environment:\n    - KB_DB_TAG_VERSION=$KB_DB_TAG_VERSION",
                "upvotes": 222,
                "answered_by": "Jakub Kukul",
                "answered_at": "2018-06-22 15:54"
            },
            {
                "answer": "You can create a .env file on the directory where you execute the docker-compose up command (and your docker-compose.yml file is located) with the following content:\nKB_DB_TAG_VERSION=kb-1.3.20-v1.0.0\nYour docker-compose.yml file should look like the following (added { and }):\nversion: '3'\nservices:\n   db:\n     user: \"1000:50\"\n     volumes:\n       - /data/mysql:/var/lib/mysql\n     container_name: k-db\n     environment:\n       - MYSQL_ALLOW_EMPTY_PASSWORD=yes\n     image: XX:${KB_DB_TAG_VERSION}\n     image: k-db\n     ports:\n       - \"3307:3306\"\nAfter making the above changes , check whether the changes are reflected or not using the command docker-compose config. The variable will be replaced by the variable value. Please refer to the page here to understand more about variable replacement.",
                "upvotes": 71,
                "answered_by": "Sebastian Brosch",
                "answered_at": "2018-03-15 08:00"
            }
        ]
    },
    {
        "title": "Dockerfile build - possible to ignore error?",
        "url": "https://stackoverflow.com/questions/30716937/dockerfile-build-possible-to-ignore-error",
        "votes": "180",
        "views": "145k",
        "author": "Oliver",
        "issued_at": "2015-06-08 18:55",
        "tags": [
            "docker",
            "dockerfile"
        ],
        "answers": [
            {
                "answer": "You can also use the standard bash ignore error || true, which is nice if you are in the middle of a chain:\nRUN <first stage> && <job that might fail> || true && <next stage>",
                "upvotes": 57,
                "answered_by": "MortenB",
                "answered_at": "2018-11-28 13:17"
            }
        ]
    },
    {
        "title": "Failed to solve with frontend Dockerfile",
        "url": "https://stackoverflow.com/questions/64985913/failed-to-solve-with-frontend-dockerfile",
        "votes": "179",
        "views": "431k",
        "author": "helloWORLD",
        "issued_at": "2020-11-24 11:51",
        "tags": [
            "docker",
            "docker-compose",
            "dockerfile"
        ],
        "answers": [
            {
                "answer": "The name of Docker files doesn't have any extension. It's just Dockerfile with capital D and lowercase f.\nYou can also specify the Dockerfile name, such as docker build . -f Dockerfile.txt if you'd like to name it something else.",
                "upvotes": 330,
                "answered_by": "Gabriel Petersson",
                "answered_at": "2020-11-24 11:53"
            },
            {
                "answer": "One can provide the filename of the Docker file using -f.\nFor instance, if your Docker file is called Dockerfile.base, call the build command as follows:\ndocker build . -f Dockerfile.base -t helloworld\nThen, you can start the build image using the following command:\ndocker run --rm -it helloworld",
                "upvotes": 36,
                "answered_by": "koppor",
                "answered_at": "2021-01-08 08:03"
            },
            {
                "answer": "Are you sure about naming your file Dockerfile? it shouldn't be dockerfile nor DockerFile.",
                "upvotes": 20,
                "answered_by": "Ammar Yasser",
                "answered_at": "2022-11-10 14:16"
            }
        ]
    },
    {
        "title": "What is the purpose of VOLUME in Dockerfile?",
        "url": "https://stackoverflow.com/questions/34809646/what-is-the-purpose-of-volume-in-dockerfile",
        "votes": "175",
        "views": "102k",
        "author": "radium226",
        "issued_at": "2016-01-15 11:03",
        "tags": [
            "docker",
            "dockerfile",
            "docker-volume"
        ],
        "answers": [
            {
                "answer": "A volume is a persistent data stored in /var/lib/docker/volumes/...\nYou can either declare it in a Dockerfile, which means each time a container is started from the image, the volume is created (empty), even if you don't have any -v option.\nYou can declare it on runtime docker run -v [host-dir:]container-dir.\nCombining the two (VOLUME + docker run -v) means that you can mount the content of a host folder into your volume persisted by the container in /var/lib/docker/volumes/...\ndocker volume create creates a volume without having to define a Dockerfile and build an image and run a container. It is used to quickly allow other containers to mount said volume.\nIf you had persisted some content in a volume, but since then, deleted the container (which by default does not delete its associated volume, unless you are using docker rm -v), you can re-attach said volume to a new container (declaring the same volume).\nSee \"Docker - How to access a volume not attached to a container?\".\nWith docker volume create, this is easy to reattach a named volume to a container.\ndocker volume create --name aname\ndocker run -v aname:/apath --name acontainer\n...\n# modify data in /apath\n...\ndocker rm acontainer\n\n# let's mount aname volume again\ndocker run -v aname:/apath --name acontainer\nls /apath\n# you find your data back!\nWhy volumes were introduced in the first place?\nDocker volumes were introduced primarily to solve the challenge of data persistence and data sharing in containerized environments.\nIn the world of Docker, containers are ephemeral and lightweight, meaning they can be created, started, stopped, and destroyed with ease, and they are designed to be stateless.\nHowever, applications often need to store data permanently, access configuration files, or share data between different containers or between containers and the host system. That is where Docker volumes come into play.\nVolumes provide a mechanism to persist data generated by and used by Docker containers.\nUnlike the container's writable layer, which is tightly coupled to the container's lifecycle and gets destroyed when the container is removed, volumes are managed by Docker and are designed to exist independently of containers.\nThat means data in volumes survives container restarts and can be securely shared among multiple containers. And volumes are platform-independent, which simplifies data migration and backup processes.\nSee \"Docker Engine / Storage / Manage data in Docker\"\nAdditionally, volumes address performance and security concerns. Since they are stored outside the container's filesystem, they offer improved I/O performance, especially important for database storage or heavy read/write operations. They also provide a safer way to handle sensitive data, as volumes can be more securely isolated from the core container filesystem.",
                "upvotes": 115,
                "answered_by": "VonC",
                "answered_at": "2016-01-15 11:35"
            },
            {
                "answer": "VOLUME instruction becomes interesting when you combine it with volumes-from runtime parameter.\nGiven the following Dockerfile:\nFROM busybox\nVOLUME /myvolume\nBuild an image with:\ndocker build -t my-busybox .\nAnd spin up a container with:\ndocker run --rm -it --name my-busybox-1 my-busybox\nThe first thing to notice is you will have a folder in this image named myvolume. But it is not particularly interesting since when we exit the container the volume will be removed as well.\nCreate an empty file in this folder, so run the following in the container:\ncd myvolume\ntouch hello.txt\nNow spin up a new container, but share the same volume with my-busybox-1:\ndocker run --rm -it --volumes-from my-busybox-1 --name my-busybox-2 my-busybox\nYou will see that my-busybox-2 contains the file hello.txt in myvolume folder.\nOnce you exit both containers, the volume will be removed as well.",
                "upvotes": 27,
                "answered_by": "Koray Tugay",
                "answered_at": "2019-10-05 12:59"
            },
            {
                "answer": "Specifying VOLUME in Dockerfile makes sure the folder is to be treated as a volume(i.e., outside container) at runtime, as opposed to be a regular directory inside the container. Note the performance and accessibility implications;\nIf having forgot to specify -v in docker run command line, the above is still true. It's just the volume name becomes anonymous. But there are still ways to access or recover data from such anonymous volumes.",
                "upvotes": 7,
                "answered_by": "M2014",
                "answered_at": "2021-04-20 19:07"
            }
        ]
    },
    {
        "title": "Share variable in multi-stage Dockerfile: ARG before FROM not substituted",
        "url": "https://stackoverflow.com/questions/53681522/share-variable-in-multi-stage-dockerfile-arg-before-from-not-substituted",
        "votes": "173",
        "views": "71k",
        "author": "Alberto Chiusole",
        "issued_at": "2018-12-08 10:19",
        "tags": [
            "docker",
            "dockerfile",
            "docker-multi-stage-build"
        ],
        "answers": [
            {
                "answer": "ARGs only last for the build phase of a single image. For the multistage, renew the ARG by simply stating:\nARG DARSHAN_VER\nafter your FROM instructions.\ncf. https://docs.docker.com/engine/reference/builder/#arg\nARG DARSHAN_VER=3.1.6\n\nFROM fedora:29 as build\nARG DARSHAN_VER\nRUN dnf install -y \\\n        gcc \\\n        make \\\n        bzip2 bzip2-devel zlib zlib-devel\nRUN curl -O \"ftp://ftp.mcs.anl.gov/pub/darshan/releases/darshan-${DARSHAN_VER}.tar.gz\" \\\n    && tar ...\n\n\nFROM fedora:29\nARG DARSHAN_VER\nCOPY --from=build \"/usr/local/darshan-${DARSHAN_VER}\" \"/usr/local/darshan-${DARSHAN_VER}\"\n...\nYou will notice how I declared the initial value at the top of the script, and pull it in on each image.",
                "upvotes": 303,
                "answered_by": "Richard Barber",
                "answered_at": "2018-12-08 11:33"
            },
            {
                "answer": "Here are quotes from the documentation:\nAn ARG instruction goes out of scope at the end of the build stage where it was defined. To use an arg in multiple stages, each stage must include the ARG instruction.\nhttps://docs.docker.com/engine/reference/builder/#scope\nAn ARG declared before a FROM is outside of a build stage, so it can\u2019t be used in any instruction after a FROM. To use the default value of an ARG declared before the first FROM use an ARG instruction without a value inside of a build stage\nhttps://docs.docker.com/engine/reference/builder/#understand-how-arg-and-from-interact",
                "upvotes": 47,
                "answered_by": "Felix K.",
                "answered_at": "2018-12-08 14:44"
            }
        ]
    },
    {
        "title": "How to write commands with multiple lines in Dockerfile while preserving the new lines?",
        "url": "https://stackoverflow.com/questions/33439230/how-to-write-commands-with-multiple-lines-in-dockerfile-while-preserving-the-new",
        "votes": "172",
        "views": "193k",
        "author": "Venkata Jaswanth",
        "issued_at": "2015-10-30 15:13",
        "tags": [
            "docker",
            "dockerfile"
        ],
        "answers": [
            {
                "answer": "You can use what is called \"ANSI-C quoting\" with $'...'. It was originally a ksh93 feature but it is now available in bash, zsh, mksh, FreeBSD sh and in busybox's ash (but only when it is compiled with ENABLE_ASH_BASH_COMPAT).\nAs RUN uses /bin/sh as shell by default you are required to switch to something like bash first by using the SHELL instruction.\nStart your command with $', end it with ' and use \\n\\ for newlines, like this:\nSHELL [\"/bin/bash\", \"-c\"]\n\nRUN echo $'[repo] \\n\\\nname            = YUM Repository \\n\\\nbaseurl         = https://example.com/packages/ \\n\\\nenabled         = 1 \\n\\\ngpgcheck        = 0' > /etc/yum.repos.d/Repo.repoxyz",
                "upvotes": 193,
                "answered_by": "Daniel Zolnai",
                "answered_at": "2015-10-30 15:33"
            },
            {
                "answer": "Use printf to allow a single RUN command to output multiple lines of text, using \\n to insert newlines.\nExecuting:\nRUN printf 'example\\ntext\\nhere' >> example.txt\nappends:\nexample\ntext\nhere\nto the file example.txt",
                "upvotes": 105,
                "answered_by": "Cristian Todea",
                "answered_at": "2016-04-19 14:12"
            },
            {
                "answer": "You can use:\nRUN echo -e \"\\\n[repo] \\n\\\nname            = YUM Repository \\n\\\nbaseurl         = https://example.com/packages/ \\n\\\nenabled         = 1 \\n\\\ngpgcheck        = 0\\\n\" > /etc/yum.repos.d/Repo.repoxyz\nThis way you will have a quick way to check what the file contents are. You just need to be aware that you need to end every line with \\ and insert the \\n when needed.",
                "upvotes": 43,
                "answered_by": "Paulo Fidalgo",
                "answered_at": "2019-01-07 12:43"
            },
            {
                "answer": "I ended up using a combination of the examples listed here since the new line \\n did not work with echo.\nRUN printf 'example \\n\\\ntext \\n\\\nhere' >> example.txt\nIt produces the following, as expected:\nexample\ntext\nhere",
                "upvotes": 33,
                "answered_by": "Sergey",
                "answered_at": "2019-12-09 09:09"
            }
        ]
    },
    {
        "title": "Docker how to run pip requirements.txt only if there was a change?",
        "url": "https://stackoverflow.com/questions/34398632/docker-how-to-run-pip-requirements-txt-only-if-there-was-a-change",
        "votes": "170",
        "views": "262k",
        "author": "Prometheus",
        "issued_at": "2015-12-21 15:01",
        "tags": [
            "python",
            "docker",
            "dockerfile"
        ],
        "answers": [
            {
                "answer": "I'm assuming that at some point in your build process, you're copying your entire application into the Docker image with COPY or ADD:\nCOPY . /opt/app\nWORKDIR /opt/app\nRUN pip install -r requirements.txt\nThe problem is that you're invalidating the Docker build cache every time you're copying the entire application into the image. This will also invalidate the cache for all subsequent build steps.\nTo prevent this, I'd suggest copying only the requirements.txt file in a separate build step before adding the entire application into the image:\nCOPY requirements.txt /opt/app/requirements.txt\nWORKDIR /opt/app\nRUN pip install -r requirements.txt\nCOPY . /opt/app\n# continue as before...\nAs the requirements file itself probably changes only rarely, you'll be able to use the cached layers up until the point that you add your application code into the image.",
                "upvotes": 297,
                "answered_by": "helmbert",
                "answered_at": "2015-12-21 15:58"
            },
            {
                "answer": "This is directly mentioned in Docker's own \"Best practices for writing Dockerfiles\":\nIf you have multiple Dockerfile steps that use different files from your context, COPY them individually, rather than all at once. This will ensure that each step\u2019s build cache is only invalidated (forcing the step to be re-run) if the specifically required files change.\nFor example:\nCOPY requirements.txt /tmp/\nRUN pip install --requirement /tmp/requirements.txt\nCOPY . /tmp/\nresults in fewer cache invalidations for the RUN step, than if you put the COPY . /tmp/ before it.",
                "upvotes": 79,
                "answered_by": "jrc",
                "answered_at": "2017-01-09 21:21"
            },
            {
                "answer": "Alternatively as a quicker means to run requirements.txt file without typing \"yes\" to confirm installation of libraries, you can re-write as:\nCOPY requirements.txt ./\nRUN pip install -y -r requirements.txt\nCOPY ./\"dir\"/* .",
                "upvotes": -4,
                "answered_by": "Asante Michael",
                "answered_at": "2020-07-04 12:48"
            }
        ]
    },
    {
        "title": "How to copy folders to docker image from Dockerfile?",
        "url": "https://stackoverflow.com/questions/37789984/how-to-copy-folders-to-docker-image-from-dockerfile",
        "votes": "169",
        "views": "467k",
        "author": "jonalv",
        "issued_at": "2016-06-13 12:34",
        "tags": [
            "docker",
            "dockerfile"
        ],
        "answers": [
            {
                "answer": "Use ADD (docs)\nThe ADD command can accept as a <src> parameter:\nA folder within the build folder (the same folder as your Dockerfile). You would then add a line in your Dockerfile like this:\nADD folder /path/inside/your/container\nor\nA single-file archive anywhere in your host filesystem. To create an archive use the command:\ntar -cvzf newArchive.tar.gz /path/to/your/folder\nYou would then add a line to your Dockerfile like this:\nADD /path/to/archive/newArchive.tar.gz  /path/inside/your/container\nNotes:\nADD will automatically extract your archive.\npresence/absence of trailing slashes is important, see the linked docs",
                "upvotes": 117,
                "answered_by": "ryanrain",
                "answered_at": "2018-10-09 16:01"
            },
            {
                "answer": "use ADD instead of COPY. Suppose you want to copy everything in directory src from host to directory dst from container:\nADD src dst\nNote: directory dst will be automatically created in container.",
                "upvotes": 35,
                "answered_by": "Hin Fan Chan",
                "answered_at": "2016-06-13 16:05"
            },
            {
                "answer": "2016: As mentioned in your ticket:\nYou have COPY files/* /test/ which expands to COPY files/dir files/file1 files/file2 files/file /test/.\nIf you split this up into individual COPY commands (e.g., COPY files/dir /test/) you'll see that (for better or worse) COPY will copy the contents of each argument dir into the destination directory. Not the argument dir itself, but the contents.\nI'm not thrilled with that fact that COPY doesn't preserve the top-level dir but its been that way for a while now.\nSo in the name of preserving a backward compatibility, it is not possible to COPY/ADD a directory structure.\nThe only workaround would be a series of RUN mkdir -p /x/y/z to build the target directory structure, followed by a series of docker ADD (one for each folder to fill).\n(ADD, not COPY, as per comments)\n2023: moby/buildkit PR 3001 proposes adding --parents opt-in flag for ADD/COPY, merged with commit 9dd188b:\nCOPY [--parents[=<boolean>]] <src>... <dest>\nThe --parents flag preserves parent directories for src entries. This flag defaults to false.\n# syntax=docker/dockerfile:1.7-labs\nFROM scratch\n\nCOPY ./x/a.txt ./y/a.txt /no_parents/\nCOPY --parents ./x/a.txt ./y/a.txt /parents/\n\n# /no_parents/a.txt\n# /parents/x/a.txt\n# /parents/y/a.txt\nThe results are encouraging:\nI just tried the feature and I have to say to everyone who has been involved: Thank you so much!\nWe have a JS monorepo and so far to split dependency installation and building into 2 stages, we had to either manually specify all package.json and lock files manually or use scripts to isolate these files in a separate folder before copying it into the build context.\nNow it's just a COPY --parents pnpm-workspace.yaml **/package.json **/pnpm-lock.yaml ./ away, and I get a super nice tree that I can just pnpm install.\nBut also:\nThe behavior was confusing because it copied files relative to the destination directory, even when the dst was specified as an absolute path and had a common path prefix with the src directory. I'm guessing this is the intended behavior.\nFor example, the following command copied the files into /usr/src/app/packages/**usr/src/app/packages**/**/dist.\nCOPY --parents --from=build /usr/src/app/packages/**/dist /usr/src/app/packages\nI managed to fix it with the following command:\nCOPY --parents --from=build /usr/src/app/packages/**/dist /\nSee COPY --parents, not yet available in stable syntax, use docker/dockerfile:1.7-labs version.",
                "upvotes": 28,
                "answered_by": "VonC",
                "answered_at": "2016-06-13 13:36"
            }
        ]
    },
    {
        "title": "Is there a way to combine Docker images into 1 container?",
        "url": "https://stackoverflow.com/questions/39626579/is-there-a-way-to-combine-docker-images-into-1-container",
        "votes": "168",
        "views": "219k",
        "author": "David",
        "issued_at": "2016-09-21 21:04",
        "tags": [
            "docker",
            "dockerfile",
            "docker-image"
        ],
        "answers": [
            {
                "answer": "You can, with the multi-stage builds feature introduced in Docker Engine 17.05\nTake a look at this:\nFROM golang:1.7.3\nWORKDIR /go/src/github.com/alexellis/href-counter/\nRUN go get -d -v golang.org/x/net/html  \nCOPY app.go .\nRUN CGO_ENABLED=0 GOOS=linux go build -a -installsuffix cgo -o app .\n\nFROM alpine:latest  \nRUN apk --no-cache add ca-certificates\nWORKDIR /root/\nCOPY --from=0 /go/src/github.com/alexellis/href-counter/app .\nCMD [\"./app\"]  \nThen build the image normally:\ndocker build -t alexellis2/href-counter:latest\nFrom : https://docs.docker.com/develop/develop-images/multistage-build/\nThe end result is the same tiny production image as before, with a significant reduction in complexity. You don\u2019t need to create any intermediate images and you don\u2019t need to extract any artifacts to your local system at all.\nHow does it work? The second FROM instruction starts a new build stage with the alpine:latest image as its base. The COPY --from=0 line copies just the built artifact from the previous stage into this new stage. The Go SDK and any intermediate artifacts are left behind, and not saved in the final image.",
                "upvotes": 157,
                "answered_by": "Mohammed Noureldin",
                "answered_at": "2018-01-06 12:55"
            },
            {
                "answer": "The following answer applies to docker 1.7 and above:\nI would prefer to use --from=NAME and from image as NAME Why? You can use --from=0 and above but this might get little hard to manage when you have many docker stages in dockerfile.\nsample example:\nFROM golang:1.7.3 as backend\nWORKDIR /backend\nRUN go get -d -v golang.org/x/net/html  \nCOPY app.go .\nRUN  #install some stuff, compile assets....\n    \nFROM golang:1.7.3 as assets\nWORKDIR /assets\nRUN ./getassets.sh\n\nFROM nodejs:latest as frontend \nRUN npm install\nWORKDIR /assets\nCOPY --from=assets /asets .\nCMD [\"./app\"] \n\nFROM alpine:latest as mergedassets\nWORKDIR /root/\nCOPY --from=frontend . /\nCOPY --from=backend ./backend .\nCMD [\"./app\"]\nNote: Managing dockerfile properly will help to build a docker image much faster. Internally docker usings docker layer caching to help with this process, incase the image have to be rebuilt.",
                "upvotes": 33,
                "answered_by": "\u0420\u0410\u0412\u0418",
                "answered_at": "2019-03-23 01:51"
            },
            {
                "answer": "Yes, you can roll a whole lot of software into a single Docker image (GitLab does this, with one image that includes Postgres and everything else), but generalhenry is right - that's not the typical way to use Docker.\nAs you say, Cassandra and Kafka are dependencies for your Scala app, they're not part of the app, so they don't all belong in the same image.\nHaving to orchestrate many containers with Docker Compose adds an extra admin layer, but it gives you much more flexibility:\nyour containers can have different lifespans, so when you have a new version of your app to deploy, you only need to run a new app container, you can leave the dependencies running;\nyou can use the same app image in any environment, using different configurations for your dependencies - e.g. in dev you can run a basic Kafka container and in prod have it clustered on many nodes, your app container is the same;\nyour dependencies can be used by other apps too - so multiple consumers can run in different containers and all work with the same Kafka and Cassandra containers;\nplus all the scalability, logging etc. already mentioned.",
                "upvotes": 11,
                "answered_by": "Elton Stoneman",
                "answered_at": "2016-09-21 21:43"
            }
        ]
    },
    {
        "title": "How to cache the RUN npm install instruction when docker build a Dockerfile",
        "url": "https://stackoverflow.com/questions/35774714/how-to-cache-the-run-npm-install-instruction-when-docker-build-a-dockerfile",
        "votes": "167",
        "views": "193k",
        "author": "ohadgk",
        "issued_at": "2016-03-03 14:19",
        "tags": [
            "node.js",
            "docker",
            "dockerfile"
        ],
        "answers": [
            {
                "answer": "Ok so I found this great article about efficiency when writing a docker file.\nThis is an example of a bad docker file adding the application code before running the RUN npm install instruction:\nFROM ubuntu\n\nRUN echo \"deb http://archive.ubuntu.com/ubuntu precise main universe\" > /etc/apt/sources.list\nRUN apt-get update\nRUN apt-get -y install python-software-properties git build-essential\nRUN add-apt-repository -y ppa:chris-lea/node.js\nRUN apt-get update\nRUN apt-get -y install nodejs\n\nWORKDIR /opt/app\n\nCOPY . /opt/app\nRUN npm install\nEXPOSE 3001\n\nCMD [\"node\", \"server.js\"]\nBy dividing the copy of the application into 2 COPY instructions (one for the package.json file and the other for the rest of the files) and running the npm install instruction before adding the actual code, any code change wont trigger the RUN npm install instruction, only changes of the package.json will trigger it. Better practice docker file:\nFROM ubuntu\nMAINTAINER David Weinstein <david@bitjudo.com>\n\n# install our dependencies and nodejs\nRUN echo \"deb http://archive.ubuntu.com/ubuntu precise main universe\" > /etc/apt/sources.list\nRUN apt-get update\nRUN apt-get -y install python-software-properties git build-essential\nRUN add-apt-repository -y ppa:chris-lea/node.js\nRUN apt-get update\nRUN apt-get -y install nodejs\n\n# use changes to package.json to force Docker not to use the cache\n# when we change our application's nodejs dependencies:\nCOPY package.json /tmp/package.json\nRUN cd /tmp && npm install\nRUN mkdir -p /opt/app && cp -a /tmp/node_modules /opt/app/\n\n# From here we load our application's code in, therefore the previous docker\n# \"layer\" thats been cached will be used if possible\nWORKDIR /opt/app\nCOPY . /opt/app\n\nEXPOSE 3000\n\nCMD [\"node\", \"server.js\"]\nThis is where the package.json file added, install its dependencies and copy them into the container WORKDIR, where the app lives:\nADD package.json /tmp/package.json\nRUN cd /tmp && npm install\nRUN mkdir -p /opt/app && cp -a /tmp/node_modules /opt/app/\nTo avoid the npm install phase on every docker build just copy those lines and change the ^/opt/app^ to the location your app lives inside the container.",
                "upvotes": 185,
                "answered_by": "ohadgk",
                "answered_at": "2016-03-03 14:21"
            },
            {
                "answer": "I've found that the simplest approach is to leverage Docker's copy semantics:\nThe COPY instruction copies new files or directories from and adds them to the filesystem of the container at the path .\nThis means that if you first explicitly copy the package.json file and then run the npm install step that it can be cached and then you can copy the rest of the source directory. If the package.json file has changed, then that will be new and it will re-run the npm install caching that for future builds.\nA snippet from the end of a Dockerfile would look like:\n# install node modules\nWORKDIR  /usr/app\nCOPY     package.json /usr/app/package.json\nRUN      npm install\n\n# install application\nCOPY     . /usr/app",
                "upvotes": 61,
                "answered_by": "J. Fritz Barnes",
                "answered_at": "2017-04-03 17:01"
            }
        ]
    },
    {
        "title": "Docker build gives \"unable to prepare context: context must be a directory: /Users/tempUser/git/docker/Dockerfile\"",
        "url": "https://stackoverflow.com/questions/43296019/docker-build-gives-unable-to-prepare-context-context-must-be-a-directory-use",
        "votes": "164",
        "views": "206k",
        "author": "Damien-Amen",
        "issued_at": "2017-04-08 15:36",
        "tags": [
            "docker",
            "dockerfile"
        ],
        "answers": [
            {
                "answer": "You need to point to the directory instead. You must not specify the dockerfile.\ndocker build -t ubuntu-test:latest . does work.\ndocker build -t ubuntu-test:latest ./Dockerfile does not work.",
                "upvotes": 248,
                "answered_by": "Damien-Amen",
                "answered_at": "2017-04-08 16:16"
            },
            {
                "answer": "Understand contexts\nThe docker build command\nThe basic syntax of docker's build command is\ndocker build -t imagename:imagetag context_dir\nThe context\nThe context is a directory and determines what the docker build process is going to see: From the Dockerfile's point of view, any file context_dir/mydir/myfile in your filesystem will become /mydir/myfile in the Dockerfile and hence during the build process.\nThe dockerfile\nIf the dockerfile is called Dockerfile and lives in the context, it will be found implicitly by naming convention. That's nice, because it means you can usually find the Dockerfile in any docker container immediately.\nIf you insist on using different name, say \"/tmp/mydockerfile\", you can use -f like this:\ndocker build -t imagename:imagetag -f /tmp/mydockerfile context_dir\nbut then the dockerfile will not be in the same folder or at least will be harder to find.",
                "upvotes": 29,
                "answered_by": "Lutz Prechelt",
                "answered_at": "2019-08-30 08:57"
            },
            {
                "answer": "To specify a Dockerfile when build, you can use:\ndocker build -t ubuntu-test:latest - < /path/to/your/Dockerfile\nBut it'll fail if there's ADD or COPY command that depends on relative path. There're many ways to specify a context for docker build, you can refer to docs of docker build for more info.",
                "upvotes": 18,
                "answered_by": "shizhz",
                "answered_at": "2017-04-09 02:21"
            }
        ]
    },
    {
        "title": "What is the difference between `docker-compose build` and `docker build`?",
        "url": "https://stackoverflow.com/questions/50230399/what-is-the-difference-between-docker-compose-build-and-docker-build",
        "votes": "163",
        "views": "185k",
        "author": "Benyamin Jafari",
        "issued_at": "2018-05-08 09:32",
        "tags": [
            "docker",
            "docker-compose",
            "dockerfile"
        ],
        "answers": [
            {
                "answer": "docker-compose can be considered a wrapper around the docker CLI (in fact it is another implementation in python as said in the comments) in order to gain time and avoid 500 characters-long lines (and also start multiple containers at the same time). It uses a file called docker-compose.yml in order to retrieve parameters.\nYou can find the reference for the docker-compose file format here.\nSo basically docker-compose build will read your docker-compose.yml, look for all services containing the build: statement and run a docker build for each one.\nEach build can specify a Dockerfile, a context and args to pass to docker.\nTo conclude with an example docker-compose.yml file:\nversion: '3.2'\n\nservices:\n  database:\n    image: mariadb\n    restart: always\n    volumes:\n      - ./.data/sql:/var/lib/mysql\n\n  web:\n    build:\n      dockerfile: Dockerfile-alpine\n      context: ./web\n    ports:\n      - 8099:80\n    depends_on:\n      - database \nWhen calling docker-compose build, only the web target will need an image to be built. The docker build command would look like:\ndocker build -t web_myproject -f Dockerfile-alpine ./web",
                "upvotes": 183,
                "answered_by": "hugoShaka",
                "answered_at": "2018-05-08 09:43"
            },
            {
                "answer": "docker-compose build will build the services in the docker-compose.yml file.\nhttps://docs.docker.com/compose/reference/build/\ndocker build will build the image defined by Dockerfile.\nhttps://docs.docker.com/engine/reference/commandline/build/",
                "upvotes": 36,
                "answered_by": "Corey Taylor",
                "answered_at": "2018-05-08 09:41"
            },
            {
                "answer": "Adding to the first answer...\nYou can give the image name and container name under the service definition.\ne.g. for the service called 'web' in the below docker-compose example, you can give the image name and container name explicitly, so that docker does not have to use the defaults.\nOtherwise the image name that docker will use will be the concatenation of the folder (Directory) and the service name. e.g. myprojectdir_web\nSo it is better to explicitly put the desired image name that will be generated when docker build command is executed.\ne.g. image: mywebserviceImage container_name: my-webServiceImage-Container\nexample docker-compose.yml file :\nversion: '3.2'\nservices:\n  web:\n    build:\n      dockerfile: Dockerfile-alpine\n      context: ./web\n    ports:\n      - 8099:80\n    image: mywebserviceImage\n    container_name: my-webServiceImage-Container\n    depends_on:\n      - database",
                "upvotes": 9,
                "answered_by": "shivbits",
                "answered_at": "2019-06-07 04:15"
            }
        ]
    },
    {
        "title": "How to view logs for a docker image?",
        "url": "https://stackoverflow.com/questions/37832575/how-to-view-logs-for-a-docker-image",
        "votes": "162",
        "views": "203k",
        "author": "Ville Miekk-oja",
        "issued_at": "2016-06-15 10:18",
        "tags": [
            "docker",
            "dockerfile",
            "docker-image"
        ],
        "answers": [
            {
                "answer": "Had the same problem, I solved it using\ndocker build --no-cache --progress=plain -t my-image .",
                "upvotes": 295,
                "answered_by": "diegocl02",
                "answered_at": "2020-10-28 22:30"
            },
            {
                "answer": "Update: Since this question has been asked, it seems everyone is finding it after seeing the output changes from buildkit. Buildkit includes the following options (docker build --help to see them all):\n      --build-arg list          Set build-time variables\n      --cache-from strings      Images to consider as cache sources\n  -f, --file string             Name of the Dockerfile (Default is 'PATH/Dockerfile')\n      --no-cache                Do not use cache when building the image\n  -o, --output stringArray      Output destination (format: type=local,dest=path)\n      --platform string         Set platform if server is multi-platform capable\n      --progress string         Set type of progress output (auto, plain, tty). Use plain to show container output (default \"auto\")\n      --pull                    Always attempt to pull a newer version of the image\n  -q, --quiet                   Suppress the build output and print image ID on success\n  -t, --tag list                Name and optionally a tag in the 'name:tag' format\n      --target string           Set the target build stage to build.\nThe option many want with buildkit is --progress=plain:\ndocker build -t my-image --progress=plain .\nIf you really want to see the previous build output, you can disable buildkit with an environment variable, but I tend to recommend against this since there are a lot of features from buildkit you'd lose (skipping unused build steps, concurrent build steps, multi-platform images, and new syntaxes for the Dockerfile for features like RUN --mount...):\nDOCKER_BUILDKIT=0 docker build -t my-image .\nThe OP is asking to include the logs of their build within the image itself. Generally I would recommend against this, you'd want those logs outside of the image.\nThat said, the easiest method for that is to use tee to send a copy of all your command output to a logfile. If you want it attached to the image, output your run commands to a logfile inside of the image with something like:\nRUN my-install-cmd | tee /logs/my-install-cmd.log\nThen you can run a quick one-off container to view the contents of those logs:\ndocker run --rm my-image cat /logs/my-install-cmd.log\nIf you don't need the logs attached to the image, you can log the output of every build with a single change to your build command (instead of lots of changes to the run commands) exactly as JHarris says:\ndocker build -t my-image . | tee my-image.build.log\nWith the classic docker build command, if you build without using --rm=true, then you have all the intermediate containers, and each one of those has a log you can review with\ndocker logs $container_id\nAnd lastly, don't forget there's a history of the layers in the image. They don't show the output of each command, but it is useful for all of those commands that don't log any output and knowing which build each layer comes from particularly when there's lots of caching being used.\ndocker history my-image",
                "upvotes": 89,
                "answered_by": "BMitch",
                "answered_at": "2016-06-15 11:38"
            },
            {
                "answer": "Use This: https://github.com/jcalles/docker-wtee\nRead instructions and please give me feedback.\nOr...\nIf you need to get logs from running container, and container has volumes exposed, run this:\ndocker run --rm -it --name testlogs --link <CONTAINERNAME/ID> --network CONTAINERNETWORK -p PORT:8080 --volumes-from CONTAINERNAME/ID  javiercalles/wtee sh",
                "upvotes": -9,
                "answered_by": "Javier Calles",
                "answered_at": "2018-03-09 19:46"
            }
        ]
    },
    {
        "title": "npm WARN old lockfile The package-lock.json file was created with an old version of npm",
        "url": "https://stackoverflow.com/questions/68260784/npm-warn-old-lockfile-the-package-lock-json-file-was-created-with-an-old-version",
        "votes": "159",
        "views": "351k",
        "author": "semural",
        "issued_at": "2021-07-05 18:40",
        "tags": [
            "node.js",
            "docker",
            "npm",
            "dockerfile"
        ],
        "answers": [
            {
                "answer": "There are several ways to deal with this. (People really seem to like #4.)\nIgnore it. It's just a warning and does not affect the installation of modules.\nRun npm install --package-lock-only (with the newer version of npm) to regenerate a package-lock.json. Commit the updated version of package-lock.json to the repo/Docker image or whatever.\nDowngrade npm to an older version in production. Consider running npm version 6 as that is what ships with the current (as of this writing) Long Term Support (LTS) version of Node.js. In the case being asked about in this question, I imagine you can just leave out the RUN npm -g install npm@7.19.1 from the Dockerfile and instead use the version of npm that is installed with the Docker image (which in this case will almost certainly be npm@6 since that is what ships with Node.js 14.x).\nIf you already have a version of npm installed but want to run one command with an older version of npm but otherwise keep the newer version, you can use npx (which ships with npm) to do that. For example, npx npm@6 ci would run npm ci with npm version 6 even if you have version 7 installed.",
                "upvotes": 215,
                "answered_by": "Trott",
                "answered_at": "2021-07-22 04:27"
            },
            {
                "answer": "An easy solution to this is to use NVM to manage your node versions. Especially on Linux this saves a lot of trouble with file permissions, developing in different environments, etc. NPM recommends this in their documentation.\nThis error for me was solved by switching Node.js versions with nvm,\nnvm install 14\nnvm use 14\nIt is always an easy thing to try and switch to a slightly older or newer Node.js version if you are running into weird Node.js or npm issues.",
                "upvotes": 10,
                "answered_by": "Greggory Wiley",
                "answered_at": "2021-11-12 16:18"
            },
            {
                "answer": "I am having the same problem as well after upgrading my npm version. It seems like a bug from npm 7.19.1, and I'd suggest to downgrade to an older version.\nYou can check below for all the npm versions\nhttps://www.npmjs.com/package/npm?activeTab=versions\nInstall the desired version with this command in the console, and substitute \"V\" with your desired version:\nnpm install -g npm@\"V\"",
                "upvotes": 5,
                "answered_by": "Eleonor",
                "answered_at": "2021-07-10 12:39"
            }
        ]
    },
    {
        "title": "Difference between Docker ENTRYPOINT and Kubernetes container spec COMMAND?",
        "url": "https://stackoverflow.com/questions/44316361/difference-between-docker-entrypoint-and-kubernetes-container-spec-command",
        "votes": "157",
        "views": "146k",
        "author": "tusharfloyd",
        "issued_at": "2017-06-01 20:15",
        "tags": [
            "docker",
            "containers",
            "kubernetes",
            "dockerfile"
        ],
        "answers": [
            {
                "answer": "Kubernetes provides us with multiple options on how to use these commands:\nWhen you override the default Entrypoint and Cmd in Kubernetes .yaml file, these rules apply:\nIf you do not supply command or args for a Container, the defaults defined in the Docker image are used.\nIf you supply only args for a Container, the default Entrypoint defined in the Docker image is run with the args that you supplied.\nIf you supply a command for a Container, only the supplied command is used. The default EntryPoint and the default Cmd defined in the Docker image are ignored. Your command is run with the args supplied (or no args if none supplied).\nHere is an example:\nDockerfile:\nFROM alpine:latest\nCOPY \"executable_file\" /\nENTRYPOINT [ \"./executable_file\" ]\nKubernetes yaml file:\n spec:\n    containers:\n      - name: container_name\n        image: image_name\n        args: [\"arg1\", \"arg2\", \"arg3\"]\nhttps://kubernetes.io/docs/tasks/inject-data-application/define-command-argument-container/",
                "upvotes": 237,
                "answered_by": "Berk Soysal",
                "answered_at": "2018-04-04 17:29"
            },
            {
                "answer": "The key difference is terminology. Kubernetes thought that the terms that Docker used to define the interface to a container were awkward, and so they used different, overlapping terms. Since the vast majority of containers Kubernetes orchestrates are Docker, confusion abounds.\nSpecifically, docker entrypoints are kubernetes commands, and docker commands are kubernetes args, as indicated here.\n-------------------------------------------------------------------------------------\n| Description                           | Docker field name | Kubernetes field name |\n-------------------------------------------------------------------------------------\n| The command run by the container      | Entrypoint        | command               |\n| The arguments passed to the command   | Cmd               | args                  |\n-------------------------------------------------------------------------------------\n@Berk's description of how Kubernetes uses those runtime options is correct, but it's also correct for how docker run uses them, as long as you translate the terms. The key is to understand the interplay between image and run specifications in either system, and to translate terms whenever speaking of the other.",
                "upvotes": 131,
                "answered_by": "billkw",
                "answered_at": "2018-04-04 18:44"
            },
            {
                "answer": "Basically the COMMAND can override what is mentioned in the docker ENTRYPOINT\nSimple example:\nTo override the dockerfile ENTRYPOINT, just add these fields to your K8s template (Look at the command and args):\napiVersion: v1\nkind: Pod\nmetadata:\n  name: command-demo\n  labels:\n    purpose: demonstrate-command\nspec:\n  containers:\n  - name: command-demo-container\n    image: debian\n    command: [\"/bin/sh\"]\n    args: [\"-c\", \"printenv; #OR WHATEVER COMMAND YOU WANT\"]\n  restartPolicy: OnFailure\nK8s docs:\ncommand field corresponds to entrypoint in some container runtimes. Refer to the Notes below.\nYou can enter Notes link (K8s documentation for better understanding on how this command overrides the K8s ENTRYPOINT)",
                "upvotes": 20,
                "answered_by": "Mercury",
                "answered_at": "2021-04-06 10:50"
            }
        ]
    },
    {
        "title": "Docker expose all ports or range of ports from 7000 to 8000",
        "url": "https://stackoverflow.com/questions/28717464/docker-expose-all-ports-or-range-of-ports-from-7000-to-8000",
        "votes": "157",
        "views": "175k",
        "author": "DarVar",
        "issued_at": "2015-02-25 11:09",
        "tags": [
            "docker",
            "dockerfile"
        ],
        "answers": [
            {
                "answer": "Since Docker 1.5 you can now expose a range of ports to other linked containers using:\nThe Dockerfile EXPOSE command:\nEXPOSE 7000-8000\nor The Docker run command:\ndocker run --expose=7000-8000\nOr instead you can publish a range of ports to the host machine via Docker run command:\ndocker run -p 7000-8000:7000-8000",
                "upvotes": 208,
                "answered_by": "DarVar",
                "answered_at": "2015-02-25 14:31"
            }
        ]
    },
    {
        "title": "E: Package 'mysql-client' has no installation candidate in php-fpm image build using docker compose",
        "url": "https://stackoverflow.com/questions/57048428/e-package-mysql-client-has-no-installation-candidate-in-php-fpm-image-build-u",
        "votes": "156",
        "views": "104k",
        "author": "surgiie",
        "issued_at": "2019-07-15 23:52",
        "tags": [
            "laravel",
            "docker",
            "docker-compose",
            "dockerfile"
        ],
        "answers": [
            {
                "answer": "php:7.2-apache triggers the error as well, but I resolve it using php:7.2.18-apache",
                "upvotes": 3,
                "answered_by": "Yassine CHABLI",
                "answered_at": "2019-09-12 10:37"
            },
            {
                "answer": "it worked for me: sudo apt-get update && apt-get install -y git curl libmcrypt-dev default-mysql-client\nor alternatively apt-cache search mysql-server find out your servers then sudo apt-get install default-mysql-server default-mysql-server-core mariadb-server-10.6  mariadb-server-core-10.6  in my case it was the above codes",
                "upvotes": -3,
                "answered_by": "lloyd tony",
                "answered_at": "2022-05-04 18:48"
            }
        ]
    },
    {
        "title": "Can we pass ENV variables through cmd line while building a docker image through dockerfile?",
        "url": "https://stackoverflow.com/questions/31198835/can-we-pass-env-variables-through-cmd-line-while-building-a-docker-image-through",
        "votes": "156",
        "views": "161k",
        "author": "Aniketh",
        "issued_at": "2015-07-03 05:01",
        "tags": [
            "docker",
            "environment-variables",
            "boot2docker",
            "dockerfile"
        ],
        "answers": [
            {
                "answer": "Containers can be built using build arguments (in Docker 1.9+) which work like environment variables.\nHere is the method:\nFROM php:7.0-fpm\nARG APP_ENV=local\nENV APP_ENV=${APP_ENV}\nRUN cd /usr/local/etc/php && ln -sf php.ini-${APP_ENV} php.ini\nand then build a production container:\ndocker build --build-arg APP_ENV=prod .\nFor your particular problem:\nFROM debian\nENV http_proxy=${http_proxy}\nand then run:\ndocker build --build-arg http_proxy=10.11.24.31 .\nNote that if you build your containers with docker-compose, you can specify these build-args in the docker-compose.yml file, but not on the command-line. However, you can use variable substitution in the docker-compose.yml file, which uses environment variables.",
                "upvotes": 216,
                "answered_by": "Sin30",
                "answered_at": "2015-12-31 11:16"
            },
            {
                "answer": "So I had to hunt this down by trial and error as many people explain that you can pass ARG -> ENV but it doesn't always work as it highly matters whether the ARG is defined before or after the FROM tag.\nThe below example should explain this clearly. My main problem originally was that all of my ARGS were defined prior to FROM which resulted all the ENV to be undefined always.\n# ARGS PRIOR TO FROM TAG ARE AVAIL ONLY TO FROM for dynamic a FROM tag\nARG NODE_VERSION\nFROM node:${NODE_VERSION}-alpine\n\n# ARGS POST FROM can bond/link args to env to make the containers environment dynamic\nARG NPM_AUTH_TOKEN\nARG EMAIL\nARG NPM_REPO\n\nENV NPM_AUTH_TOKEN=${NPM_AUTH_TOKEN}\nENV EMAIL=${EMAIL}\nENV NPM_REPO=${NPM_REPO}\n\n# for good measure, what do we really have\nRUN echo NPM_AUTH_TOKEN: $NPM_AUTH_TOKEN && \\\n  echo EMAIL: $EMAIL && \\\n  echo NPM_REPO: $NPM_REPO && \\\n  echo $HI_5\n# remember to change HI_5 every build to break `docker build`'s cache if you want to debug the stdout\n\n..... # rest of whatever you want RUN, CMD, ENTRYPOINT etc..",
                "upvotes": 60,
                "answered_by": "Nick",
                "answered_at": "2019-11-27 19:14"
            }
        ]
    },
    {
        "title": "ARG substitution in RUN command not working for Dockerfile",
        "url": "https://stackoverflow.com/questions/44438637/arg-substitution-in-run-command-not-working-for-dockerfile",
        "votes": "155",
        "views": "120k",
        "author": "user3139545",
        "issued_at": "2017-06-08 14:34",
        "tags": [
            "docker",
            "dockerfile"
        ],
        "answers": [
            {
                "answer": "Another thing to be careful about is that after every FROM statement, all the ARGs get collected and are no longer available. Be careful with multi-stage builds.\nYou can reuse ARG with omitted default value inside FROM to get through this problem:\nARG VERSION=latest\nFROM busybox:$VERSION\nARG VERSION\nRUN echo $VERSION > image_version\nExample taken from docs: https://docs.docker.com/engine/reference/builder/#understand-how-arg-and-from-interact",
                "upvotes": 406,
                "answered_by": "Hongtao Yang",
                "answered_at": "2019-06-25 06:49"
            },
            {
                "answer": "I had the same problem using Windows containers for Windows.\nInstead of doing this (Which works in linux containers)\nFROM alpine\nARG TARGETPLATFORM\nRUN echo \"I'm building for $TARGETPLATFORM\"\nYou need to do this\nFROM mcr.microsoft.com/windows/servercore\nARG TARGETPLATFORM\nRUN echo \"I'm building for %TARGETPLATFORM%\"\nJust change the variable resolution according to the OS.",
                "upvotes": 37,
                "answered_by": "Justin Lessard",
                "answered_at": "2019-05-28 18:34"
            },
            {
                "answer": "I spent much time to have the argument substitution working, but the solution was really simple. The substitution within RUN needs the argument reference to be enclosed in double quotes.\nARG CONFIGURATION=Debug\nRUN dotnet publish \"Project.csproj\" -c \"$CONFIGURATION\" -o /app/publish",
                "upvotes": 20,
                "answered_by": "Oleg",
                "answered_at": "2019-03-13 16:35"
            }
        ]
    },
    {
        "title": "How can I set Bash aliases for docker containers in Dockerfile?",
        "url": "https://stackoverflow.com/questions/36388465/how-can-i-set-bash-aliases-for-docker-containers-in-dockerfile",
        "votes": "153",
        "views": "166k",
        "author": "np20",
        "issued_at": "2016-04-03 17:09",
        "tags": [
            "bash",
            "unix",
            "docker",
            "alias",
            "dockerfile"
        ],
        "answers": [
            {
                "answer": "Basically like you always do, by adding it to the user's .bashrc file:\nFROM foo\nRUN echo 'alias hi=\"echo hello\"' >> ~/.bashrc\nAs usual this will only work for interactive shells:\ndocker build -t test .\ndocker run -it --rm --entrypoint /bin/bash test hi\n/bin/bash: hi: No such file or directory\ndocker run -it --rm test bash\n$ hi\nhello\nFor non-interactive shells you should create a small script and put it in your path, i.e.:\nRUN echo -e '#!/bin/bash\\necho hello' > /usr/bin/hi && \\\n    chmod +x /usr/bin/hi\nIf your alias uses parameters (ie. hi Jim -> hello Jim), just add \"$@\":\nRUN echo -e '#!/bin/bash\\necho hello \"$@\"' > /usr/bin/hi && \\\n    chmod +x /usr/bin/hi",
                "upvotes": 269,
                "answered_by": "Erik Dannenberg",
                "answered_at": "2016-04-03 17:44"
            },
            {
                "answer": "To create an alias of an existing command, might also use ln -s:\nln -s $(which <existing_command>) /usr/bin/<my_command>",
                "upvotes": 27,
                "answered_by": "Laurent Magnin",
                "answered_at": "2017-07-11 18:47"
            },
            {
                "answer": "If you want to use aliases just in Dockerfile, but not inside a container then the shortest way is the ENV declaration:\nENV update='apt-get update -qq'\nENV install='apt-get install -qq'\n\nRUN $update && $install apt-utils \\\n    curl \\\n    gnupg \\\n    python3.6\nAnd for use in a container the way like already described:\n RUN printf '#!/bin/bash \\n $(which apt-get) install -qq $@' > /usr/bin/install\n RUN chmod +x /usr/bin/install\nMost of the time I use aliases just in the building stage and do not go inside containers, so the first example is quicker, clearer and simpler for every day use.",
                "upvotes": 8,
                "answered_by": "Sonique",
                "answered_at": "2018-07-17 22:13"
            },
            {
                "answer": "I think the easiest way would be to mount a file into your container containing your aliases, and then specify where Bash should find it:\ndocker run \\\n    -it \\\n    --rm \\\n    -v ~/.bash_aliases:/tmp/.bash_aliases \\\n    [image] \\\n    /bin/bash --init-file /tmp/.bash_aliases\nSample usage:\necho 'alias what=\"echo it works\"' > my_aliases\ndocker run -it --rm -v ~/my_aliases:/tmp/my_aliases ubuntu:18.04 /bin/bash --init-file /tmp/my_aliases\nalias\nOutput:\nalias what='echo it works'\nwhat\nOutput:\nit works",
                "upvotes": 7,
                "answered_by": "Gillespie",
                "answered_at": "2020-01-16 15:25"
            }
        ]
    },
    {
        "title": "Docker Copy and change owner",
        "url": "https://stackoverflow.com/questions/28879364/docker-copy-and-change-owner",
        "votes": "152",
        "views": "199k",
        "author": "Christian Metzler",
        "issued_at": "2015-03-05 13:49",
        "tags": [
            "docker",
            "file-permissions",
            "dockerfile"
        ],
        "answers": [
            {
                "answer": "A --chown flag has finally been added to COPY:\nCOPY --chown=patrick hostPath containerPath\nThis new syntax seems to work on Docker 17.09.\nSee the PR for more information.",
                "upvotes": 361,
                "answered_by": "Georgi Hristozov",
                "answered_at": "2017-10-03 08:46"
            },
            {
                "answer": "I think I found a solution, which works. Using a data volume container will do the trick. First I create the Data Volume Container, which contains the copy of my external directory:\nFROM busybox\nRUN mkdir /data\nVOLUME /data\nCOPY /test /data/test\nCMD /bin/sh\nIn my application container, where I have my users, which could look something like this\nFROM ubuntu\nRUN groupadd mygroup\nRUN useradd -ms /bin/bash -G mygroup john\nCOPY setpermissions.sh /root/setpermissions.sh\nCMD /root/setpermissions.sh && /bin/bash\nThe setpermissions script does the job of setting the user permissions:\n#!/bin/bash\n\nif [ ! -e /data/.bootstrapped ] ; then\n  chown -R john:mygroup /data\n  touch /data/.bootstrapped\nfi\nNow I just have to use the --volumes-from <myDataContainerId> when running the application container.",
                "upvotes": 10,
                "answered_by": "Christian Metzler",
                "answered_at": "2015-03-06 07:58"
            },
            {
                "answer": "Docker Copy and change owner (for Windows Container)\nFROM mcr.microsoft.com/windows/servercore:ltsc2019\nWORKDIR /src\nCOPY . /src \nRUN takeown /F . /d Y /r\nFor other user as owner, add /u <username>. For more detail see reference below.\nReference:\ntakeown",
                "upvotes": 1,
                "answered_by": "Yeo",
                "answered_at": "2023-04-12 10:14"
            }
        ]
    },
    {
        "title": "Docker - image operating system \"windows\" cannot be used on this platform",
        "url": "https://stackoverflow.com/questions/43346580/docker-image-operating-system-windows-cannot-be-used-on-this-platform",
        "votes": "151",
        "views": "116k",
        "author": "Lea A",
        "issued_at": "2017-04-11 12:56",
        "tags": [
            "docker",
            "docker-compose",
            "dockerfile",
            "boot2docker",
            "docker-machine"
        ],
        "answers": [
            {
                "answer": "Your Docker host is configured to run Linux containers inside of a VM. To run Windows containers, you need to right click on the Docker icon in the system tray, and select \"Switch to Windows containers\u2026\" in the Docker menu. This option is not available in \"Home\" versions of Windows. Documentation is available here.",
                "upvotes": 215,
                "answered_by": "BMitch",
                "answered_at": "2017-04-11 19:45"
            },
            {
                "answer": "You need to go to the Taskbar \u2192 right click the Docker icon \u2192 use option Switch to Windows containers...\nSource https://docs.docker.com/docker-for-windows/",
                "upvotes": 50,
                "answered_by": "pbaranski",
                "answered_at": "2017-12-15 12:16"
            },
            {
                "answer": "Switch to Windows Container needs to selected from docker icon running under hidden icon from the bottom right... The moment you switch from Linux to Windows or Windows to Linux, Docker daemon automatically restarts to consider switched container...\nI would highly recommend you to view these 2 links to get more insight into how to create window containers:\nForum post\nBlog post\nI really found the content of these links very helpful to make a window container which is still being enhanced for the generic issue we are all are facing since support for window container is yet to mature like Linux containers!",
                "upvotes": 12,
                "answered_by": "Abhishek Jain",
                "answered_at": "2018-10-06 09:19"
            }
        ]
    },
    {
        "title": "How to remove entrypoint from parent Image in Dockerfile",
        "url": "https://stackoverflow.com/questions/40122152/how-to-remove-entrypoint-from-parent-image-in-dockerfile",
        "votes": "150",
        "views": "68k",
        "author": "fsword",
        "issued_at": "2016-10-19 04:29",
        "tags": [
            "docker",
            "dockerfile"
        ],
        "answers": [
            {
                "answer": "Per the discussion here, you should be able to reset the entrypoint with\nENTRYPOINT []",
                "upvotes": 218,
                "answered_by": "Roman",
                "answered_at": "2016-10-19 04:49"
            }
        ]
    },
    {
        "title": "How can I use a local file on container?",
        "url": "https://stackoverflow.com/questions/44876778/how-can-i-use-a-local-file-on-container",
        "votes": "150",
        "views": "179k",
        "author": "zanini",
        "issued_at": "2017-07-03 01:54",
        "tags": [
            "docker",
            "dockerfile",
            "boot2docker",
            "docker-machine"
        ],
        "answers": [
            {
                "answer": "Yes, you can do this. What you are describing is a bind mount. See https://docs.docker.com/storage/bind-mounts/ for documentation on the subject.\nFor example, if I want to mount a folder from my home directory into /mnt/mydata in a container, I can do:\ndocker run -v /Users/andy/mydata:/mnt/mydata myimage\nNow, /mnt/mydata inside the container will have access to /Users/andy/mydata on my host.\nKeep in mind, if you are using Docker for Mac or Docker for Windows there are specific directories on the host that are allowed by default:\nIf you are using Docker Machine on Mac or Windows, your Docker Engine daemon has only limited access to your macOS or Windows filesystem. Docker Machine tries to auto-share your /Users (macOS) or C:\\Users (Windows) directory. So, you can mount files or directories on macOS using.\nUpdate July 2019:\nI've updated the documentation link and naming to be correct. These type of mounts are called \"bind mounts\". The snippet about Docker for Mac or Windows no longer appears in the documentation but it should still apply. I'm not sure why they removed it (my Docker for Mac still has an explicit list of allowed mounting paths on the host).",
                "upvotes": 193,
                "answered_by": "Andy Shinn",
                "answered_at": "2017-07-03 02:15"
            }
        ]
    },
    {
        "title": "Docker: What is the default WORKDIR in a Dockerfile?",
        "url": "https://stackoverflow.com/questions/37782488/docker-what-is-the-default-workdir-in-a-dockerfile",
        "votes": "145",
        "views": "142k",
        "author": "Freewind",
        "issued_at": "2016-06-13 05:40",
        "tags": [
            "docker",
            "dockerfile",
            "workdir"
        ],
        "answers": [
            {
                "answer": "The default working directory for running binaries within a container is the root directory (/), but the developer can set a different directory with the Dockerfile WORKDIR command. The operator can override this with:\n-w=\"\": Working directory inside the container\nHere: https://docs.docker.com/engine/reference/run/#workdir",
                "upvotes": 26,
                "answered_by": "hvardhan",
                "answered_at": "2016-06-13 05:46"
            }
        ]
    },
    {
        "title": "Can a Dockerfile extend another one?",
        "url": "https://stackoverflow.com/questions/36362233/can-a-dockerfile-extend-another-one",
        "votes": "143",
        "views": "142k",
        "author": "Sylvain",
        "issued_at": "2016-04-01 17:19",
        "tags": [
            "docker",
            "dockerfile"
        ],
        "answers": [
            {
                "answer": "Using multi-stage build is definitely one part of the answer here.\ndocker-compose v3.4 target being the second and last.\nHere is a example to have 2 containers (1 normal & 1 w/ xdebug installed) living together :\nDockerfile\nFROM php:7-fpm AS php_base \nENV DEBIAN_FRONTEND noninteractive\n\nRUN apt-get update && \\\n    apt-get install -y git libicu-dev libmagickwand-dev libmcrypt-dev libcurl3-dev jpegoptim\nRUN pecl install imagick && \\\n    docker-php-ext-enable imagick\n\nRUN docker-php-ext-install intl\nRUN docker-php-ext-install pdo_mysql\nRUN docker-php-ext-install opcache\nRUN docker-php-ext-install mcrypt\nRUN docker-php-ext-install curl\nRUN docker-php-ext-install zip\n\nFROM php_base AS php_test\n\nRUN pecl install xdebug \nRUN docker-php-ext-enable xdebug\ndocker-compose.yml\nversion: '3.4'\n\nservices:\n  php:\n    build:\n      context: ./\n      target: php_base\n\n  php_test:\n    build:\n      context: ./\n      target: php_test\n  \n# ...",
                "upvotes": 109,
                "answered_by": "Cethy",
                "answered_at": "2018-11-13 20:42"
            }
        ]
    },
    {
        "title": "Single file volume mounted as directory in Docker",
        "url": "https://stackoverflow.com/questions/34134343/single-file-volume-mounted-as-directory-in-docker",
        "votes": "141",
        "views": "142k",
        "author": "TTT",
        "issued_at": "2015-12-07 13:07",
        "tags": [
            "linux",
            "docker",
            "dockerfile"
        ],
        "answers": [
            {
                "answer": "test is the name of your image that you have built with 'docker build -t test', not a /test folder.\nTry a Dockerfile with:\nCMD [\"ls\", \"-lah\", \"/\"]\nor\nCMD [\"cat\", \"/file.json\"]\nAnd:\ndocker run --rm -it -v $(pwd)/file.json:/file.json test\nNote the use of $(pwd) in order to mount a file with its full absolute path (relative paths are not supported)\nBy using $(pwd), you will get an absolute path which does exists, and respect the case, as opposed to a file name or path which might not exist.\nAn non-existing host path would be mounted as a folder in the container.",
                "upvotes": 65,
                "answered_by": "VonC",
                "answered_at": "2015-12-07 14:18"
            },
            {
                "answer": "When running docker inside docker (by mounting /var/run/docker.sock for example), you need to be aware that if you do mounts inside docker, the filepaths that are used are always the one on your host.\nSo if on your host you do the following mount :\n-v /tmp/foobar.txt:/my/path/foobar.txt\nyou should not do the following mount inside docker :\n-v /my/path/foobar.txt:/my/other/path.txt\nbut instead, use the host filepath, eg :\n-v /tmp/foobar.txt:/my/other/path.txt",
                "upvotes": 39,
                "answered_by": "edi9999",
                "answered_at": "2019-04-02 18:38"
            }
        ]
    },
    {
        "title": "Dockerfile: how to redirect the output of a RUN command to a variable?",
        "url": "https://stackoverflow.com/questions/34213837/dockerfile-how-to-redirect-the-output-of-a-run-command-to-a-variable",
        "votes": "139",
        "views": "439k",
        "author": "meallhour",
        "issued_at": "2015-12-10 23:45",
        "tags": [
            "docker",
            "dockerfile"
        ],
        "answers": [
            {
                "answer": "You cannot save a variable for later use in other Dockerfile commands (if that is your intention). This is because each RUN happens in a new shell.\nHowever, if you just want to capture the output of ls you should be able to do it in one RUN compound command. For example:\nRUN file=\"$(ls -1 /tmp/dir)\" && echo $file\nOr just using the subshell inline:\nRUN echo $(ls -1 /tmp/dir)\nIf you have an actual error or problem to solve I could expand on this instead of a hypothetical answer.\nA full example Dockerfile demonstrating this would be:\nFROM alpine:3.7\nRUN mkdir -p /tmp/dir && touch /tmp/dir/file1 /tmp//dir/file2\nRUN file=\"$(ls -1 /tmp/dir)\" && echo $file\nRUN echo $(ls -1 /tmp/dir)\nWhen building you should see steps 3 and 4 output the variable (which contains the list of file1 and file2 creating in step 2). The option for --progress plain forces the output to show the steps in later version of Docker:\n$ docker build --no-cache --progress plain -t test .\nSending build context to Docker daemon  2.048kB\nStep 1/4 : FROM alpine:3.7\n ---> 3fd9065eaf02\nStep 2/4 : RUN mkdir -p /tmp/dir && touch /tmp/dir/file1 /tmp//dir/file2\n ---> Running in abb2fe683e82\nRemoving intermediate container abb2fe683e82\n ---> 2f6dfca9385c\nStep 3/4 : RUN file=\"$(ls -1 /tmp/dir)\" && echo $file\n ---> Running in 060a285e3d8a\nfile1 file2\nRemoving intermediate container 060a285e3d8a\n ---> 2e4cc2873b8c\nStep 4/4 : RUN echo $(ls -1 /tmp/dir)\n ---> Running in 528fc5d6c721\nfile1 file2\nRemoving intermediate container 528fc5d6c721\n ---> 1be7c54e1f29\nSuccessfully built 1be7c54e1f29\nSuccessfully tagged test:latest",
                "upvotes": 223,
                "answered_by": "Andy Shinn",
                "answered_at": "2015-12-11 02:46"
            },
            {
                "answer": "Just highlight the answer given in the comments, which is probably the correct one if you are using a modern version of Docker (in my case v20.10.5) and the logs do not show the expected output, when, for example, you run RUN ls.\nYou should use the option --progress <string> in the docker build command:\n --progress string         Set type of progress output (auto, plain, tty). Use plain to show container output\n                            (default \"auto\")\nFor example:\ndocker build --progress=plain .\nIn the latest versions of docker, the classic build engine that docker ships with has been upgraded to Buildkit, which displays different information.\nYou should see output like:\n#12 [8/8] RUN ls -alh\n#12 sha256:a8cf7b9a7b1f3dc25e3a97700d4cc3d3794862437a5fe2e39683ab229474746c\n#12 0.174 total 184K\n#12 0.174 drwxr-xr-x    1 root     root        4.0K Mar 28 19:37 .\n#12 0.174 drwxr-xr-x    1 root     root        4.0K Mar 28 19:35 ..\n#12 0.174 drwxr-xr-x  374 root     root       12.0K Mar 28 19:37 node_modules\n#12 0.174 -rw-r--r--    1 root     root        1.1K Mar 28 19:36 package.json\n#12 0.174 -rw-r--r--    1 root     root         614 Mar 28 15:48 server.js\n#12 0.174 -rw-r--r--    1 root     root      149.5K Mar 28 16:54 yarn.lock\n#12 DONE 0.2s\nAs noted by @venimus in the comments, you may also need --no-cache because cached containers do not show any output.\nRelated question with more details.",
                "upvotes": 82,
                "answered_by": "Pablo EM",
                "answered_at": "2022-01-07 11:39"
            },
            {
                "answer": "docker compose alternative to:\ndocker build --progress=plain .\nis\nBUILDKIT_PROGRESS=plain docker compose build\nor in compose.yml file\nservices:\n    build:\n        progress: plain",
                "upvotes": 9,
                "answered_by": "zemil",
                "answered_at": "2022-11-01 13:40"
            }
        ]
    },
    {
        "title": "Install packages in Alpine docker",
        "url": "https://stackoverflow.com/questions/48281323/install-packages-in-alpine-docker",
        "votes": "139",
        "views": "183k",
        "author": "Ankur100",
        "issued_at": "2018-01-16 12:28",
        "tags": [
            "linux",
            "docker",
            "dependencies",
            "dockerfile",
            "alpine-linux"
        ],
        "answers": [
            {
                "answer": "The equivalent of apt or apt-get in Alpine is apk\nA typical Dockerfile will contain, for example:\nRUN apk add --no-cache wget\n--no-cache is the equivalent to: apk add wget && rm -rf /var/cache/apk/*\nor, before the --no-cache option was available:\nRUN apk update && apk add wget\nAlpine rm -rf /var/cache/apk/* has the Debian equivalent rm -rf /var/lib/apt/lists/*.\nSee the Alpine comparison with other distros for more details.",
                "upvotes": 279,
                "answered_by": "user2915097",
                "answered_at": "2018-01-16 12:58"
            }
        ]
    },
    {
        "title": "Conditional ENV in Dockerfile",
        "url": "https://stackoverflow.com/questions/37057468/conditional-env-in-dockerfile",
        "votes": "139",
        "views": "163k",
        "author": "Matthew Herbst",
        "issued_at": "2016-05-05 18:13",
        "tags": [
            "docker",
            "dockerfile"
        ],
        "answers": [
            {
                "answer": "Yes, it is possible, but you need to use your build argument as flag. You can use parameter expansion feature of shell to check condition. Here is a proof-of-concept Docker file:\nFROM debian:stable\nARG BUILD_DEVELOPMENT\n# if --build-arg BUILD_DEVELOPMENT=1, set NODE_ENV to 'development' or set to null otherwise.\nENV NODE_ENV=${BUILD_DEVELOPMENT:+development}\n# if NODE_ENV is null, set it to 'production' (or leave as is otherwise).\nENV NODE_ENV=${NODE_ENV:-production}\nTesting build:\ndocker build --rm -t env_prod ./\n...\ndocker run -it env_prod bash\nroot@2a2c93f80ad3:/# echo $NODE_ENV \nproduction\nroot@2a2c93f80ad3:/# exit\ndocker build --rm -t env_dev --build-arg BUILD_DEVELOPMENT=1 ./\n...\ndocker run -it env_dev bash\nroot@2db6d7931f34:/# echo $NODE_ENV\ndevelopment",
                "upvotes": 122,
                "answered_by": "Ruslan Kabalin",
                "answered_at": "2018-07-10 11:56"
            },
            {
                "answer": "You cannot run bash code in the Dockerfile directly, but you have to use the RUN command. So, for example, you can change ENV with RUN and export the variable in the if, like below:\nARG BUILDVAR=sad \nRUN if [ \"$BUILDVAR\" = \"SO\" ]; \\\n    then export SOMEVAR=hello; \\\n    else export SOMEVAR=world; \\\n    fi \nI didn't try it but should work.",
                "upvotes": 27,
                "answered_by": "Mario Cairone",
                "answered_at": "2016-05-05 19:06"
            },
            {
                "answer": "While you can't set conditional ENV variables but you may be able to acomplish what you are after with the RUN command and a null-coalescing environment variable:\nRUN node /var/app/current/index.js --env ${BUILD_ENV:-${NODE_ENV:-\"development\"}}",
                "upvotes": 6,
                "answered_by": "Steven de Salas",
                "answered_at": "2016-10-26 04:41"
            }
        ]
    },
    {
        "title": "How to copy file from host to container using Dockerfile",
        "url": "https://stackoverflow.com/questions/30455036/how-to-copy-file-from-host-to-container-using-dockerfile",
        "votes": "133",
        "views": "351k",
        "author": "Sasikiran Vaddi",
        "issued_at": "2015-05-26 09:46",
        "tags": [
            "docker",
            "dockerfile",
            "boot2docker"
        ],
        "answers": [
            {
                "answer": "Use COPY command like this:\nCOPY foo.txt /data/foo.txt\n# where foo.txt is the relative path on host\n# and /data/foo.txt is the absolute path in the image\nread more details for COPY in the official documentation\nAn alternative would be to use ADD but this is not the best practise if you dont want to use some advanced features of ADD like decompression of tar.gz files.If you still want to use ADD command, do it like this:\nADD abc.txt /data/abc.txt\n# where abc.txt is the relative path on host\n# and /data/abc.txt is the absolute path in the image\nread more details for ADD in the official documentation",
                "upvotes": 185,
                "answered_by": "george.yord",
                "answered_at": "2015-05-29 09:34"
            },
            {
                "answer": "I faced this issue, I was not able to copy zeppelin [1GB] directory into docker container and was getting issue\nCOPY failed: stat /var/lib/docker/tmp/docker-builder977188321/zeppelin-0.7.2-bin-all: no such file or directory\nI am using docker Version: 17.09.0-ce and resolved the issue with the following steps.\nStep 1: copy zeppelin directory [which i want to copy into docker package]into directory contain \"Dockfile\"\nStep 2: edit Dockfile and add command [location where we want to copy] ADD ./zeppelin-0.7.2-bin-all /usr/local/\nStep 3: go to directory which contain DockFile and run command [alternatives also available] docker build\nStep 4: docker image created Successfully with logs\nStep 5/9 : ADD ./zeppelin-0.7.2-bin-all /usr/local/ ---> 3691c902d9fe\nStep 6/9 : WORKDIR $ZEPPELIN_HOME ---> 3adacfb024d8 .... Successfully built b67b9ea09f02",
                "upvotes": 10,
                "answered_by": "Rajeev Rathor",
                "answered_at": "2017-12-07 12:55"
            }
        ]
    },
    {
        "title": "Copy multiple directories with one command",
        "url": "https://stackoverflow.com/questions/37715224/copy-multiple-directories-with-one-command",
        "votes": "133",
        "views": "79k",
        "author": "Claudiu",
        "issued_at": "2016-06-09 00:49",
        "tags": [
            "docker",
            "copy",
            "dockerfile"
        ],
        "answers": [
            {
                "answer": "As BMitch answered, that is expected COPY behaviour.\nAn alternative would be to ADD the contents of a tarball.\nCreate the initial tarball\ntar -cvf dirs.tar dirone/ dirtwo/ dirthree/\nAdd it to the build\nFROM busybox\nADD dirs.tar /\nCMD find /dirone /dirtwo /dirthree\nThe tarball is automatically extracted\n\u25cb \u2192docker run c28f96eadd58\n/dirone\n/dirone/one\n/dirtwo\n/dirtwo/two\n/dirthree\n/dirthree/three\nNote that every time you update the tar file you are invalidating the Docker build cache for that step. If you are dealing with a lot of files you might want to be smart about when you do the tar -c. You could also use tar -u if you can deal with files not being automatically deleted from the tarball.\n[ -f dirs.tar ] && tar -uf dirs.tar something || tar -cf dirs.tar something",
                "upvotes": 21,
                "answered_by": "Matt",
                "answered_at": "2016-06-09 02:57"
            }
        ]
    },
    {
        "title": "Activate python virtualenv in Dockerfile",
        "url": "https://stackoverflow.com/questions/48561981/activate-python-virtualenv-in-dockerfile",
        "votes": "132",
        "views": "191k",
        "author": "igsm",
        "issued_at": "2018-02-01 11:46",
        "tags": [
            "python",
            "docker",
            "dockerfile",
            "virtualenv",
            "python-venv"
        ],
        "answers": [
            {
                "answer": "You don't need to use virtualenv inside a Docker Container.\nvirtualenv is used for dependency isolation. You want to prevent any dependencies or packages installed from leaking between applications. Docker achieves the same thing, it isolates your dependencies within your container and prevent leaks between containers and between applications.\nTherefore, there is no point in using virtualenv inside a Docker Container unless you are running multiple apps in the same container, if that's the case I'd say that you're doing something wrong and the solution would be to architect your app in a better way and split them up in multiple containers.\nEDIT 2022: Given this answer get a lot of views, I thought it might make sense to add that now 4 years later, I realized that there actually is valid usages of virtual environments in Docker images, especially when doing multi staged builds:\nFROM python:3.9-slim as compiler\nENV PYTHONUNBUFFERED 1\n\nWORKDIR /app/\n\nRUN python -m venv /opt/venv\n# Enable venv\nENV PATH=\"/opt/venv/bin:$PATH\"\n\nCOPY ./requirements.txt /app/requirements.txt\nRUN pip install -Ur requirements.txt\n\nFROM python:3.9-slim as runner\nWORKDIR /app/\nCOPY --from=compiler /opt/venv /opt/venv\n\n# Enable venv\nENV PATH=\"/opt/venv/bin:$PATH\"\nCOPY . /app/\nCMD [\"python\", \"app.py\", ]\nIn the Dockerfile example above, we are creating a virtualenv at /opt/venv and activating it using an ENV statement, we then install all dependencies into this /opt/venv and can simply copy this folder into our runner stage of our build. This can help with minimizing docker image size.",
                "upvotes": 148,
                "answered_by": "Marcus Lind",
                "answered_at": "2018-02-01 12:29"
            },
            {
                "answer": "There are perfectly valid reasons for using a virtualenv within a container.\nYou don't necessarily need to activate the virtualenv to install software or use it. Try invoking the executables directly from the virtualenv's bin directory instead:\nFROM python:2.7\n\nRUN virtualenv /ve\nRUN /ve/bin/pip install somepackage\n\nCMD [\"/ve/bin/python\", \"yourcode.py\"]\nYou may also just set the PATH environment variable so that all further Python commands will use the binaries within the virtualenv as described in https://pythonspeed.com/articles/activate-virtualenv-dockerfile/\nFROM python:2.7\n\nRUN virtualenv /ve\nENV PATH=\"/ve/bin:$PATH\"\nRUN pip install somepackage\n\nCMD [\"python\", \"yourcode.py\"]",
                "upvotes": 98,
                "answered_by": "Ellis Percival",
                "answered_at": "2018-08-19 16:13"
            },
            {
                "answer": "Although I agree with Marcus that this is not the way of doing with Docker, you can do what you want.\nUsing the RUN command of Docker directly will not give you the answer as it will not execute your instructions from within the virtual environment. Instead squeeze the instructions executed in a single line using /bin/bash. The following Dockerfile worked for me:\nFROM python:2.7\n\nRUN virtualenv virtual\nRUN /bin/bash -c \"source /virtual/bin/activate && pip install pyserial && deactivate\"\n...\nThis should install the pyserial module only on the virtual environment.",
                "upvotes": 18,
                "answered_by": "pinty",
                "answered_at": "2018-02-01 14:34"
            }
        ]
    },
    {
        "title": "\"The headers or library files could not be found for jpeg\" installing Pillow on Alpine Linux",
        "url": "https://stackoverflow.com/questions/44043906/the-headers-or-library-files-could-not-be-found-for-jpeg-installing-pillow-on",
        "votes": "131",
        "views": "137k",
        "author": "Kurt Peek",
        "issued_at": "2017-05-18 09:31",
        "tags": [
            "python",
            "docker",
            "dockerfile",
            "python-imaging-library",
            "alpine-linux"
        ],
        "answers": [
            {
                "answer": "For debian\nsudo apt install libjpeg-dev zlib1g-dev\npip install Pillow",
                "upvotes": 239,
                "answered_by": "vamsi",
                "answered_at": "2018-04-10 10:45"
            },
            {
                "answer": "For macOS:\nbrew install libtiff libjpeg webp little-cms2\nReason: https://pillow.readthedocs.io/en/latest/installation/building-from-source.html",
                "upvotes": 42,
                "answered_by": "tanpengsccd",
                "answered_at": "2021-01-14 15:58"
            }
        ]
    },
    {
        "title": "Dockerfile: Setting multiple environment variables in single line",
        "url": "https://stackoverflow.com/questions/45529121/dockerfile-setting-multiple-environment-variables-in-single-line",
        "votes": "131",
        "views": "115k",
        "author": "BrianWilson",
        "issued_at": "2017-08-06 06:10",
        "tags": [
            "docker",
            "dockerfile"
        ],
        "answers": [
            {
                "answer": "There are two formats for specifying environments. If you need single variable then you below format\nENV X Y\nThis will assign X as Y\nENV X Y Z\nThis will assign X as Y Z\nIf you need to assign multiple environment variables then you use the other format\nENV X=Y Z=A\nThis will assign X as Y and Z as A. So your Dockerfile should be\nFROM alpine:3.6\nENV RUBY_MAJOR=2.4 \\\n    RUBY_VERSION=2.4.1 \\\n    RUBY_DOWNLOAD_SHA256=4fc8a9992de3e90191de369270ea4b6c1b171b7941743614cc50822ddc1fe654 \\\n    RUBYGEMS_VERSION=2.6.12 \\\n    BUNDLER_VERSION=1.15.3\n\nRUN env",
                "upvotes": 231,
                "answered_by": "Tarun Lalwani",
                "answered_at": "2017-08-06 06:36"
            },
            {
                "answer": "You do not need to worry about many ENV commands each creating a new intermediate layer for your final image created by your Dockerfile.\nfrom Best practices for writing Dockerfiles\nMinimize the number of layers\nPrior to Docker 17.05, and even more, prior to Docker 1.10, it was important to minimize the number of layers in your image. The following improvements have mitigated this need:\nIn Docker 1.10 and higher, only RUN, COPY, and ADD instructions create layers. Other instructions create temporary intermediate images, and no longer directly increase the size of the build.\nDocker 17.05 and higher add support for multi-stage builds, which allow you to copy only the artifacts you need into the final image. This allows you to include tools and debug information in your intermediate build stages without increasing the size of the final image.",
                "upvotes": 39,
                "answered_by": "Mike Lippert",
                "answered_at": "2018-03-27 14:51"
            }
        ]
    },
    {
        "title": "Docker filling up storage on macOS",
        "url": "https://stackoverflow.com/questions/39878939/docker-filling-up-storage-on-macos",
        "votes": "125",
        "views": "99k",
        "author": "Franco Rabaglia",
        "issued_at": "2016-10-05 16:08",
        "tags": [
            "macos",
            "docker",
            "containers",
            "dockerfile",
            "diskspace"
        ],
        "answers": [
            {
                "answer": "WARNING:\nBy default, volumes are not removed to prevent important data from being deleted if there is currently no container using the volume. Use the --volumes flag when running the command to prune volumes as well:\nDocker now has a single command to do that:\ndocker system prune -a --volumes\nSee the Docker system prune docs",
                "upvotes": 176,
                "answered_by": "zhongjiajie",
                "answered_at": "2019-03-26 02:19"
            },
            {
                "answer": "There are three areas of Docker storage that can mount up, because Docker is cautious - it doesn't automatically remove any of them: exited containers, unused container volumes, unused image layers. In a dev environment with lots of building and running, that can be a lot of disk space.\nThese three commands clear down anything not being used:\ndocker rm $(docker ps -f status=exited -aq) - remove stopped containers\ndocker rmi $(docker images -f \"dangling=true\" -q) - remove image layers that are not used in any images\ndocker volume rm $(docker volume ls -qf dangling=true) - remove volumes that are not used by any containers.\nThese are safe to run, they won't delete image layers that are referenced by images, or data volumes that are used by containers. You can alias them, and/or put them in a CRON job to regularly clean up the local disk.",
                "upvotes": 49,
                "answered_by": "Elton Stoneman",
                "answered_at": "2016-10-06 07:28"
            },
            {
                "answer": "Why does the file keep growing?\nIf Docker is used regularly, the size of the Docker.raw (or Docker.qcow2) can keep growing, even when files are deleted.\nTo demonstrate the effect, first check the current size of the file on the host:\n$ cd ~/Library/Containers/com.docker.docker/Data/com.docker.driver.amd64-linux/\n$ ls -s Docker.raw\n9964528 Docker.raw\nNote the use of -s which displays the number of filesystem blocks actually used by the file. The number of blocks used is not necessarily the same as the file \u201csize\u201d, as the file can be sparse.\nNext start a container in a separate terminal and create a 1GiB file in it:\n$ docker run -it alpine sh\n# and then inside the container:\n/ # dd if=/dev/zero of=1GiB bs=1048576 count=1024\n1024+0 records in\n1024+0 records out\n/ # sync\nBack on the host check the file size again:\n$ ls -s Docker.raw \n12061704 Docker.raw\nNote the increase in size from 9964528 to 12061704, where the increase of 2097176 512-byte sectors is approximately 1GiB, as expected. If you switch back to the alpine container terminal and delete the file:\n/ # rm -f 1GiB\n/ # sync\nthen check the file on the host:\n$ ls -s Docker.raw \n12059672 Docker.raw\nThe file has not got any smaller! Whatever has happened to the file inside the VM, the host doesn\u2019t seem to know about it.\nNext if you re-create the \u201csame\u201d 1GiB file in the container again and then check the size again you will see:\n$ ls -s Docker.raw \n14109456 Docker.raw\nIt\u2019s got even bigger! It seems that if you create and destroy files in a loop, the size of the Docker.raw (or Docker.qcow2) will increase up to the upper limit (currently set to 64 GiB), even if the filesystem inside the VM is relatively empty.\nThe explanation for this odd behaviour lies with how filesystems typically manage blocks. When a file is to be created or extended, the filesystem will find a free block and add it to the file. When a file is removed, the blocks become \u201cfree\u201d from the filesystem\u2019s point of view, but no-one tells the disk device. Making matters worse, the newly-freed blocks might not be re-used straight away \u2013 it\u2019s completely up to the filesystem\u2019s block allocation algorithm. For example, the algorithm might be designed to favour allocating blocks contiguously for a file: recently-freed blocks are unlikely to be in the ideal place for the file being extended.\nSince the block allocator in practice tends to favour unused blocks, the result is that the Docker.raw (or Docker.qcow2) will constantly accumulate new blocks, many of which contain stale data. The file on the host gets larger and larger, even though the filesystem inside the VM still reports plenty of free space.\nTRIM\nA TRIM command (or a DISCARD or UNMAP) allows a filesystem to signal to a disk that a range of sectors contain stale data and they can be forgotten. This allows:\nan SSD drive to erase and reuse the space, rather than spend time shuffling it around; and\nDocker for Mac to deallocate the blocks in the host filesystem, shrinking the file.\nSo how do we make this work?\nAutomatic TRIM in Docker for Mac\nIn Docker for Mac 17.11 there is a containerd \u201ctask\u201d called trim-after-delete listening for Docker image deletion events. It can be seen via the ctr command:\n$ docker run --rm -it --privileged --pid=host walkerlee/nsenter -t 1 -m -u -i -n ctr t ls\nTASK                    PID     STATUS    \nvsudd                   1741    RUNNING\nacpid                   871     RUNNING\ndiagnose                913     RUNNING\ndocker-ce               958     RUNNING\nhost-timesync-daemon    1046    RUNNING\nntpd                    1109    RUNNING\ntrim-after-delete       1339    RUNNING\nvpnkit-forwarder        1550    RUNNING\nWhen an image deletion event is received, the process waits for a few seconds (in case other images are being deleted, for example as part of a docker system prune ) and then runs fstrim on the filesystem.\nReturning to the example in the previous section, if you delete the 1 GiB file inside the alpine container\n/ # rm -f 1GiB\nthen run fstrim manually from a terminal in the host:\n$ docker run --rm -it --privileged --pid=host walkerlee/nsenter -t 1 -m -u -i -n fstrim /var/lib/docker\nthen check the file size:\n$ ls -s Docker.raw \n9965016 Docker.raw\nThe file is back to (approximately) it\u2019s original size \u2013 the space has finally been freed!\nHopefully this blog will be helpful, also checkout the following macos docker utility scripts for this problem:\nhttps://github.com/wanliqun/macos_docker_toolkit",
                "upvotes": 33,
                "answered_by": "Liqun",
                "answered_at": "2020-02-14 08:25"
            }
        ]
    },
    {
        "title": "Can't create a docker image for COPY failed: stat /var/lib/docker/tmp/docker-builder error",
        "url": "https://stackoverflow.com/questions/46907584/cant-create-a-docker-image-for-copy-failed-stat-var-lib-docker-tmp-docker-bui",
        "votes": "125",
        "views": "204k",
        "author": "EdoBen",
        "issued_at": "2017-10-24 10:01",
        "tags": [
            "python",
            "docker",
            "dockerfile",
            "docker-image"
        ],
        "answers": [
            {
                "answer": "Check if there's a .dockerignore file, if so, add:\n!mydir/test.json\n!mydir/test.py",
                "upvotes": 55,
                "answered_by": "Brahimi Boubakeur",
                "answered_at": "2019-02-11 10:27"
            }
        ]
    },
    {
        "title": "apt-get update' returned a non-zero code: 100",
        "url": "https://stackoverflow.com/questions/38002543/apt-get-update-returned-a-non-zero-code-100",
        "votes": "124",
        "views": "307k",
        "author": "rp346",
        "issued_at": "2016-06-23 21:55",
        "tags": [
            "ubuntu",
            "docker",
            "dockerfile"
        ],
        "answers": [
            {
                "answer": "Because you have an https sources. Install apt-transport-https before executing update.\nFROM ubuntu:14.04.4\nRUN apt-get update && apt-get install -y apt-transport-https\nRUN echo 'deb http://private-repo-1.hortonworks.com/HDP/ubuntu14/2.x/updates/2.4.2.0 HDP main' >> /etc/apt/sources.list.d/HDP.list\nRUN echo 'deb http://private-repo-1.hortonworks.com/HDP-UTILS-1.1.0.20/repos/ubuntu14 HDP-UTILS main'  >> /etc/apt/sources.list.d/HDP.list\nRUN echo 'deb [arch=amd64] https://apt-mo.trafficmanager.net/repos/azurecore/ trusty main' >> /etc/apt/sources.list.d/azure-public-trusty.list\n\n....\nRest of your Dockerfile.",
                "upvotes": 116,
                "answered_by": "techtabu",
                "answered_at": "2016-06-24 00:53"
            }
        ]
    },
    {
        "title": "Editing Files from dockerfile",
        "url": "https://stackoverflow.com/questions/27713362/editing-files-from-dockerfile",
        "votes": "123",
        "views": "168k",
        "author": "Myles McDonnell",
        "issued_at": "2014-12-30 22:14",
        "tags": [
            "docker",
            "dockerfile"
        ],
        "answers": [
            {
                "answer": "I would use the following approach in the Dockerfile\nRUN   echo \"Some line to add to a file\" >> /etc/sysctl.conf\nThat should do the trick. If you wish to replace some characters or similar you can work this out with sed by using e.g. the following:\nRUN   sed -i \"s|some-original-string|the-new-string |g\" /etc/sysctl.conf\nHowever, if your problem lies in simply getting the settings to \"bite\" this question might be of help.",
                "upvotes": 195,
                "answered_by": "wassgren",
                "answered_at": "2014-12-30 23:27"
            },
            {
                "answer": "sed work pretty well to replace stuff, if you need to append, you can user double redirect\nsed -i 's/origin text/new text/g' /etc/sysctl.conf\nbash -c 'echo hello world' >> /etc/sysctl.conf\n-i is a non-standard option of GNU sed for inline editing (alleviating the need for dealing with temporary files).\nThe s is the substitute command of sed for find and replace\nThe g means global replace i.e. find all occurrences of origin text and replace with new text using sed",
                "upvotes": 19,
                "answered_by": "creack",
                "answered_at": "2014-12-30 22:46"
            },
            {
                "answer": "To complement the answers that already explain how you can append a single line, you can append multiple lines to a file like this:\nRUN /bin/echo -e '\\\nHello World!\\n\\\nThis is an example on how to\\n\\\nappend multiple lines to a file from within a Dockerfile.'\\\n>> /etc/sysctl.conf\nUse double quotes if you want to expand variables.",
                "upvotes": 5,
                "answered_by": "stackprotector",
                "answered_at": "2023-03-14 16:05"
            }
        ]
    },
    {
        "title": "How to pass arguments within docker-compose?",
        "url": "https://stackoverflow.com/questions/34322631/how-to-pass-arguments-within-docker-compose",
        "votes": "122",
        "views": "214k",
        "author": "meallhour",
        "issued_at": "2015-12-16 21:41",
        "tags": [
            "docker",
            "dockerfile",
            "docker-compose"
        ],
        "answers": [
            {
                "answer": "Now docker-compose supports variable substitution.\nCompose uses the variable values from the shell environment in which docker-compose is run. For example, suppose the shell contains POSTGRES_VERSION=9.3 and you supply this configuration in your docker-compose.yml file:\ndb:\n  image: \"postgres:${POSTGRES_VERSION}\"\nWhen you run docker-compose up with this configuration, Compose looks for the POSTGRES_VERSION environment variable in the shell and substitutes its value in. For this example, Compose resolves the image to postgres:9.3 before running the configuration.",
                "upvotes": 103,
                "answered_by": "Hemerson Varela",
                "answered_at": "2016-01-29 22:46"
            },
            {
                "answer": "This feature was added in Compose file format 1.6.\nReference: https://docs.docker.com/compose/compose-file/#args\nservices:\n  web:\n    build:\n      context: .\n      args:\n        FOO: foo",
                "upvotes": 67,
                "answered_by": "dnephin",
                "answered_at": "2015-12-16 22:58"
            }
        ]
    },
    {
        "title": "MYSQL_ROOT_PASSWORD is set but getting \"Access denied for user 'root'@'localhost' (using password: YES)\" in docker container",
        "url": "https://stackoverflow.com/questions/59838692/mysql-root-password-is-set-but-getting-access-denied-for-user-rootlocalhost",
        "votes": "122",
        "views": "201k",
        "author": "Mukit09",
        "issued_at": "2020-01-21 10:07",
        "tags": [
            "mysql",
            "docker",
            "docker-compose",
            "dockerfile"
        ],
        "answers": [
            {
                "answer": "The below description is specifically for MySQL but many other official db docker images (postgres, mongodb....) work a similar way. Hence the symptom (i.e. access denied with configured credentials) and workaround (i.e. delete the data volume to start initialization from scratch) are the same.\nTaking for granted you have shown your entire start log, it appears you started your mysql container against a pre-existing db_data volume already containing a mysql database filesystem.\nIn this case, absolutely nothing will be initialized on container start and environment variables are useless. Quoting the official image documentation in the \"Environment Variables\" section:\nDo note that none of the variables below will have any effect if you start the container with a data directory that already contains a database: any pre-existing database will always be left untouched on container startup.\nIf you want your instance to be initialized, you have to start from scratch. It is quite easy to do with docker compose when using a named volume like in your case. Warning: this will permanently delete the contents in your db_data volume, wiping out any previous database you had there. Create a backup first if you need to keep the contents.\ndocker-compose down -v\ndocker-compose up -d\nIf you ever convert to a bind mount, you will have to delete all it's content yourself (i.e. rm -rf /path/to/bind/mount/*)",
                "upvotes": 238,
                "answered_by": "Zeitounator",
                "answered_at": "2020-01-21 10:34"
            },
            {
                "answer": "I've tested with all of the possible solutions posted in this thread. However, after try and error, I identified for any reason complex passwords were not recognized.\nI've changed\n      mariadb:\n        container_name: dev_db\n        image: mariadb:10.5\n        restart: always\n        environment:\n          MARIADB_ROOT_PASSWORD: a8Gh@c8wi#gL^\n          MARIADB_DATABASE: wp_my_database\n          MARIADB_USER: wp\n          MARIADB_PASSWORD: a8Gh@c8wi#gL^\nby\n      mariadb:\n        container_name: dev_db\n        image: mariadb:10.5\n        restart: always\n        environment:\n          MARIADB_ROOT_PASSWORD: qwerty\n          MARIADB_DATABASE: wp_my_database\n          MARIADB_USER: wp\n          MARIADB_PASSWORD: qwerty\ndocker compose version: '3.9'.\nservices:\nnginx:1.20-alpine\nphp:7.2.34-fpm-alpine\nmariadb:mariadb:10.5\nphpmyadmin: phpmyadmin/phpmyadmin:latest.",
                "upvotes": 17,
                "answered_by": "user3931921",
                "answered_at": "2022-04-11 21:47"
            },
            {
                "answer": "Changing host_port from 3306 to 3906 worked for me.\nExample of conf with mysql and healthcheck:\nmysql:\n    image: 'mysql/mysql-server:8.0'\n    ports:\n      - '3906:3306'\n    environment:\n      MYSQL_ROOT_PASSWORD: '${DB_PASSWORD}'\n      MYSQL_ROOT_HOST: \"%\"\n      MYSQL_DATABASE: '${DB_DATABASE}'\n      MYSQL_USER: '${DB_USERNAME}'\n      MYSQL_PASSWORD: '${DB_PASSWORD}'\n      MYSQL_ALLOW_EMPTY_PASSWORD: 1\n    volumes:\n      - db:/var/lib/mysql\n    networks:\n      - my_network\n    healthcheck:\n      test: [ \"CMD\", \"mysqladmin\", \"ping\", \"-p${DB_PASSWORD}\" ]\n      retries: 3\n      timeout: 5s",
                "upvotes": 1,
                "answered_by": "illummina",
                "answered_at": "2023-06-06 14:32"
            }
        ]
    },
    {
        "title": "Why the \"none\" image appears in Docker and how can we avoid it",
        "url": "https://stackoverflow.com/questions/53221412/why-the-none-image-appears-in-docker-and-how-can-we-avoid-it",
        "votes": "122",
        "views": "69k",
        "author": "Chen Hanhan",
        "issued_at": "2018-11-09 07:26",
        "tags": [
            "docker",
            "docker-compose",
            "dockerfile"
        ],
        "answers": [
            {
                "answer": "Below are some parts from What are Docker <none>:<none> images?\nThe Good <none>:<none>\nThese are intermediate images and can be seen using docker images -a. They don't result into a disk space problem but it is definitely a screen \"real estate\" problem. Since all these <none>:<none> images can be quite confusing as what they signify.\nThe Bad <none>:<none>\nThese images are the dangling ones, which can cause disk space problems. These <none>:<none> images are being listed as part of docker images and need to be pruned.\n(a dangling file system layer in Docker is something that is unused and is not being referenced by any images. Hence we need a mechanism for Docker to clear these dangling images)\nSo,\nif your case has to do with dangling images, it's ok to remove them with:\n docker rmi $(docker images -f \"dangling=true\" -q)\nThere is also the option of docker image prune but the client and daemon API must both be at least v1.25 to use this command.\nif your case has to do with intermediate images, it's ok to keep them, other images are pointing references to them.\nRelated documentation:\ndocker rmi\ndocker image rm\ndocker image prune",
                "upvotes": 144,
                "answered_by": "tgogos",
                "answered_at": "2018-11-09 10:43"
            },
            {
                "answer": "In my experience most of the <none> images are held by temporary containers. Due to Docker architecture those containers are preserved even after they stop. You can verify how many stopped containers you have using\ndocker ps -a\nSo to remove the <none> images you first need to remove the unneeded containers:\ndocker container prune\ndocker image prune\nThe above two commands can be abbreviated to\ndocker system prune",
                "upvotes": 55,
                "answered_by": "SnakE",
                "answered_at": "2019-06-11 06:14"
            }
        ]
    },
    {
        "title": "Why won't my docker-entrypoint.sh execute?",
        "url": "https://stackoverflow.com/questions/38905135/why-wont-my-docker-entrypoint-sh-execute",
        "votes": "115",
        "views": "277k",
        "author": "Amin Shah Gilani",
        "issued_at": "2016-08-11 20:02",
        "tags": [
            "docker",
            "dockerfile"
        ],
        "answers": [
            {
                "answer": "I was tearing my hair out with an issue very similar to this. In my case /bin/bash DID exist. But actually the problem was Windows line endings.\nIn my case the git repository had an entry point script with Unix line endings (\\n). But when the repository was checked out on a windows machine, git decided to try and be clever and replace the line endings in the files with windows line endings (\\r\\n).\nThis meant that the shebang didn't work because instead of looking for /bin/bash, it was looking for /bin/bash\\r.\nThe solution for me was to disable git's automatic conversion:\ngit config --global core.autocrlf input\nReset the repo using this (don't forget to save your changes):\ngit rm --cached -r .\ngit reset --hard\nAnd then rebuild.\nSome more helpful info here: How to change line-ending settings and here http://willi.am/blog/2016/08/11/docker-for-windows-dealing-with-windows-line-endings/\nFor repo owners and contributors\nIf you own a repo or contribute to it, set mandatory LF line endings for .sh files right in the repo by adding the .gitattributes file with the following line:\n*.sh text eol=lf",
                "upvotes": 220,
                "answered_by": "Daniel Howard",
                "answered_at": "2016-11-10 21:12"
            }
        ]
    },
    {
        "title": "How can I see Dockerfile for each docker image?",
        "url": "https://stackoverflow.com/questions/43309737/how-can-i-see-dockerfile-for-each-docker-image",
        "votes": "111",
        "views": "134k",
        "author": "Lone Learner",
        "issued_at": "2017-04-09 17:29",
        "tags": [
            "docker",
            "dockerfile",
            "docker-image"
        ],
        "answers": [
            {
                "answer": "As far as I know, no, you can't. Because a Dockerfile is used for building the image, it is not packed with the image itself. That means you should reverse engineer it. You can use docker inspect on an image or container, thus getting some insight and a feel of how it is configured. The layers an image are also visible, since you pull them when you pull a specific image, so that is also no secret.\nHowever, you can usually see the Dockerfile in the repository of the image itself on Dockerhub. I can't say most repositories have Dockerfiles attached, but the most of the repositories I seen do have it.\nDifferent repository maintainers may opt for different ways to document the Dockerfiles. You can see the Dockerfile tab on the repository page if automatic builds are set up. But when multiple parallel versions are available (like for Ubuntu), maintainers usually opt to put links the Dockerfiles for different versions in the description. If you take a look here: https://hub.docker.com/_/ubuntu/, under the \"Supported tags\" (again, for Ubuntu), you can see there are links to multiple Dockerfiles, for each respective Ubuntu version.",
                "upvotes": 82,
                "answered_by": "Aleksandar Stojadinovic",
                "answered_at": "2017-04-09 18:57"
            },
            {
                "answer": "You can also regenerate the dockerfile from an image or use the docker history <image name> command to see what is inside. check this: Link to answer\nTL;DR So if you have a docker image that was built by a dockerfile, you can recover this information (All except from the original FROM command, which is important, I\u2019ll grant that. But you can often guess it, especially by entering the container and asking \u201cWhat os are you?\u201d). However, the maker of the image could have manual steps that you\u2019d never know about anyways, plus they COULD just export an image, and re-import it and there would be no intermediate images at that point.",
                "upvotes": 27,
                "answered_by": "themozel",
                "answered_at": "2020-08-24 09:38"
            }
        ]
    },
    {
        "title": "How to configure different dockerfile for development and production",
        "url": "https://stackoverflow.com/questions/46440909/how-to-configure-different-dockerfile-for-development-and-production",
        "votes": "106",
        "views": "101k",
        "author": "Dev Khadka",
        "issued_at": "2017-09-27 06:30",
        "tags": [
            "docker",
            "dockerfile"
        ],
        "answers": [
            {
                "answer": "As a best practice you should try to aim to use one Dockerfile to avoid unexpected errors between different environments. However, you may have a use case where you cannot do that.\nThe Dockerfile syntax is not rich enough to support such a scenario, however you can use shell scripts to achieve that.\nCreate a shell script, called install.sh that does something like:\nif [ ${ENV} = \"DEV\" ]; then \n    composer install\nelse\n    npm install\nfi\nIn your Dockerfile add this script and then execute it when building\n...\nCOPY install.sh install.sh\nRUN chmod u+x install.sh && ./install.sh\n...\nWhen building pass a build arg to specify the environment, example:\ndocker build --build-arg \"ENV=PROD\" ...",
                "upvotes": 99,
                "answered_by": "yamenk",
                "answered_at": "2017-09-27 10:15"
            },
            {
                "answer": "UPDATE (2020): Since this was written 3 years ago, many things have changed (including my opinion about this topic). My suggested way of doing this, is using one dockerfile and using scripts. Please see @yamenk's answer.\nORIGINAL:\nYou can use two different Dockerfiles.\n# ./Dockerfile (non production)\nFROM foo/bar\nMAINTAINER ...\n\n# ....\nAnd a second one:\n# ./Dockerfile.production\nFROM foo/bar\nMAINTAINER ...\n\nRUN composer install\nWhile calling the build command, you can tell which file it should use:\n$> docker build -t mytag .\n$> docker build -t mytag-production -f Dockerfile.production .",
                "upvotes": 76,
                "answered_by": "Michael Hirschler",
                "answered_at": "2017-09-27 06:42"
            },
            {
                "answer": "I have tried several approaches to this, including using docker-compose, a multi-stage build, passing an argument through a file and the approaches used in other answers. My company needed a good way to do this and after trying these, here is my opinion.\nThe best method is to pass the arg through the cmd. You can pass it through vscode while right clicking and choosing build image Image of visual studio code while clicking image build using this code:\nARG BuildMode\nRUN echo $BuildMode\nRUN if [ \"$BuildMode\" = \"debug\" ] ; then apt-get update \\\n    && apt-get install -y --no-install-recommends \\\n       unzip \\\n    && rm -rf /var/lib/apt/lists/* \\\n    && curl -sSL https://aka.ms/getvsdbgsh | bash /dev/stdin -v latest -l /vsdbg ; fi\nand in the build section of dockerfile:\nARG BuildMode\nENV Environment=${BuildMode:-debug}\nRUN dotnet build \"debugging.csproj\" -c $Environment -o /app\n\nFROM build AS publish\nRUN dotnet publish \"debugging.csproj\" -c $Environment -o /app",
                "upvotes": 8,
                "answered_by": "jorence jovin",
                "answered_at": "2019-07-19 05:53"
            }
        ]
    },
    {
        "title": "How to pass ARG value to ENTRYPOINT?",
        "url": "https://stackoverflow.com/questions/34324277/how-to-pass-arg-value-to-entrypoint",
        "votes": "106",
        "views": "71k",
        "author": "meallhour",
        "issued_at": "2015-12-16 23:43",
        "tags": [
            "docker",
            "dockerfile",
            "docker-compose",
            "docker-registry"
        ],
        "answers": [
            {
                "answer": "Like Blake Mitchell sais, you cannot use ARG in ENTRYPOINT. However you can use your ARG as a value for ENV, that way you can use it with ENTRYPOINT:\nDockerfile\nARG my_arg\nENV my_env_var=$my_arg\n\nENTRYPOINT echo $my_env_var\nand run:\ndocker build --build-arg \"my_arg=foo\" ...",
                "upvotes": 122,
                "answered_by": "Rotareti",
                "answered_at": "2017-12-04 23:44"
            },
            {
                "answer": "I'm struggling with this for a day, thanks for @Rotareti for mentioning. It needs to be in ENV before it can be used to the ENTRYPOINT.\nENV variable=$from_ARG_variable\nENTRYPOINT exec your_exec_sh_file $variable\nHope this helps.",
                "upvotes": 11,
                "answered_by": "Quer",
                "answered_at": "2020-02-16 02:23"
            }
        ]
    },
    {
        "title": "docker run pass arguments to entrypoint",
        "url": "https://stackoverflow.com/questions/53543881/docker-run-pass-arguments-to-entrypoint",
        "votes": "105",
        "views": "175k",
        "author": "nad87563",
        "issued_at": "2018-11-29 16:49",
        "tags": [
            "docker",
            "dockerfile"
        ],
        "answers": [
            {
                "answer": "Use ENTRYPOINT in its exec form\nENTRYPOINT [\"java\", \"-jar\", \"/dir/test-1.0.1.jar\"]\nthen when you run docker run -it testjava $value, $value will be \"appended\" after your entrypoint, just like java -jar /dir/test-1.0.1.jar $value",
                "upvotes": 134,
                "answered_by": "Siyu",
                "answered_at": "2018-11-29 17:00"
            },
            {
                "answer": "You should unleash the power of combination of ENTRYPOINT and CMD.\nPut the beginning part of your command line, which is not expected to change, into ENTRYPOINT and the tail, which should be configurable, into CMD. Then you can simple append necessary arguments to your docker run command. Like this:\nDockerfile\nFROM openjdk\nADD . /dir\nWORKDIR /dir\nCOPY ./test-1.0.1.jar /dir/test-1.0.1.jar\nENTRYPOINT [\"java\", \"-jar\"]\nCMD [\"/dir/test-1.0.1.jar\"]\nSh\n# this will run default jar - /dir/test-1.0.1.jar\ndocker run testjava\n\n# this will run overriden jar\ndocker run testjava /dir/blahblah.jar\nThis article gives a good explanation.",
                "upvotes": 77,
                "answered_by": "grapes",
                "answered_at": "2018-11-29 16:55"
            }
        ]
    },
    {
        "title": "Docker COPY files using glob pattern?",
        "url": "https://stackoverflow.com/questions/49939960/docker-copy-files-using-glob-pattern",
        "votes": "105",
        "views": "138k",
        "author": "Fez Vrasta",
        "issued_at": "2018-04-20 10:49",
        "tags": [
            "docker",
            "dockerfile",
            "yarnpkg"
        ],
        "answers": [
            {
                "answer": "There is a solution based on multistage-build feature:\nFROM node:12.18.2-alpine3.11\n\nWORKDIR /app\nCOPY [\"package.json\", \"yarn.lock\", \"./\"]\n# Step 2: Copy whole app\nCOPY packages packages\n\n# Step 3: Find and remove non-package.json files\nRUN find packages \\! -name \"package.json\" -mindepth 2 -maxdepth 2 -print | xargs rm -rf\n\n# Step 4: Define second build stage\nFROM node:12.18.2-alpine3.11\n\nWORKDIR /app\n# Step 5: Copy files from the first build stage.\nCOPY --from=0 /app .\n\nRUN yarn install --frozen-lockfile\n\nCOPY . .\n\n# To restore workspaces symlinks\nRUN yarn install --frozen-lockfile\n\nCMD yarn start\nOn Step 5 the layer cache will be reused even if any file in packages directory has changed.",
                "upvotes": 60,
                "answered_by": "mbelsky",
                "answered_at": "2020-07-28 20:43"
            },
            {
                "answer": "As mentioned in the official Dockerfile reference for COPY <src> <dest>\nThe COPY instruction copies new files or directories from <src> and adds them to the filesystem of the container at the path <dest>.\nFor your case\nEach may contain wildcards and matching will be done using Go\u2019s filepath.Match rules.\nThese are the rules. They contain this:\n'*' matches any sequence of non-Separator characters\nSo try to use * instead of ** in your pattern.",
                "upvotes": 31,
                "answered_by": "v.karbovnichy",
                "answered_at": "2018-04-20 10:59"
            },
            {
                "answer": "Using Docker's new BuildKit executor it has become possible to use a bind mount into the Docker context, from which you can then copy any files as needed.\nFor example, the following snippet copies all package.json files from the Docker context into the image's /app/ directory (the workdir in the below example)\nUnfortunately, changing any file in the mount still results in a layer cache miss. This can be worked around using the multi-stage approach as presented by @mbelsky, but this time the explicit deletion is no longer needed.\n# syntax = docker/dockerfile:1.2\nFROM ... AS packages\n\nWORKDIR /app/\nRUN --mount=type=bind,target=/docker-context \\\n    cd /docker-context/; \\\n    find . -name \"package.json\" -mindepth 0 -maxdepth 4 -exec cp --parents \"{}\" /app/ \\;\n\nFROM ...\n\nWORKDIR /app/\nCOPY --from=packages /app/ .\nThe mindepth/maxdepth arguments are specified to reduce the number of directories to search, this can be adjusted/removed as desirable for your use-case.\nIt may be necessary to enable the BuildKit executor using environment variable DOCKER_BUILDKIT=1, as the traditional executor silently ignores the bind mounts.\nMore information about BuildKit and bind bounds can be found here.",
                "upvotes": 22,
                "answered_by": "Joost",
                "answered_at": "2021-02-10 13:28"
            },
            {
                "answer": "If you can't technically enumerate all the subdirectories at stake in the Dockerfile (namely, writing COPY packages/one/package.json packages/one/ for each one), but want to copy all the files in two steps and take advantage of Docker's caching feature, you can try the following workaround:\nDevise a wrapper script (say, in bash) that copies the required package.json files to a separate directory (say, .deps/) built with a similar hierarchy, then call docker build \u2026\nAdapt the Dockerfile to copy (and rename) the separate directory beforehand, and then call yarn install --pure-lockfile\u2026\nAll things put together, this could lead to the following files:\n./build.bash:\n#!/bin/bash\n\ntag=\"image-name:latest\"\n\nrm -f -r .deps  # optional, to be sure that there is\n# no extraneous \"package.json\" from a previous build\n\nfind . -type d \\( -path \\*/.deps \\) -prune -o \\\n  -type f \\( -name \"package.json\" \\) \\\n  -exec bash -c 'dest=\".deps/$1\" && \\\n    mkdir -p -- \"$(dirname \"$dest\")\" && \\\n    cp -av -- \"$1\" \"$dest\"' bash '{}' \\;\n# instead of mkdir + cp, you may also want to use\n# rsync if it is available in your environment...\n\nsudo docker build -t \"$tag\" .\nand\n./Dockerfile:\nFROM \u2026\n\nWORKDIR /usr/src/app\n\n# COPY package.json .  # subsumed by the following command\nCOPY .deps .\n# and not \"COPY .deps .deps\", to avoid doing an extra \"mv\"\nCOPY yarn.lock .\nRUN yarn install --pure-lockfile\n\nCOPY . .\n# Notice that \"COPY . .\" will also copy the \".deps\" folder; this is\n# maybe a minor issue, but it could be avoided by passing more explicit\n# paths than just \".\" (or by adapting the Dockerfile and the script and\n# putting them in the parent folder of the Yarn application itself...)",
                "upvotes": 19,
                "answered_by": "ErikMD",
                "answered_at": "2018-04-24 20:14"
            }
        ]
    },
    {
        "title": "standard_init_linux.go:211: exec user process caused \"exec format error\"",
        "url": "https://stackoverflow.com/questions/58298774/standard-init-linux-go211-exec-user-process-caused-exec-format-error",
        "votes": "103",
        "views": "181k",
        "author": "Pandit Biradar",
        "issued_at": "2019-10-09 07:08",
        "tags": [
            "python",
            "docker",
            "kubernetes",
            "dockerfile",
            "minikube"
        ],
        "answers": [
            {
                "answer": "This can also happen when your host machine has a different architecture from your guest container image.\nE.g. running an arm container on a host with x86-64 architecture",
                "upvotes": 191,
                "answered_by": "Rufus",
                "answered_at": "2020-09-08 05:24"
            },
            {
                "answer": "I can see that you add the command command: [/app/helloworld.py] to yaml file.\nso you need to (in Dockerfile):\nRUN chmod +x /app/helloworld.py\nset shebang to your py file:\n#!/usr/bin/env python # whatever your defualt python to run the script\nor setup the command the same as you did in Dockerfile",
                "upvotes": 60,
                "answered_by": "LinPy",
                "answered_at": "2019-10-09 07:30"
            },
            {
                "answer": "You probably compile your docker image for the wrong platform.\nFor instance, if you run on arm64 (Apple M1 or M2?) and want to compile to run on intel/amd, you shall use the --platform option of docker build.\ndocker build --platform linux/amd64 -t registry.gitlab.com/group/project:1.0-amd64 ./\nIf you use docker-compose.yml to build your image, use:\nservices:\n  app:\n    platform: linux/amd64\n    build: \n      context: ./myapp\n      ...\nFor the docker platform naming list, and more info, you should read: https://docs.docker.com/build/building/multi-platform/\ntip: to build for multiple platforms, use --platform linux/amd64,linux/arm64",
                "upvotes": 58,
                "answered_by": "Vincent J",
                "answered_at": "2021-09-21 17:24"
            }
        ]
    },
    {
        "title": "Docker command/option to display or list the build context",
        "url": "https://stackoverflow.com/questions/43808558/docker-command-option-to-display-or-list-the-build-context",
        "votes": "103",
        "views": "64k",
        "author": "Steve",
        "issued_at": "2017-05-05 15:19",
        "tags": [
            "docker",
            "dockerfile"
        ],
        "answers": [
            {
                "answer": "Answers above are great, but there is a low-tech solution for most cases - ncdu. This utility will show pretty and interactive tree structure with sizes. It has an option that will take patterns from a file and exclude them from scan. So you can just do ncdu -X .dockerignore. You will get something like this:\nThis is pretty close to what you will get in your docker image. One caveat is thou if you add a dot directory (like .yarn) into an image, it will not show in ncdu output.",
                "upvotes": 94,
                "answered_by": "RawCode",
                "answered_at": "2019-05-30 07:32"
            },
            {
                "answer": "The only way would be to add the current directory to an specific directory and list it.\nTry building with this Dockerfile:\nFROM busybox\n\nRUN mkdir /tmp/build/\n# Add context to /tmp/build/\nCOPY . /tmp/build/\nBuild it with:\ndocker build -t test .\nList all the files and directories in /tmp/build:\ndocker run --rm -it test find /tmp/build",
                "upvotes": 77,
                "answered_by": "Ricardo Branco",
                "answered_at": "2017-05-05 17:06"
            }
        ]
    },
    {
        "title": "What is the difference between Docker Service and Docker Container?",
        "url": "https://stackoverflow.com/questions/43408493/what-is-the-difference-between-docker-service-and-docker-container",
        "votes": "102",
        "views": "67k",
        "author": "Kunal Sehegal",
        "issued_at": "2017-04-14 09:05",
        "tags": [
            "docker",
            "docker-compose",
            "dockerfile"
        ],
        "answers": [
            {
                "answer": "docker run command is used to create a standalone container\ndocker service create command is used to create instances (called tasks) of that service running in a cluster (called swarm) of computers (called nodes). Those tasks are containers of course, but not standalone containers. In a sense a service acts as a template when instantiating tasks.\nFor example\ndocker service create --name MY_SERVICE_NAME --replicas 3 IMAGE:TAG\ncreates 3 tasks of the MY_SERVICE_NAME service, which is based on the IMAGE:TAG image.\nMore information can be found here",
                "upvotes": 30,
                "answered_by": "Teo Bebekis",
                "answered_at": "2019-02-13 19:37"
            },
            {
                "answer": "You can use docker in two way.\n1. Standalone mode\nWhen you are using the standalone mode you have installed docker daemon in only one machine. Here you have the ability to create/destroy/run a single container or multiple containers in that single machine.\nSo when you run docker run; the docker-cli creates an API query to the dockerd daemon to run the specified container.\nSo what you do with the docker run command only affects the single node/machine/host where you are running the command. If you add a volume or network with the container then those resources would only be available in the single node where you are running the docker run command.\n2. Swarm mode (or cluster mode)\nWhen you want or need to utilize the advantages of cluster computing like high availability, fault tolerance, horizontal scalability then you can use the swarm mode. With swarm mode, you can have multiple node/machine/host in your cluster and you can distribute your workload throughout the cluster. You can even initiate swarm mode in a single node cluster and you can add more node later.\nExample\nYou can recreate the scenario for free here. Suppose at this moment we have only one node called node-01.dc.local, where we have initiated following commands,\n####### Initiating swarm mode ########\n\n$ docker swarm init --advertise-addr eth0\n\nTo add a worker to this swarm, run the following command:\n\n    docker swarm join --token SWMTKN-1-21mxdqipe5lvzyiunpbrjk1mnzaxrlksnu0scw7l5xvri4rtjn-590dyij6z342uyxthletg7fu6 192.168.0.8:2377\n\n\n####### create a standalone container  #######\n[node1] (local) root@192.168.0.8 ~\n$ docker run -d --name app1 nginx\n\n####### creating a service #######\n[node1] (local) root@192.168.0.8 ~\n$ docker service create --name app2 nginx\nAfter a while, when you feel that you need to scale your workload you have added another machine named node-02.dc.local. And you want to scale and distribute your service to the newly created node. So we have run the following command on the node-02.dc.local node,\n####### Join the second machine/node/host in the cluster #######\n\n[node2] (local) root@192.168.0.7 ~\n$ docker swarm join --token SWMTKN-1-21mxdqipe5lvzyiunpbrjk1mnzaxrlksnu0scw7l5xvri4rtjn-590dyij6z342uyxthletg7fu6 192.168.0.8:2377\nThis node joined a swarm as a worker.\nNow from the first node I have run the followings to scale up the service.\n####### Listing services #######\n[node1] (local) root@192.168.0.8 ~\n$ docker service ls\nID             NAME      MODE         REPLICAS   IMAGE          PORTS\nsyn9jo2t4jcn   app2      replicated   1/1        nginx:latest   \n\n####### Scalling app2 from single container to 10 more container #######\n[node1] (local) root@192.168.0.8 ~\n$ docker service update --replicas 10 app2\napp2\noverall progress: 10 out of 10 tasks \n1/10: running   [==================================================>] \n2/10: running   [==================================================>] \n3/10: running   [==================================================>] \n4/10: running   [==================================================>] \n5/10: running   [==================================================>] \n6/10: running   [==================================================>] \n7/10: running   [==================================================>] \n8/10: running   [==================================================>] \n9/10: running   [==================================================>] \n10/10: running   [==================================================>] \nverify: Service converged \n[node1] (local) root@192.168.0.8 ~\n\n####### Verifying that app2's workload is distributed to both of the ndoes #######\n$ docker service ps app2\nID             NAME      IMAGE          NODE      DESIRED STATE   CURRENT STATE            ERROR     PORTS\nz12bzz5sop6i   app2.1    nginx:latest   node1     Running         Running 15 minutes ago             \n8a78pqxg38cb   app2.2    nginx:latest   node2     Running         Running 15 seconds ago             \nrcc0l0x09li0   app2.3    nginx:latest   node2     Running         Running 15 seconds ago             \nos19nddrn05m   app2.4    nginx:latest   node1     Running         Running 22 seconds ago             \nd30cyg5vznhz   app2.5    nginx:latest   node1     Running         Running 22 seconds ago             \no7sb1v63pny6   app2.6    nginx:latest   node2     Running         Running 15 seconds ago             \niblxdrleaxry   app2.7    nginx:latest   node1     Running         Running 22 seconds ago             \n7kg6esguyt4h   app2.8    nginx:latest   node2     Running         Running 15 seconds ago             \nk2fbxhh4wwym   app2.9    nginx:latest   node1     Running         Running 22 seconds ago             \n2dncdz2fypgz   app2.10   nginx:latest   node2     Running         Running 15 seconds ago  \nBut if you need to scale your app1 you can't because you have created the container with standalone mode.",
                "upvotes": 19,
                "answered_by": "arif",
                "answered_at": "2021-01-28 11:29"
            }
        ]
    },
    {
        "title": "Docker using gosu vs USER",
        "url": "https://stackoverflow.com/questions/36781372/docker-using-gosu-vs-user",
        "votes": "101",
        "views": "81k",
        "author": "MrE",
        "issued_at": "2016-04-21 22:14",
        "tags": [
            "docker",
            "dockerfile"
        ],
        "answers": [
            {
                "answer": "Dockerfiles are for creating images. I see gosu as more useful as part of a container initialization when you can no longer change users between run commands in your Dockerfile.\nAfter the image is created, something like gosu allows you to drop root permissions at the end of your entrypoint inside of a container. You may initially need root access to do some initialization steps (fixing uid's, host mounted volume permissions, etc). Then once initialized, you run the final service without root privileges and as pid 1 to handle signals cleanly.\nEdit: Here's a simple example of using gosu in an image for docker and jenkins: https://github.com/bmitch3020/jenkins-docker\nThe entrypoint.sh looks up the gid of the /var/lib/docker.sock file and updates the gid of the docker user inside the container to match. This allows the image to be ported to other docker hosts where the gid on the host may differ. Changing the group requires root access inside the container. Had I used USER jenkins in the dockerfile, I would be stuck with the gid of the docker group as defined in the image which wouldn't work if it doesn't match that of the docker host it's running on. But root access can be dropped when running the app which is where gosu comes in.\nAt the end of the script, the exec call prevents the shell from forking gosu, and instead it replaces pid 1 with that process. Gosu in turn does the same, switching the uid and then exec'ing the jenkins process so that it takes over as pid 1. This allows signals to be handled correctly which would otherwise be ignored by a shell as pid 1.",
                "upvotes": 81,
                "answered_by": "BMitch",
                "answered_at": "2016-06-20 21:11"
            }
        ]
    },
    {
        "title": "Multiple commands on docker ENTRYPOINT",
        "url": "https://stackoverflow.com/questions/54121031/multiple-commands-on-docker-entrypoint",
        "votes": "99",
        "views": "171k",
        "author": "radicaled",
        "issued_at": "2019-01-10 02:08",
        "tags": [
            "docker",
            "dockerfile"
        ],
        "answers": [
            {
                "answer": "In case you want to run many commands at entrypoint, the best idea is to create a bash file. For example commands.sh like this\n#!/bin/bash\nmkdir /root/.ssh\necho \"Something\"\ncd tmp\nls\n...\nAnd then, in your DockerFile, set entrypoint to commands.sh file (that execute and run all your commands inside)\nCOPY commands.sh /scripts/commands.sh\nRUN [\"chmod\", \"+x\", \"/scripts/commands.sh\"]\nENTRYPOINT [\"/scripts/commands.sh\"]\nAfter that, each time you start your container, commands.sh will be execute and run all commands that you need. You can take a look here https://github.com/dangminhtruong/drone-chatwork",
                "upvotes": 113,
                "answered_by": "Truong Dang",
                "answered_at": "2019-01-10 02:39"
            },
            {
                "answer": "I personally encountered a problem where I needed to preserve the behavior of the parent image. My solution Example : Dockerfile (PARENT)\nRUN ...\nRUN ...\nCOPY ./entrypoint.sh /\nENTRYPOINT [\"/entrypoint.sh\"]\nCMD [\"default_param1\"]\nDockerfile (CHILD)\nRUN mv /entrypoint.sh /parent-entrypoint.sh\nCOPY ./entrypoint.sh /\n\n#Optional. If the entry point is through a different script name\n#ENTRYPOINT [\"/entrypoint.sh\"] \n\n#Indicate if the entry point has changed\n#CMD [\"default_param1\"] \n/entrypoint.sh (child)\n#!/bin/sh\n/runMyscript.sh ; /parent-entrypoint.sh $@",
                "upvotes": 2,
                "answered_by": "AlexeyP0708",
                "answered_at": "2024-03-21 19:47"
            },
            {
                "answer": "You can use npm concurrently package.\nFor e.g.\nENTRYPOINT [\"npx\",\"concurrently\",\"command1\",\"command2\"]\nIt will run them in parallel.",
                "upvotes": -16,
                "answered_by": "Aminadav Glickshtein",
                "answered_at": "2019-07-09 14:07"
            }
        ]
    },
    {
        "title": "What does minikube docker-env mean?",
        "url": "https://stackoverflow.com/questions/52310599/what-does-minikube-docker-env-mean",
        "votes": "98",
        "views": "63k",
        "author": "Satyajit Das",
        "issued_at": "2018-09-13 09:22",
        "tags": [
            "docker",
            "kubernetes",
            "dockerfile",
            "minikube"
        ],
        "answers": [
            {
                "answer": "The command minikube docker-env returns a set of Bash environment variable exports to configure your local environment to re-use the Docker daemon inside the Minikube instance.\nPassing this output through eval causes bash to evaluate these exports and put them into effect.\nYou can review the specific commands which will be executed in your shell by omitting the evaluation step and running minikube docker-env directly. However, this will not perform the configuration \u2013 the output needs to be evaluated for that.\nThis is a workflow optimization intended to improve your experience with building and running Docker images which you can run inside the minikube environment. It is not mandatory that you re-use minikube's Docker daemon to use minikube effectively, but doing so will significantly improve the speed of your code-build-test cycle.\nIn a normal workflow, you would have a separate Docker registry on your host machine to that in minikube, which necessitates the following process to build and run a Docker image inside minikube:\nBuild the Docker image on the host machine.\nRe-tag the built image in your local machine's image registry with a remote registry or that of the minikube instance.\nPush the image to the remote registry or minikube.\n(If using a remote registry) Configure minikube with the appropriate permissions to pull images from the registry.\nSet up your deployment in minikube to use the image.\nBy re-using the Docker registry inside Minikube, this becomes:\nBuild the Docker image using Minikube's Docker instance. This pushes the image to Minikube's Docker registry.\nSet up your deployment in minikube to use the image.\nMore details of the purpose can be found in the minikube docs.",
                "upvotes": 104,
                "answered_by": "Cosmic Ossifrage",
                "answered_at": "2018-09-13 09:38"
            },
            {
                "answer": "One way to figure out what $ eval $(minikube docker-env) does is to understand that we want to build a docker image in our local machine and then deploy them to the minikube environment. This command as others have explained makes it easier to do so.\nIt is telling minikube to use the configs returned from minikube docker-env\nYou can then build your docker image locally and be able to access it within the minikube env\nOnce you're done building you can unset docker env i.e. disconnect your minikube env by unsetting these docker configs if you run minikube docker-env --unset\nWithout setting your docker configs to minikube env it'll be a bit tedious to build your image locally and run it in a container in your minikube env.\nIf you have minikube running you can ssh into the env and see all docker images running within it.",
                "upvotes": 4,
                "answered_by": "rghv404",
                "answered_at": "2022-01-03 02:26"
            },
            {
                "answer": "You should run this command after running 'minikube start'\neval $(minikube docker-env) this command let you Connect your cli tool to docker-env of Kubernetes cluster",
                "upvotes": 0,
                "answered_by": "elmagharoui khalil",
                "answered_at": "2021-11-21 12:17"
            }
        ]
    },
    {
        "title": "Use docker run command to pass arguments to CMD in Dockerfile",
        "url": "https://stackoverflow.com/questions/40873165/use-docker-run-command-to-pass-arguments-to-cmd-in-dockerfile",
        "votes": "96",
        "views": "209k",
        "author": "Jaaaaaaay",
        "issued_at": "2016-11-29 18:29",
        "tags": [
            "node.js",
            "docker",
            "dockerfile",
            "docker-cmd"
        ],
        "answers": [
            {
                "answer": "Make sure your Dockerfile declares an environment variable with ENV:\nENV environment default_env_value\nENV cluster default_cluster_value\nThe ENV <key> <value> form can be replaced inline.\nThen you can pass an environment variable with docker run. Note that each variable requires a specific -e flag to run.\ndocker run -p 9000:9000 -e environment=dev -e cluster=0 -d me/app\nOr you can set them through your compose file:\nnode:\n  environment:\n    - environment=dev\n    - cluster=0\nYour Dockerfile CMD can use that environment variable, but, as mentioned in issue 5509, you need to do so in a sh -c form:\nCMD [\"sh\", \"-c\", \"node server.js ${cluster} ${environment}\"]\nThe explanation is that the shell is responsible for expanding environment variables, not Docker. When you use the JSON syntax, you're explicitly requesting that your command bypass the shell and be executed directly.\nSame idea with Builder RUN (applies to CMD as well):\nUnlike the shell form, the exec form does not invoke a command shell.\nThis means that normal shell processing does not happen.\nFor example, RUN [ \"echo\", \"$HOME\" ] will not do variable substitution on $HOME. If you want shell processing then either use the shell form or execute a shell directly, for example: RUN [ \"sh\", \"-c\", \"echo $HOME\" ].\nWhen using the exec form and executing a shell directly, as in the case for the shell form, it is the shell that is doing the environment variable expansion, not docker.\nAs noted by Pwnstar in the comments:\nWhen you use a shell command, this process will not be started with id=1.",
                "upvotes": 143,
                "answered_by": "VonC",
                "answered_at": "2016-11-29 18:39"
            },
            {
                "answer": "Another option is to use ENTRYPOINT to specify that node is the executable to run and CMD to provide the arguments. The docs have an example in Exec form ENTRYPOINT example.\nUsing this approach, your Dockerfile will look something like\nFROM ...\n\nENTRYPOINT [ \"node\",  \"server.js\" ]\nCMD [ \"0\", \"dev\" ]\nRunning it in dev would use the same command\ndocker run -p 9000:9000 -d me/app\nand running it in prod you would pass the parameters to the run command\ndocker run -p 9000:9000 -d me/app 1 prod\nYou may want to omit CMD entirely and always pass in 0 dev or 1 prod as arguments to the run command. That way you don't accidentally start a prod container in dev or a dev container in prod.",
                "upvotes": 75,
                "answered_by": "Roman",
                "answered_at": "2016-11-29 20:27"
            },
            {
                "answer": "Late joining the discussion. Here's a nifty trick you can use to set default command line parameters while also supporting overriding the default arguments with custom ones:\nStep#1 In your dockerfile invoke your program like so:\nENV DEFAULT_ARGS \"--some-default-flag=123 --foo --bar\"\nCMD [\"/bin/bash\", \"-c\", \"./my-nifty-executable   ${ARGS:-${DEFAULT_ARGS}}\"]\nStep#2 When can now invoke the docker-image like so:\n# this will invoke it with DEFAULT_ARGS\ndocker run mydockerimage   \n\n# but this will invoke the docker image with custom arguments\ndocker run   --env  ARGS=\"--alternative-args  --and-then-some=123\"   mydockerimage \nYou can also adjust this technique to do much more complex argument-evaluation however you see fit. Bash supports many kinds of one-line constructs to help you towards that goal.\nHope this technique helps some folks out there save a few hours of head-scratching.",
                "upvotes": 7,
                "answered_by": "XDS",
                "answered_at": "2022-01-23 23:26"
            }
        ]
    },
    {
        "title": "How does the new Docker --squash work",
        "url": "https://stackoverflow.com/questions/41764336/how-does-the-new-docker-squash-work",
        "votes": "94",
        "views": "83k",
        "author": "Fore",
        "issued_at": "2017-01-20 13:00",
        "tags": [
            "docker",
            "dockerfile",
            "squash"
        ],
        "answers": [
            {
                "answer": "If I add a secret file in my first layer, then use the secret file in my second layer, and the finally remove my secret file in the third layer, and then build with the --squash flag.\nWill there be any way now to get the secret file?\nAnswer: Your image won't have the secret file.\nHow --squash works:\nOnce the build is complete, Docker creates a new image loading the diffs from each layer into a single new layer and references all the parent's layers.\nIn other words: when squashing, Docker will take all the filesystem layers produced by a build and collapse them into a single new layer.\nThis can simplify the process of creating minimal container images, but may result in slightly higher overhead when images are moved around (because squashed layers can no longer be shared between images). Docker still caches individual layers to make subsequent builds fast.\nPlease note this feature squashes all the newly built layers into a single layer, it is not squashing to scratch.\nSide notes:\nDocker 1.13 also has support for compressing the build context that is sent from CLI to daemon using the --compress flag. This will speed up builds done on remote daemons by reducing the amount of data sent.\nPlease note as of Docker 1.13 this feature is experimental.\nUpdate 2024: Squash has been moved to buildkit and later on deprecated from buildkit\nWARNING: experimental flag squash is removed with BuildKit. You should squash inside build using a multi-stage Dockerfile for efficiency.\nAs the warning suggests you need to use multi-stage builds instead of squashing layers.\nExample:\n# syntax=docker/dockerfile:1\nFROM golang:1.21\nWORKDIR /src\nCOPY <<EOF ./main.go\npackage main\n\nimport \"fmt\"\n\nfunc main() {\n  fmt.Println(\"hello, world\")\n}\nEOF\nRUN go build -o /bin/hello ./main.go\n\nFROM scratch\nCOPY --from=0 /bin/hello /bin/hello\nCMD [\"/bin/hello\"]",
                "upvotes": 99,
                "answered_by": "Farhad Farahi",
                "answered_at": "2017-01-20 13:23"
            },
            {
                "answer": "in 2023\nWith BuildKit, there is no such working thing as squash. You can run docker build --squash=true ., but then you'll get the message:\nWARNING: experimental flag squash is removed with BuildKit. You should squash inside build using a multi-stage Dockerfile for efficiency.\nHow does this work?\nSimply start FROM scratch like so:\nFROM nice-image:latest AS base\n\nRUN command-that-gives-huge-diff\nRUN more-things\n\n# can also do multiple times\nFROM scratch #(AS something)\n\nCOPY --from=base / /\nAs noted by @AndreKR:\nUnfortunately FROM scratch + COPY --from=base / / is not enough to squash an image, you also need to find out and copy the original ENV, CMD, ENTRYPONT, etc. commands.\nPractical example\nInstalling anything on archlinux:latest (afaik) requires to initialize the pacman db. This will fetch super new pacman repositories. Installing anything without upgrading will probably not work, since it will reference the new repositories, while the old ones are installed. So, to update, run pacman -Syu, which shows:\nTotal Download Size: 87.56 MiB\nTotal Installed Size: 359.99 MiB\nNet Upgrade Size: 4.99 MiB\nThat is a huge increase in container size that could be avoided! (Since the diffs between layers are saved, increase in container size is like >360MiB, even though the total size of the filesystem would only increase by ~5MiB)\nHere is the relevant dockerfile, where I installed gpg1:\nARG BASE=archlinux:latest\n\nFROM $BASE as base # use this stage for running system updates\n\nRUN pacman -Syu --noconfirm\n\n# start magic\nFROM scratch as stage1\n\nCOPY --from=base / /\n# end magic\n\n# start arch build stuff\nRUN pacman -Syu --noconfirm && \\\n    pacman -S --noconfirm base-devel bzip2 curl libldap libusb-compat pinentry readline zlib && \\\n    useradd --create-home --shell /bin/bash user\n\nUSER user\nWORKDIR /home/user\n\nRUN curl -L -O https://aur.archlinux.org/cgit/aur.git/snapshot/gnupg1.tar.gz && \\\n    tar -xvf gnupg1.tar.gz && \\\n    cd gnupg1 && \\\n    makepkg --skippgpcheck\n#end arch build stuff\n\nFROM scratch AS final\n\n# so now this is the initial container\nCOPY --from=base / /\n\n# copy built package from stage1\nCOPY --from=stage1 /home/user/gnupg1/gnupg1-1.4.23-1-x86_64.pkg.tar.zst /home/gnupg1-1.4.23-1-x86_64.pkg.tar.zst\n\n#install the package (no need for -Syu, since we copied from base) and clean\nRUN pacman -U --noconfirm /home/gnupg1-1.4.23-1-x86_64.pkg.tar.zst && \\\n    pacman -Scc --noconfirm && \\\n    rm -f /var/lib/pacman/sync/*\n\n#this saves only like 10MB, so not really necessary\nFROM scratch\n\nCOPY --from=final / /",
                "upvotes": 17,
                "answered_by": "Fee",
                "answered_at": "2023-09-06 13:04"
            }
        ]
    },
    {
        "title": "Execute command on host during docker build",
        "url": "https://stackoverflow.com/questions/42735468/execute-command-on-host-during-docker-build",
        "votes": "94",
        "views": "56k",
        "author": "Piotr Aleksander Chmielowski",
        "issued_at": "2017-03-11 12:55",
        "tags": [
            "docker",
            "dockerfile"
        ],
        "answers": [
            {
                "answer": "(Just a suggestion)\nWe usually have the following structure for building our docker images:\nmy-image/\n\u251c\u2500\u2500 assets\n\u2502   \u251c\u2500\u2500 entrypoint.sh\n\u2502   \u2514\u2500\u2500 install.sh\n\u251c\u2500\u2500 build.sh\n\u251c\u2500\u2500 Dockerfile\n\u251c\u2500\u2500 README.md\n\u2514\u2500\u2500 VERSION\nbuild.sh: This is where you should invoke script_that_creates_magic_file.sh. Other common tasks involve downloading required files or temporarily copying ssh keys from the host. Finally, this script will call docker build .\nDockerfile: As usual, but depending on the number of commands we need to run we might have an install.sh\ninstall.sh: This is copied and run inside the container, installs packages, removes unnecessary files, etc. Without being 100% sure - I think such an approach reduces the number of layers avoiding multiple commands in a single RUN\nentrypoint.sh: Container's entrypoint. Allows us to perform tasks when the container starts (like parse environment variables) and print debugging info\nI find the above structure convenient and self-documented since everyone in the team can build any image (no special instructions/steps). The README is there to explain what the image is doing... but I won't lie to you... it is usually empty... (or has an h1 for the gitlab to display) :)",
                "upvotes": 77,
                "answered_by": "urban",
                "answered_at": "2017-03-12 23:53"
            }
        ]
    },
    {
        "title": "Wordpress Docker won't increase upload limit",
        "url": "https://stackoverflow.com/questions/42983276/wordpress-docker-wont-increase-upload-limit",
        "votes": "93",
        "views": "81k",
        "author": "Kyle Calica-St",
        "issued_at": "2017-03-23 17:42",
        "tags": [
            "wordpress",
            "docker",
            "wordpress-theming",
            "docker-compose",
            "dockerfile"
        ],
        "answers": [
            {
                "answer": "it worked for me as follows: i created uploads.ini alongside of docker-compose.yml with following lines. this is exactly how it stated in fist post.\nfile_uploads = On\nmemory_limit = 500M\nupload_max_filesize = 500M\npost_max_size = 500M\nmax_execution_time = 600\nafter this i added\nvolumes: \n   - ./uploads.ini:/usr/local/etc/php/conf.d/uploads.ini\nto my .yml file as it states in first post.\nafter this i had to delete the container/images (basically start from scratch):\ndocker stop [image name]\ndocker rm [image name]\ndocker image rm [image name]\nsome places i end up using ID insted of image name. name or ID basically you have to stop, remove container and image. bottom line is start from scratch with additional line in your .yml file as describe in first post. remember, you'll lose all your wp work. now run\ndocker-compose up -d --build\nupload limit should be increased now. i was able to upload my new bigger theme after this change. no more upload file size error. only question is if you need to increase this upload size limit in the middle of your work then how would you do this?...",
                "upvotes": 163,
                "answered_by": "smaqsood",
                "answered_at": "2019-01-02 20:59"
            },
            {
                "answer": "I discovered my problem.\ndocker-compose kill will kill a container but rebuild it from a cache. Meaning no changes to my files were taking place.\nUse docker-compose up -d --build",
                "upvotes": 25,
                "answered_by": "Kyle Calica-St",
                "answered_at": "2017-04-05 17:56"
            }
        ]
    },
    {
        "title": "rebuild docker image from specific step",
        "url": "https://stackoverflow.com/questions/35154219/rebuild-docker-image-from-specific-step",
        "votes": "93",
        "views": "59k",
        "author": "sag",
        "issued_at": "2016-02-02 12:58",
        "tags": [
            "docker",
            "dockerfile"
        ],
        "answers": [
            {
                "answer": "You can rebuild the entire thing without using the cache by doing a\ndocker build --no-cache -t user/image-name\nTo force a rerun starting at a specific line, you can pass an arg that is otherwise unused. Docker passes ARG values as environment variables to your RUN command, so changing an ARG is a change to the command which breaks the cache. It's not even necessary to define it yourself on the RUN line.\nFROM ubuntu:14.04\nMAINTAINER Samuel Alexander <samuel@alexander.com>\n\nRUN apt-get -y install software-properties-common\nRUN apt-get -y update\n\n# Install Java.\nRUN echo oracle-java8-installer shared/accepted-oracle-license-v1-1 select true | debconf-set-selections\nRUN add-apt-repository -y ppa:webupd8team/java\nRUN apt-get -y update\nRUN apt-get install -y oracle-java8-installer\nRUN rm -rf /var/lib/apt/lists/*\nRUN rm -rf /var/cache/oracle-jdk8-installer\n\n# Define working directory.\nWORKDIR /work\n\n# Define commonly used JAVA_HOME variable\nENV JAVA_HOME /usr/lib/jvm/java-8-oracle\n\n# JAVA PATH\nENV PATH /usr/lib/jvm/java-8-oracle/bin:$PATH\n\n# Install maven\nRUN apt-get -y update\nRUN apt-get -y install maven\n\n# Install Open SSH and git\nRUN apt-get -y install openssh-server\nRUN apt-get -y install git\n\n# clone Spark\nRUN git clone https://github.com/apache/spark.git\nWORKDIR /work/spark\nRUN mvn -DskipTests clean package\n\n# clone and build zeppelin fork, changing INCUBATOR_VER will break the cache here\nARG INCUBATOR_VER=unknown\nRUN git clone https://github.com/apache/incubator-zeppelin.git\nWORKDIR /work/incubator-zeppelin\nRUN mvn clean package -Pspark-1.6 -Phadoop-2.6 -DskipTests\n\n# Install Supervisord\nRUN apt-get -y install supervisor\nRUN mkdir -p var/log/supervisor\n\n# Configure Supervisord\nCOPY conf/supervisord.conf /etc/supervisor/conf.d/supervisord.conf\n\n# bash\nRUN sed -i s#/home/git:/bin/false#/home/git:/bin/bash# /etc/passwd\n\nEXPOSE 8080 8082\nCMD [\"/usr/bin/supervisord\"]\nAnd then just run it with a unique arg:\ndocker build --build-arg INCUBATOR_VER=20160613.2 -t user/image-name .\nTo change the argument with every build, you can pass a timestamp as the arg:\ndocker build --build-arg INCUBATOR_VER=$(date +%Y%m%d-%H%M%S) -t user/image-name .\nor:\ndocker build --build-arg INCUBATOR_VER=$(date +%s) -t user/image-name .\nAs an aside, I'd recommend the following changes to keep your layers smaller, the more you can merge the cleanup and delete steps on a single RUN command after the download and install, the smaller your final image will be. Otherwise your layers will include all the intermediate steps between the download and cleanup:\nFROM ubuntu:14.04\nMAINTAINER Samuel Alexander <samuel@alexander.com>\n\nRUN DEBIAN_FRONTEND=noninteractive \\\n    apt-get -y install software-properties-common && \\\n    apt-get -y update\n\n# Install Java.\nRUN echo oracle-java8-installer shared/accepted-oracle-license-v1-1 select true | debconf-set-selections && \\\n    add-apt-repository -y ppa:webupd8team/java && \\\n    apt-get -y update && \\\n    DEBIAN_FRONTEND=noninteractive \\\n    apt-get install -y oracle-java8-installer && \\\n    apt-get clean && \\\n    rm -rf /var/lib/apt/lists/* && \\\n    rm -rf /var/cache/oracle-jdk8-installer && \\\n\n# Define working directory.\nWORKDIR /work\n\n# Define commonly used JAVA_HOME variable\nENV JAVA_HOME /usr/lib/jvm/java-8-oracle\n\n# JAVA PATH\nENV PATH /usr/lib/jvm/java-8-oracle/bin:$PATH\n\n# Install maven\nRUN apt-get -y update && \\\n    DEBIAN_FRONTEND=noninteractive \\\n    apt-get -y install \n      maven \\\n      openssh-server \\\n      git \\\n      supervisor && \\\n    apt-get clean && \\\n    rm -rf /var/lib/apt/lists/*\n\n# clone Spark\nRUN git clone https://github.com/apache/spark.git\nWORKDIR /work/spark\nRUN mvn -DskipTests clean package\n\n# clone and build zeppelin fork\nARG INCUBATOR_VER=unknown\nRUN git clone https://github.com/apache/incubator-zeppelin.git\nWORKDIR /work/incubator-zeppelin\nRUN mvn clean package -Pspark-1.6 -Phadoop-2.6 -DskipTests\n\n# Configure Supervisord\nRUN mkdir -p var/log/supervisor\nCOPY conf/supervisord.conf /etc/supervisor/conf.d/supervisord.conf\n\n# bash\nRUN sed -i s#/home/git:/bin/false#/home/git:/bin/bash# /etc/passwd\n\nEXPOSE 8080 8082\nCMD [\"/usr/bin/supervisord\"]",
                "upvotes": 96,
                "answered_by": "BMitch",
                "answered_at": "2016-06-13 20:32"
            },
            {
                "answer": "Update 2023\nDocker now includes --cache-from and --cache-to arguments. The post from @reddot below is now the best answer.\nPrevious answer\nA simpler technique.\nDockerfile:\nAdd this line where you want the caching to start being skipped.\nCOPY marker /dev/null\nThen build using\ndate > marker && docker build .",
                "upvotes": 17,
                "answered_by": "Bernard",
                "answered_at": "2018-04-14 11:48"
            },
            {
                "answer": "To complete Dmitry's answer, you can use uniq arg like date +%s to keep always same commandline\ndocker build --build-arg DUMMY=`date +%s` -t me/myapp:1.0.0\nDockerfile:\n...\nARG DUMMY=unknown\nRUN DUMMY=${DUMMY} git clone xxx\n...",
                "upvotes": 16,
                "answered_by": "toms130",
                "answered_at": "2017-10-30 14:35"
            }
        ]
    },
    {
        "title": "Docker compose with name other than dockerfile",
        "url": "https://stackoverflow.com/questions/57528077/docker-compose-with-name-other-than-dockerfile",
        "votes": "92",
        "views": "87k",
        "author": "Chris",
        "issued_at": "2019-08-16 16:24",
        "tags": [
            "docker",
            "docker-compose",
            "dockerfile"
        ],
        "answers": [
            {
                "answer": "In your docker-compose, under the service:\nservices:\n  serviceA:\n    build: \n      context: <folder of your project>\n      dockerfile: <path and name to your Dockerfile>",
                "upvotes": 166,
                "answered_by": "Mihai",
                "answered_at": "2019-08-16 16:31"
            },
            {
                "answer": "As mentioned in the documentation of docker-compose.yml, you can overwrite the Dockerfile filename within the build properties of your docker-compose services.\nFor example:\nversion: 3\nservices:\n  foo:\n    image: user/foo\n    build:\n      context: .../docks\n      dockerfile: foo.Dockerfile\n  bar:\n    image: user/bar\n    build:\n      context: .../docks\n      dockerfile: bar.Dockerfile",
                "upvotes": 73,
                "answered_by": "ErikMD",
                "answered_at": "2019-08-16 16:31"
            }
        ]
    },
    {
        "title": "Docker create network should ignore existing network",
        "url": "https://stackoverflow.com/questions/48643466/docker-create-network-should-ignore-existing-network",
        "votes": "90",
        "views": "50k",
        "author": "Anna Ira Hurnaus",
        "issued_at": "2018-02-06 12:49",
        "tags": [
            "docker",
            "dockerfile"
        ],
        "answers": [
            {
                "answer": "Building on @AndyTriggs' answer, a neat (and correct) solution would be:\ndocker network inspect my_local_network >/dev/null 2>&1 || \\\n    docker network create --driver bridge my_local_network",
                "upvotes": 116,
                "answered_by": "yktoo",
                "answered_at": "2018-10-29 19:19"
            },
            {
                "answer": "You can do it also in this way:\nNETWORK_NAME=my_local_network\nif [ -z $(docker network ls --filter name=^${NETWORK_NAME}$ --format=\"{{ .Name }}\") ] ; then \n     docker network create ${NETWORK_NAME} ; \nfi\nAdvantages:\nRegexp prevents omitting network creation in case of existing network with similar name.\nErrors in docker commands will not pass silently.\nIn fact it is very similar to the solution provided by @yktoo in comment under the answer of @Andy Triggs.",
                "upvotes": 12,
                "answered_by": "Grzegorz Skupien",
                "answered_at": "2018-08-21 08:54"
            }
        ]
    },
    {
        "title": "How to mount local volumes in docker machine",
        "url": "https://stackoverflow.com/questions/30040708/how-to-mount-local-volumes-in-docker-machine",
        "votes": "90",
        "views": "75k",
        "author": "jdcaballerov",
        "issued_at": "2015-05-04 21:58",
        "tags": [
            "docker",
            "dockerfile",
            "docker-compose"
        ],
        "answers": [
            {
                "answer": "Docker-machine automounts the users directory... But sometimes that just isn't enough.\nI don't know about docker 1.6, but in 1.8 you CAN add an additional mount to docker-machine\nAdd Virtual Machine Mount Point (part 1)\nCLI: (Only works when machine is stopped)\nVBoxManage sharedfolder add <machine name/id> --name <mount_name> --hostpath <host_dir> --automount\nSo an example in windows would be\n/c/Program\\ Files/Oracle/VirtualBox/VBoxManage.exe sharedfolder add default --name e --hostpath 'e:\\' --automount\nGUI: (does NOT require the machine be stopped)\nStart \"Oracle VM VirtualBox Manager\"\nRight-Click <machine name> (default)\nSettings...\nShared Folders\nThe Folder+ Icon on the Right (Add Share)\nFolder Path: <host dir> (e:)\nFolder Name: <mount name> (e)\nCheck on \"Auto-mount\" and \"Make Permanent\" (Read only if you want...) (The auto-mount is sort of pointless currently...)\nMounting in boot2docker (part 2)\nManually mount in boot2docker:\nThere are various ways to log in, use \"Show\" in \"Oracle VM VirtualBox Manager\", or ssh/putty into docker by IP address docker-machine ip default, etc...\nsudo mkdir -p <local_dir>\nsudo mount -t vboxsf -o defaults,uid=`id -u docker`,gid=`id -g docker` <mount_name> <local_dir>\nBut this is only good until you restart the machine, and then the mount is lost...\nAdding an automount to boot2docker:\nWhile logged into the machine\nEdit/create (as root) /mnt/sda1/var/lib/boot2docker/bootlocal.sh, sda1 may be different for you...\nAdd\nmkdir -p <local_dir>\nmount -t vboxsf -o defaults,uid=`id -u docker`,gid=`id -g docker` <mount_name> <local_dir>\nWith these changes, you should have a new mount point. This is one of the few files I could find that is called on boot and is persistent. Until there is a better solution, this should work.\nOld method: Less recommended, but left as an alternative\nEdit (as root) /mnt/sda1/var/lib/boot2docker/profile, sda1 may be different for you...\nAdd\nadd_mount() {\n  if ! grep -q \"try_mount_share $1 $2\" /etc/rc.d/automount-shares ; then\n    echo \"try_mount_share $1 $2\" >> /etc/rc.d/automount-shares\n  fi\n}\n\nadd_mount <local dir> <mount name>\nAs a last resort, you can take the slightly more tedious alternative, and you can just modify the boot image.\ngit -c core.autocrlf=false clone https://github.com/boot2docker/boot2docker.git\ncd boot2docker\ngit -c core.autocrlf=false checkout v1.8.1 #or your appropriate version\nEdit rootfs/etc/rc.d/automount-shares\nAdd try_mount_share <local_dir> <mount_name> line right before fi at the end. For example\ntry_mount_share /e e\nJust be sure not to set the to anything the os needs, like /bin, etc...\ndocker build -t boot2docker . #This will take about an hour the first time :(\ndocker run --rm boot2docker > boot2docker.iso\nBackup the old boot2docker.iso and copy your new one in its place, in ~/.docker/machine/machines/\nThis does work, it's just long and complicated\ndocker version 1.8.1, docker-machine version 0.4.0",
                "upvotes": 95,
                "answered_by": "Andy",
                "answered_at": "2015-08-15 23:30"
            },
            {
                "answer": "At the moment I can't really see any way to mount volumes on machines, so the approach by now would be to somehow copy or sync the files you need into the machine.\nThere are conversations on how to solve this issue on the docker-machine's github repo. Someone made a pull request implementing scp on docker-machine and it's already merged on master, so it's very likely that the next release will include it.\nSince it's not yet released, by now I would recommend that if you have your code hosted on github, just clone your repo before you run the app\nweb:\n  build: .\n  command: git clone https://github.com/my/repo.git; ./repo/run_web.sh\n  volumes:\n    - .:/app\n  ports:\n    - \"8000:8000\"\n  links:\n    - db:db\n    - rabbitmq:rabbit\n    - redis:redis\nUpdate: Looking further I found that the feature is already available in the latest binaries, when you get them you'll be able to copy your local project running a command like this:\ndocker-machine scp -r . dev:/home/docker/project\nBeing this the general form:\ndocker-machine scp [machine:][path] [machine:][path]\nSo you can copy files from, to and between machines.\nCheers!1",
                "upvotes": 14,
                "answered_by": "claudevandort",
                "answered_at": "2015-06-07 00:52"
            },
            {
                "answer": "Since October 2017 there is a new command for docker-machine that does the trick, but make sure there is nothing in the directory before executing it, otherwise it might get lost:\ndocker-machine mount <machine-name>:<guest-path> <host-path>\nCheck the docs for more information: https://docs.docker.com/machine/reference/mount/\nPR with the change: https://github.com/docker/machine/pull/4018",
                "upvotes": 5,
                "answered_by": "Jorge",
                "answered_at": "2018-02-13 23:23"
            }
        ]
    },
    {
        "title": "Can we mount sub-directories of a named volume in docker?",
        "url": "https://stackoverflow.com/questions/38164939/can-we-mount-sub-directories-of-a-named-volume-in-docker",
        "votes": "90",
        "views": "76k",
        "author": "nPn",
        "issued_at": "2016-07-02 22:21",
        "tags": [
            "docker",
            "docker-compose",
            "dockerfile"
        ],
        "answers": [
            {
                "answer": "2023: As noted by Michael Bolli in the comments, that feature is now a work-in-progress:\nPR 45687: \"volumes: Implement subpath mount\"\nMake it possible to mount subdirectory of a named volume.\nQ1 2024: this is merged! Commit 31ccdbb.\nPossibly for Moby 26.0.\n[Documentation: docker/docs PR 20577 with example:\nMount a volume subdirectory\nWhen you mount a volume to a container, you can specify a subdirectory of the volume to use, with the volume-subpath parameter for the --mount flag. The subdirectory that you specify must exist in the volume before you attempt to mount it into a container; if it doesn't exist, the mount fails.\nSpecifying volume-subpath is useful if you only want to share a specific portion of a volume with a container. Say for example that you have multiple containers running and you want to store logs from each container in a shared volume. You can create a subdirectory for each container in the shared volume, and mount the subdirectory to the container.\nThe following example creates a logs volume and initiates the subdirectories app1 and app2 in the volume. It then starts two containers and mounts one of the subdirectories of the logs volume to each container. This example assumes that the processes in the containers write their logs to /var/log/app1 and /var/log/app2.\n$ docker volume create logs\n$ docker run --rm \\\n  --mount src=logs,dst=/logs \\\n  alpine mkdir -p /logs/app1 /logs/app2\n$ docker run -d \\\n  --name=app1 \\\n  --mount src=logs,dst=/var/log/app1/,volume-subpath=app1 \\\n  app1:latest\n$ docker run -d \\\n  --name=app2 \\\n  --mount src=logs,dst=/var/log/app2,volume-subpath=app2 \\\n  app2:latest\nWith this setup, the containers write their logs to separate subdirectories of the logs volume. The containers can't access the other container's logs.\nMarch 2024, as mentioned in issue 32582:\nCLI already supports it (starting from v26.0.0-rc1): docker/cli PR #4331\n(and you can already install it from the test channel)\nCompose support is still WIP (it needs to move to v26 first).\nMarch 2024: Pawe\u0142 Gronowski confirms in the same issue:\nmoby v26.0.0 is released, so you can already try out this feature.\n2016: No because compose/config/config.py#load(config_details) check if datavolume/sql_data matches a named volume (in compose/config/validation.py#match_named_volumes()).\ndatavolume would, datavolume/sql_data would not.\nAs memetech points out in the comments, the is an issue tracking this since April 2017:\nmoby/moby issue 32582: \"[feature] Allow mounting sub-directories of named volumes\".\nIn that issue, Joohansson adds (see comment)\nIn the meantime, I use this workaround to mount the whole volume on a separate path and then symlink it to the sub path.\n# In the Dockerfile:\nRUN mkdir -p /data/subdir\nRUN ln -s /data/subdir /var/www/subdir\nThen mount the volume as normal.\nThe /subdir must exist in the volume.\ndocker run -d -v myvol:/data mycontainer\nNow anything read or written by the webserver will be stored in the volume subdir and can't access the other data.",
                "upvotes": 85,
                "answered_by": "VonC",
                "answered_at": "2016-07-03 06:07"
            },
            {
                "answer": "As VonC mentioned, a feature was released. I'll just leave an example here for docker compose:\nservices:\n  some-service:\n    volumes:\n      - type: volume\n        source: NAMED_VOLUME_NAME\n        target: PATH_INSIDE_CONTAINER\n        volume:\n          subpath: NAMED_VOLUME_SUBPATH\nvolumes:\n  NAMED_VOLUME_NAME:\nmore detailed example on the difference in configs between a physical volume and a named one:\nservices:\n  foo:\n    image: foo:latest\n    volumes:\n      - ./foo/configs:/app/data/configs\n      - ./foo/icons:/app/public/icons\n      - ./foo/data:/data\n    \n#--------------------------------------------\n\nservices:\n  foo:\n    image: foo:latest\n    volumes:\n      - type: volume\n        source: foo-storage\n        target: /app/data/configs\n        volume:\n          subpath: configs\n      - type: volume\n        source: foo-storage\n        target: /app/public/icons\n        volume:\n          subpath: icons\n      - type: volume\n        source: foo-storage\n        target: /data\n        volume:\n          subpath: data\n\nvolumes:\n  foo-storage:\nPS. you may potentially get an error \"no such file or directory\". So you need just create them.",
                "upvotes": 6,
                "answered_by": "Leonid Pavlov",
                "answered_at": "2024-11-27 09:52"
            },
            {
                "answer": "Yes, it can be using subpath while mounting a volume subdirectory to a container. However,\nvolume must exist beforehand and\nsubdirectories must be created before containers attach to it\nTo create a volume with subfolders, I made a utility function for me to use. I do:\n# source the function\nsource create_volume_with_folders.sh \n# create a new volume with however many folders you want\ncreate_volume_add_folders home_data pgadmin pgdata\nAfter that, pay attention to setting external=True for the volume in docker compose file. See from my create_volume_with_folders.sh Gist.",
                "upvotes": 1,
                "answered_by": "metinsenturk",
                "answered_at": "May 16 at 21:44"
            }
        ]
    },
    {
        "title": "Docker build pull access denied, repository does not exist or may require",
        "url": "https://stackoverflow.com/questions/65960211/docker-build-pull-access-denied-repository-does-not-exist-or-may-require",
        "votes": "89",
        "views": "159k",
        "author": "greektreat",
        "issued_at": "2021-01-29 18:54",
        "tags": [
            "windows",
            "docker",
            "dockerfile",
            "linux-containers"
        ],
        "answers": [
            {
                "answer": "Are you logged in? Try with docker login and then execute docker build once again\nAs I can see here aspnet in docker hub the repository that you intent to use has been renamed. Use the following instead\nfrom mcr.microsoft.com/dotnet/aspnet:3.1-buster-slim\nRepositories have been renamed and a similar issue can be found here similar issue",
                "upvotes": 5,
                "answered_by": "Panagiotis Bougioukos",
                "answered_at": "2021-01-29 19:41"
            },
            {
                "answer": "You will also get insufficient_scope: authorization failed if the image you're referencing does not match your system architecture. You can work around this by explicitly specifying the architecture in the build command.\nIn my case it went something like this.\n\u276f  docker pull sometargetimage@sha256:25f9d7l...f8caaa\n\u276f  docker tag sometargetimage awesome\n\u276f  docker build .\n  Dockerfile:1\n  --------------------\n     1 | >>> FROM awesome\n     2 |\n     3 |\n  --------------------\n  ERROR: failed to solve: awesome: failed to resolve source metadata for docker.io/library/awesome:latest: pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed\n\n\u276f  docker build .  --platform linux/amd64\n  # no error",
                "upvotes": 0,
                "answered_by": "MatrixManAtYrService",
                "answered_at": "Jan 9 at 6:20"
            }
        ]
    },
    {
        "title": "\"docker build\" requires exactly 1 argument(s)",
        "url": "https://stackoverflow.com/questions/46517241/docker-build-requires-exactly-1-arguments",
        "votes": "87",
        "views": "122k",
        "author": "Homunculus Reticulli",
        "issued_at": "2017-10-01 21:51",
        "tags": [
            "docker",
            "dockerfile"
        ],
        "answers": [
            {
                "answer": "Parameter -f changes the name of the Dockerfile (when it's different than regular Dockerfile). It is not for passing the full path to docker build. The path goes as the first argument.\nSyntax is:\ndocker build [PARAMS] PATH\nSo in your case, this should work:\ndocker build -f MyDockerfile -t proj:myapp /full/path/to/\nor in case you are in the project directory, you just need to use a dot:\ndocker build -f MyDockerfile -t proj:myapp .",
                "upvotes": 150,
                "answered_by": "Krzysztof At\u0142asik",
                "answered_at": "2017-10-01 21:58"
            },
            {
                "answer": "docker build .\nDO NOT MISS \"THE DOT\".",
                "upvotes": 109,
                "answered_by": "best wishes",
                "answered_at": "2021-03-02 12:02"
            },
            {
                "answer": "A. Please upvote Krzysztof's answer. I was lost without his hints.\nBut B, my contribution.\nI was able to use a full path for the --file argument.\ndocker build --build-arg JAR_FILE=\"/source/java/mystuff/build/libs/mything-1.0.10-SNAPSHOT.jar\" --file /full/path/to/MyDockerfile -t proj:myapp .\nI had forgotten that little \".\" at the end.\nD'oh!\nAka, do NOT overlook the last little \".\" (period) at the end !!!",
                "upvotes": 12,
                "answered_by": "granadaCoder",
                "answered_at": "2020-09-22 19:45"
            }
        ]
    },
    {
        "title": "Is it possible to show the `WORKDIR` when building a docker image?",
        "url": "https://stackoverflow.com/questions/37782505/is-it-possible-to-show-the-workdir-when-building-a-docker-image",
        "votes": "87",
        "views": "168k",
        "author": "Freewind",
        "issued_at": "2016-06-13 05:42",
        "tags": [
            "docker",
            "dockerfile"
        ],
        "answers": [
            {
                "answer": "There's no builtin way for Docker to print the WORKDIR during a build. You can inspect the final workdir for an image/layer via the .Config.WorkingDir property in the inspect output:\ndocker image inspect -f '{{.Config.WorkingDir}}' {image-name}\nIt's possible to view a Linux containers build steps workdir by printing the shells default working directory:\nRUN pwd\nor the shell often stores the working directory in the PWD environment variable\nRUN echo \"$PWD\"\nIf the RUN step has run before and is cached, add the --no-cache flag. If you are using newer versions of docker with BuildKit, stdout from the build will need to be enabled with --progress=plain\ndocker build --no-cache --progress=plain . ",
                "upvotes": 82,
                "answered_by": "Matt",
                "answered_at": "2016-06-13 07:02"
            },
            {
                "answer": "You can check the content of directories during build steps and print it with commands like\nRUN ls\nRUN ls ..\nUnder ubuntu 20.04.4 LTS WSL2 I have to combine --progress=plain with debug mode and no-cache to see output:\ndocker -D build --progress=plain --no-cache. ",
                "upvotes": 12,
                "answered_by": "Evgeny",
                "answered_at": "2022-08-06 23:33"
            }
        ]
    },
    {
        "title": "How do I run a Bash script in an Alpine Docker container?",
        "url": "https://stackoverflow.com/questions/44803982/how-do-i-run-a-bash-script-in-an-alpine-docker-container",
        "votes": "85",
        "views": "144k",
        "author": "Kurt Peek",
        "issued_at": "2017-06-28 13:31",
        "tags": [
            "docker",
            "dockerfile",
            "alpine-linux"
        ],
        "answers": [
            {
                "answer": "Alpine comes with ash as the default shell instead of bash.\nSo you can\nHave a shebang defining /bin/bash as the first line of your sayhello.sh, so your file sayhello.sh will begin with bin/sh\n#!/bin/sh\nInstall Bash in your Alpine image, as you seem to expect Bash is present, with such a line in your Dockerfile:\nRUN apk add --no-cache --upgrade bash",
                "upvotes": 139,
                "answered_by": "user2915097",
                "answered_at": "2017-06-28 13:53"
            },
            {
                "answer": "This answer is completely right and works fine.\nThere is another way. You can run a Bash script in an Alpine-based Docker container.\nYou need to change CMD like below:\nCMD [\"sh\", \"sayhello.sh\"]\nAnd this works too.",
                "upvotes": 44,
                "answered_by": "Shahriar",
                "answered_at": "2018-02-03 13:06"
            },
            {
                "answer": "By using the CMD, Docker is searching the sayhello.sh file in the PATH, BUT you copied it in / which is not in the PATH.\nSo use an absolute path to the script you want to execute:\nCMD [\"/sayhello.sh\"]\nBTW, as @user2915097 said, be careful that Alpine doesn't have Bash by default in case of your script using it in the shebang.",
                "upvotes": 5,
                "answered_by": "zigarn",
                "answered_at": "2017-06-28 13:56"
            }
        ]
    },
    {
        "title": "/bin/sh: 1: apk: not found while creating docker image",
        "url": "https://stackoverflow.com/questions/40445243/bin-sh-1-apk-not-found-while-creating-docker-image",
        "votes": "85",
        "views": "223k",
        "author": "john",
        "issued_at": "2016-11-06 01:49",
        "tags": [
            "ubuntu",
            "docker",
            "dockerfile",
            "apt"
        ],
        "answers": [
            {
                "answer": "As larsks mentions, apk is for Alpine distributions and you selected FROM ubuntu:trusty which is Debian based with the apt-get command. Change your FROM line to FROM alpine:3.4 to switch to the Alpine based image with apk support.",
                "upvotes": 86,
                "answered_by": "BMitch",
                "answered_at": "2016-11-06 02:40"
            },
            {
                "answer": "Quite late to the party. I'll put down what worked for me.\nAs john rightly pointed out that apk is package manager for alpine distributions, for ubuntu image, we need to use apt-get:\nFROM ubuntu:trusty\nRUN apt-get update && apt-get install -y tini\nOtherwise Alpine base image can be used to run apk commands:\nFROM python:3.7-alpine3.12\nRUN apk add --no-cache tini",
                "upvotes": 24,
                "answered_by": "sxddhxrthx",
                "answered_at": "2021-04-05 07:56"
            },
            {
                "answer": "Hmm I don't see this case but I'm using Docker with an ubuntu:22.04 base image and what actually works is apt (without -get) as in:\nRUN apt install util-linux",
                "upvotes": -1,
                "answered_by": "Pawel Gorczynski",
                "answered_at": "2024-06-26 09:15"
            }
        ]
    },
    {
        "title": "How to prevent Dockerfile caching git clone",
        "url": "https://stackoverflow.com/questions/36996046/how-to-prevent-dockerfile-caching-git-clone",
        "votes": "84",
        "views": "29k",
        "author": "Raindy",
        "issued_at": "2016-05-03 05:11",
        "tags": [
            "git",
            "docker",
            "dockerfile"
        ],
        "answers": [
            {
                "answer": "Another workaround:\nIf you use GitHub (or gitlab or bitbucket too most likely) you can ADD the GitHub API's representation of your repo to a dummy location.\nADD https://api.github.com/repos/$USER/$REPO/git/refs/heads/$BRANCH version.json\nRUN git clone -b $BRANCH https://github.com/$USER/$REPO.git $GIT_HOME/\nThe API call will return different results when the head changes, invalidating the docker cache.\nIf you're dealing with private repos you can use github's x-oauth-basic authentication scheme with a personal access token like so:\nADD https://$ACCESS_TOKEN:x-oauth-basic@api.github.com/repos/$USER/$REPO/git/refs/heads/$BRANCH version.json\n(thx @captnolimar for a suggested edit to clarify authentication)",
                "upvotes": 94,
                "answered_by": "anq",
                "answered_at": "2016-09-01 18:07"
            },
            {
                "answer": "Issue 1996 is not yet available, but you have the following workaround:\nFROM foo\nARG CACHE_DATE=2016-01-01\nRUN git clone ...\n\ndocker build --build-arg CACHE_DATE=$(date) ....\nThat would invalidate cache after the ARG CACHE_DATE line for every build.\nOr:\nADD http://www.convert-unix-time.com/api?timestamp=now /tmp/bustcache\nRUN git pull\nThat would also invalidate cache after this ADD line.\nSimilar idea:\nAdd ARG command to your Dockerfile:\n# Dockerfile\n# add this and below command will run without cache\nARG CACHEBUST=1\nWhen you need to rebuild with selected cache, run it with --build-arg option\n$ docker build -t your-image --build-arg CACHEBUST=$(date +%s) .\nthen only layer below ARG command in Dockerfile will rebuild.",
                "upvotes": 17,
                "answered_by": "VonC",
                "answered_at": "2016-05-03 05:16"
            }
        ]
    },
    {
        "title": "How to give folder permissions inside a docker container Folder",
        "url": "https://stackoverflow.com/questions/45972608/how-to-give-folder-permissions-inside-a-docker-container-folder",
        "votes": "84",
        "views": "339k",
        "author": "Shivanand T",
        "issued_at": "2017-08-31 03:11",
        "tags": [
            "docker",
            "permissions",
            "dockerfile",
            "permission-denied"
        ],
        "answers": [
            {
                "answer": "On line two, after FROM ..., adding one line:\nUSER root\nThis will give you the power of root user.",
                "upvotes": 0,
                "answered_by": "Raymond",
                "answered_at": "2022-08-11 21:06"
            }
        ]
    },
    {
        "title": "How to add user with dockerfile?",
        "url": "https://stackoverflow.com/questions/39855304/how-to-add-user-with-dockerfile",
        "votes": "83",
        "views": "181k",
        "author": "Atlantic0",
        "issued_at": "2016-10-04 14:46",
        "tags": [
            "docker",
            "dockerfile"
        ],
        "answers": [
            {
                "answer": "Use useradd instead of its interactive adduser to add user.\nRUN useradd -ms /bin/bash  vault\nBelow command will not create user .\nUSER vault\nWORKDIR /usr/local/bin/vault\nit will use vault user\nplease Refer Dockerfile User Documentation\nThe USER instruction sets the user name or UID to use when running the image and for any RUN, CMD and ENTRYPOINT instructions that follow it in the Dockerfile.\nNOTE : Ensures that bash is the default shell.\nIf default shell is /bin/sh you can do like:\nRUN ln -sf /bin/bash /bin/sh\nRUN useradd -ms /bin/bash  vault",
                "upvotes": 90,
                "answered_by": "pl_rock",
                "answered_at": "2016-10-04 14:49"
            },
            {
                "answer": "To add group and to associate a new user, use code below.\nFROM <base image>\nRUN groupadd -g 2000 go \\\n&& useradd -m -u 2001 -g go go\nUSER go\nOR\nRUN addgroup -g 1001 -S appuser && adduser -u 1001 -S appuser  -G appuser ",
                "upvotes": 11,
                "answered_by": "Renato Coutinho",
                "answered_at": "2018-05-10 17:47"
            }
        ]
    },
    {
        "title": "alpine package py-pip missing",
        "url": "https://stackoverflow.com/questions/44633903/alpine-package-py-pip-missing",
        "votes": "82",
        "views": "118k",
        "author": "user1050619",
        "issued_at": "2017-06-19 14:58",
        "tags": [
            "docker",
            "dockerfile",
            "alpine-linux"
        ],
        "answers": [
            {
                "answer": "For python3 on alpine edge:\napk add py3-setuptools",
                "upvotes": 23,
                "answered_by": "user2601130",
                "answered_at": "2019-01-09 21:17"
            }
        ]
    },
    {
        "title": "Hidden file .env not copied using Docker COPY",
        "url": "https://stackoverflow.com/questions/42132475/hidden-file-env-not-copied-using-docker-copy",
        "votes": "82",
        "views": "84k",
        "author": "Hsiu Chuan Tsao",
        "issued_at": "2017-02-09 09:14",
        "tags": [
            "linux",
            "ubuntu",
            "docker",
            "docker-compose",
            "dockerfile"
        ],
        "answers": [
            {
                "answer": "There is a statement in .dockerignore file documentation:\nNote: For historical reasons, the pattern . is ignored.",
                "upvotes": 29,
                "answered_by": "eQ19",
                "answered_at": "2019-09-07 21:24"
            }
        ]
    },
    {
        "title": "Docker-Compose file has yaml.scanner.ScannerError",
        "url": "https://stackoverflow.com/questions/39077526/docker-compose-file-has-yaml-scanner-scannererror",
        "votes": "81",
        "views": "123k",
        "author": "Skeffington",
        "issued_at": "2016-08-22 10:45",
        "tags": [
            "docker",
            "yaml",
            "docker-compose",
            "dockerfile"
        ],
        "answers": [
            {
                "answer": "Ok, I wasted around 3 hours to debug a similar issue.\nIf you guys ever get the below error\nERROR: yaml.scanner.ScannerError: mapping values are not allowed here\nin \".\\docker-compose.yml\", line 2, column 9\nIt's because a space is needed between\nversion:'3' <-- this is wrong\nversion: '3' <-- this is correct.\nAlso, if you are using eclipse, do yourself a favor and install the YEdit YAML editor plugin",
                "upvotes": 164,
                "answered_by": "virtuvious",
                "answered_at": "2017-08-15 00:54"
            }
        ]
    },
    {
        "title": "How to ADD all files/directories except a hidden directory like .git in Dockerfile",
        "url": "https://stackoverflow.com/questions/28079872/how-to-add-all-files-directories-except-a-hidden-directory-like-git-in-dockerfi",
        "votes": "81",
        "views": "77k",
        "author": "Larry Cai",
        "issued_at": "2015-01-22 01:23",
        "tags": [
            "docker",
            "dockerfile"
        ],
        "answers": [
            {
                "answer": "You may exclude unwanted files with the help of the .dockerignore file",
                "upvotes": 99,
                "answered_by": "Mykola Gurov",
                "answered_at": "2015-01-22 07:39"
            },
            {
                "answer": "How can we avoid including the .git directory in simple way ?\nJust create a file called .dockerignore in the root context folder with the following lines\n**/.git\n**/node_modules\nWith such lines Docker will exclude directories .git and node_modules from any subdirectory including root. Docker also supports a special wildcard string ** that matches any number of directories (including zero).\nAnd secondly, it will lose the directory structure, since good\\a1 gets changed to a1\nWith .dockerignore it won't\n$ docker run -it --rm sample tree /opt/\n/opt/\n\u251c\u2500\u2500 Dockerfile\n\u251c\u2500\u2500 c\n\u2502   \u2514\u2500\u2500 no_sslv2.patch\n\u2514\u2500\u2500 good\n    \u2514\u2500\u2500 a1\n        \u2514\u2500\u2500 README\n\n3 directories, 3 files\nReference to official docs: .dockerignore",
                "upvotes": 47,
                "answered_by": "ALex_hha",
                "answered_at": "2017-07-01 18:23"
            }
        ]
    },
    {
        "title": "Error response from daemon: Dockerfile parse error Unknown flag: mount",
        "url": "https://stackoverflow.com/questions/55153089/error-response-from-daemon-dockerfile-parse-error-unknown-flag-mount",
        "votes": "80",
        "views": "82k",
        "author": "PMende",
        "issued_at": "2019-03-14 00:21",
        "tags": [
            "docker",
            "dockerfile"
        ],
        "answers": [
            {
                "answer": "tl;dr\nDockerfile\n# syntax=docker/dockerfile:experimental\nFROM continuumio/miniconda3\n\nRUN --mount=type=ssh pip install git+ssh://git@github.com/myrepo/myproject.git@develop\nRUN conda install numpy\n...\nNote: the comment on the first line is required voodoo\nThen build your docker image with:\nDOCKER_BUILDKIT=1 docker build --ssh default -t my_image .\nWith this, you will be able to use the --mount option for the RUN directive in your Dockerfile.\nLong answer\nAs found in the documentation here, ssh forwarding when building docker image is enabled only when using the BuildKit backend:\nExternal implementation features\nThis feature is only available when using the BuildKit backend.\nDocker build supports experimental features like cache mounts, build secrets and ssh forwarding that are enabled by using an external implementation of the builder with a syntax directive. To learn about these features, refer to the documentation in BuildKit repository.\nFor this you need Docker 18.09 (or later) and you also need to run the docker build command with the DOCKER_BUILDKIT=1 environment variable and start your Docker file with the following magic comment : # syntax=docker/dockerfile:experimental.\nAlso you can edit /etc/docker/daemon.json and add :\n{\n    \"experimental\" : false,\n    \"debug\" : true,\n    \"features\": {\n        \"buildkit\" : true\n    }\n}",
                "upvotes": 150,
                "answered_by": "Thomasleveil",
                "answered_at": "2019-03-14 00:36"
            },
            {
                "answer": "The error message that you are getting due to writing --mount inside the Dockerfile. You have to enable Docker BuildKit first in order to use this syntax.\nYou can check all of the currently available build options through here",
                "upvotes": 6,
                "answered_by": "Mostafa Hussein",
                "answered_at": "2019-03-14 00:31"
            }
        ]
    },
    {
        "title": "Does putting ARG at top of Dockerfile prevent layer re-use?",
        "url": "https://stackoverflow.com/questions/41593135/does-putting-arg-at-top-of-dockerfile-prevent-layer-re-use",
        "votes": "80",
        "views": "29k",
        "author": "Woodrow Barlow",
        "issued_at": "2017-01-11 14:11",
        "tags": [
            "docker",
            "dockerfile"
        ],
        "answers": [
            {
                "answer": "In general, it is better to place ARG just before it is used rather than at the top of the file.\nTo be more precise, not all lines are cache invalidated after an ARG declaration. Only those that use ARG values and RUNs. The docker documentation provides details:\nImpact on build caching\nARG variables are not persisted into the built image as ENV variables are. However, ARG variables do impact the build cache in similar ways. If a Dockerfile defines an ARG variable whose value is different from a previous build, then a \u201ccache miss\u201d occurs upon its first usage, not its definition. In particular, all RUN instructions following an ARG instruction use the ARG variable implicitly (as an environment variable), thus can cause a cache miss. All predefined ARG variables are exempt from caching unless there is a matching ARG statement in the Dockerfile.\nhttps://docs.docker.com/engine/reference/builder/#impact-on-build-caching\nYou'll have to move your ARGs under the RUNs that would not need the argument in order to keep layer cache optimized.\nFor more info:\nhttps://github.com/moby/moby/issues/18017\nhttps://github.com/moby/moby/pull/18161\nRUN explanation here: https://github.com/moby/moby/pull/21885",
                "upvotes": 69,
                "answered_by": "Brown nightingale",
                "answered_at": "2019-07-13 09:12"
            }
        ]
    },
    {
        "title": "Difference between 'image' and 'build' within docker compose",
        "url": "https://stackoverflow.com/questions/34316047/difference-between-image-and-build-within-docker-compose",
        "votes": "79",
        "views": "33k",
        "author": "meallhour",
        "issued_at": "2015-12-16 15:35",
        "tags": [
            "docker",
            "dockerfile",
            "docker-compose",
            "docker-registry"
        ],
        "answers": [
            {
                "answer": "image means docker compose will run a container based on that image\nbuild means docker compose will first build an image based on the Dockerfile found in the path associated with build (and then run a container based on that image).\nPR 2458 was eventually merged to allow both (and use image as the image name when building, if it exists).\ntherobyouknow mentions in the comments:\ndockerfile: as a sub-statement beneath build: can be used to specify the filename/path of the Dockerfile.\nversion: '3'\nservices:\n  webapp:\n    build:\n      context: ./dir\n      dockerfile: Dockerfile-alternate\n      args:\n        buildno: 1",
                "upvotes": 72,
                "answered_by": "VonC",
                "answered_at": "2015-12-16 15:43"
            }
        ]
    },
    {
        "title": "Should I minimize the number of docker layers?",
        "url": "https://stackoverflow.com/questions/47079114/should-i-minimize-the-number-of-docker-layers",
        "votes": "78",
        "views": "24k",
        "author": "gukoff",
        "issued_at": "2017-11-02 15:34",
        "tags": [
            "docker",
            "optimization",
            "dockerfile",
            "layer"
        ],
        "answers": [
            {
                "answer": "I just wanted to see what were the differences of 2 images, one built with multiple RUNs and the other built with one RUN concatenating commands.\nIn the first case, the images are doing trivial operations (creating and deleting files).\nContent of the \"single\" layer image:\nFROM busybox\n\nRUN echo This is the 1 > 1 \\\n    && rm -f 1 \\\n    && echo This is the 2 > 2 \\\n    && rm -f 2 \\\n# ... for about 70 commands\nContent of the multiple layers image:\nFROM busybox\n\nRUN echo This is the 1 > 1\nRUN rm -f 1\nRUN echo This is the 2 > 2\nRUN rm -f 2\n# ... for about 70 layers\nThe build time is very different (multiple: 0m34,973s, singular: 0m0,568s). The container start-up time is also different but less noticeable (multiple: 0m0,435s, singular: 0m0,378s). I've run different times the images but the times do not change that much.\nConcerning the space, I've looked on purpose for the worst case for the multiple layer case and as expected the multiple layer image is bigger than the single layer.\nIn another test, I concatenated layers that only add content to the image. The build time does not change from the previous case but the run-time case shows something a little different: the multi layer image is faster to start-up than the single layer image. Concerning the space, same results.\nI don't think this proves anything but I had fun in doing it :P",
                "upvotes": 47,
                "answered_by": "Stefano",
                "answered_at": "2017-11-02 19:03"
            }
        ]
    },
    {
        "title": "Docker image error: \"/bin/sh: 1: [python,: not found\"",
        "url": "https://stackoverflow.com/questions/32709075/docker-image-error-bin-sh-1-python-not-found",
        "votes": "76",
        "views": "108k",
        "author": "Joe Mornin",
        "issued_at": "2015-09-22 05:32",
        "tags": [
            "python",
            "django",
            "ubuntu",
            "docker",
            "dockerfile"
        ],
        "answers": [
            {
                "answer": "Use \" instead of ' in CMD. (Documentation)",
                "upvotes": 121,
                "answered_by": "Aleksandr Kovalev",
                "answered_at": "2015-09-22 05:42"
            },
            {
                "answer": "I have resolved my issue on my Mac by changing\nCMD [\"python\", \"app.py\"]\nto\nCMD python app.py",
                "upvotes": 22,
                "answered_by": "srinivasa karadi",
                "answered_at": "2018-02-15 21:06"
            },
            {
                "answer": "I had the same error. But in my case it was syntax error in command.\nI had a missing comma \",\"\nCMD [\"python\" \"app.py\"]\ninstead of\nCMD [\"python\", \"app.py\"]\nValidating the yaml file format can help in this case. Can use any online yaml validator.",
                "upvotes": 13,
                "answered_by": "Apoorva Manjunath",
                "answered_at": "2019-05-26 06:24"
            }
        ]
    },
    {
        "title": "Maven docker cache dependencies",
        "url": "https://stackoverflow.com/questions/42208442/maven-docker-cache-dependencies",
        "votes": "76",
        "views": "77k",
        "author": "Daniel Watrous",
        "issued_at": "2017-02-13 16:06",
        "tags": [
            "java",
            "docker",
            "maven",
            "caching",
            "dockerfile"
        ],
        "answers": [
            {
                "answer": "You should also consider using mvn dependency:resolve or mvn dependency:go-offline accordingly as other comments & answers suggest.\nUsually, there's no change in pom.xml file but just some other source code changes when you're attempting to start docker image build. In such circumstance you can do this:\nFROM maven:3-jdk-8\n\nENV HOME=/home/usr/app\n\nRUN mkdir -p $HOME\n\nWORKDIR $HOME\n\n# 1. add pom.xml only here\n\nADD pom.xml $HOME\n\n# 2. start downloading dependencies\n\nRUN [\"/usr/local/bin/mvn-entrypoint.sh\", \"mvn\", \"verify\", \"clean\", \"--fail-never\"]\n\n# 3. add all source code and start compiling\n\nADD . $HOME\n\nRUN [\"mvn\", \"package\"]\n\nEXPOSE 8005\n\nCMD [\"java\", \"-jar\", \"./target/dist.jar\"]\nSo the key is:\nadd pom.xml file.\nthen mvn verify --fail-never it, it will download maven dependencies.\nadd all your source file then, and start your compilation(mvn package).\nWhen there are changes in your pom.xml file or you are running this script for the first time, docker will do 1 -> 2 -> 3. When there are no changes in pom.xml file, docker will skip step 1\u30012 and do 3 directly.\nThis simple trick can be used in many other package management circumstances(gradle, yarn, npm, pip).",
                "upvotes": 70,
                "answered_by": "Kim",
                "answered_at": "2017-08-31 07:01"
            },
            {
                "answer": "Using BuildKit\nFrom Docker v18.03 onwards you can use BuildKit instead of volumes that were mentioned in the other answers. It allows mounting caches that can persist between builds and you can avoid downloading contents of the corresponding .m2/repository every time.\nAssuming that the Dockerfile is in the root of your project:\n# syntax = docker/dockerfile:1.0-experimental\n\nFROM maven:3.6.0-jdk-11-slim AS build\nCOPY . /home/build\nRUN mkdir /home/.m2\nWORKDIR /home/.m2\nUSER root\nRUN --mount=type=cache,target=/root/.m2 mvn -f /home/build/pom.xml clean compile\ntarget=/root/.m2 mounts cache to the specified place in maven image Dockerfile docs.\nFor building you can run the following command:\nDOCKER_BUILDKIT=1 docker build --rm --no-cache  .   \nMore info on BuildKit can be found here.",
                "upvotes": 28,
                "answered_by": "Farzad Vertigo",
                "answered_at": "2019-09-04 04:06"
            }
        ]
    },
    {
        "title": "How are Packer and Docker different? Which one should I prefer when provisioning images?",
        "url": "https://stackoverflow.com/questions/47169353/how-are-packer-and-docker-different-which-one-should-i-prefer-when-provisioning",
        "votes": "76",
        "views": "33k",
        "author": "Abhineetraj",
        "issued_at": "2017-11-07 23:55",
        "tags": [
            "docker",
            "docker-compose",
            "dockerfile",
            "docker-image",
            "packer"
        ],
        "answers": [
            {
                "answer": "Docker is a system for building, distributing and running OCI images as containers. Containers can be run on Linux and Windows.\nPacker is an automated build system to manage the creation of images for containers and virtual machines. It outputs an image that you can then take and run on the platform you require.\nFor v1.8 this includes - Alicloud ECS, Amazon EC2, Azure, CloudStack, DigitalOcean, Docker, Google Cloud, Hetzner, Hyper-V, Libvirt, LXC, LXD, 1&1, OpenStack, Oracle OCI, Parallels, ProfitBricks, Proxmox, QEMU, Scaleway, Triton, Vagrant, VirtualBox, VMware, Vultr\nDocker's Dockerfile\nDocker uses a Dockerfile to manage builds which has a specific set of instructions and rules about how you build a container.\nImages are built in layers. Each FROM RUN ADD COPY commands modify the layers included in an OCI image. These layers can be cached which helps speed up builds. Each layer can also be addressed individually which helps with disk usage and download usage when multiple images share layers.\nDockerfiles have a bit of a learning curve, It's best to look at some of the official Docker images for practices to follow.\nPacker's Docker builder\nPacker does not require a Dockerfile to build a container image. The docker plugin has a HCL or JSON config file which start the image build from a specified base image (like FROM).\nPacker then allows you to run standard system config tools called \"Provisioners\" on top of that image. Tools like Ansible, Chef, Salt, shell scripts etc. This image will then be exported as a single layer, so you lose the layer caching/addressing benefits compared to a Dockerfile build.\nPacker allows some modifications to the build container environment, like running as --privileged or mounting a volume at build time, that Docker builds will not allow.\nTimes you might want to use Packer are if you want to build images for multiple platforms and use the same setup. It also makes it easy to use existing build scripts if there is a provisioner for it.",
                "upvotes": 86,
                "answered_by": "Matt",
                "answered_at": "2017-11-08 03:25"
            },
            {
                "answer": "Expanding on the Which one is easier/quickest to provision/maintain and why? What are the pros and cons of having a docker file?`\nFrom personal experience learning and using both, I found: (YMMV)\ndocker configuration was easier to learn than packer\ndocker configuration was harder to coerce into doing what I wanted than packer\nspeed difference in creating the image was negligible, after development\ndocker was faster during development, because of the caching\nthe docker daemon consumed some system resources even when not using docker\nthere are a handful of processes running as the daemon\nI did my development on Windows, though I was targeting LINUX servers for running the images. That isn't an issue during development, except for a foible of running Docker on Windows.\nThe docker daemon reserves various TCP port ranges for itself\nThe ranges might change every time you reboot your system or restart the daemon\nThe only error message is to the effect: can't use that port! but not why it can't\nBTW, The workaround is to:\nturn off Hypervisor\nreboot\nreserve the public ports you want your host system to see\nturn on hypervisor\nreboot\nRunning packer on Windows, however, the issue I found is that the provisioner I wanted to use, ansible, doesn't run on Windows.\nSigh.\nSo I end up having to run packer on a LINUX system after all.\nJust because I was feeling perverse, I wrote a Dockerfile so I could run both packer and ansible from my Windows station in a docker container using that image.",
                "upvotes": 12,
                "answered_by": "Jesse Chisholm",
                "answered_at": "2020-03-25 23:24"
            }
        ]
    },
    {
        "title": "Docker container doesn't expose ports when --net=host is mentioned in the docker run command",
        "url": "https://stackoverflow.com/questions/35586778/docker-container-doesnt-expose-ports-when-net-host-is-mentioned-in-the-docker",
        "votes": "75",
        "views": "103k",
        "author": "arevur",
        "issued_at": "2016-02-23 19:47",
        "tags": [
            "docker",
            "ip",
            "dockerfile",
            "docker-build"
        ],
        "answers": [
            {
                "answer": "I was confused by this answer. Apparently my docker image should be reachable on port 8080. But it wasn't. Then I read\nhttps://docs.docker.com/network/host/\nTo quote\nThe host networking driver only works on Linux hosts, and is not supported on Docker for Mac, Docker for Windows, or Docker EE for Windows Server.\nThat's rather annoying as I'm on a Mac. The docker command should report an error rather than let me think it was meant to work.\nDiscussion on why it does not report an error\nhttps://github.com/docker/for-mac/issues/2716\nNot sure I'm convinced.\nUpdated 2024: As per comments and other answers there have been changes in this area. See Docker container doesn't expose ports when --net=host is mentioned in the docker run command",
                "upvotes": 179,
                "answered_by": "Shane Gannon",
                "answered_at": "2018-09-28 11:47"
            },
            {
                "answer": "On Linux, I have always used --net=host when myapp needed to connect to an another docker container hosting PostgreSQL.\nmyapp reads an environment variable DATABASE in this example\nLike Shane mentions this does not work on MacOS or Windows...\ndocker run -d -p 127.0.0.1:5432:5432 postgres:latest\nSo my app can't connect to my other other docker container:\ndocker run -e DATABASE=127.0.0.1:5432 --net=host myapp\nTo work around this, you can use host.docker.internal instead of 127.0.0.1 to resolve your hosts IP address.\nTherefore, this works\ndocker run -e DATABASE=host.docker.internal:5432 -d myapp\nHope this saves someone time!",
                "upvotes": 23,
                "answered_by": "rjdkolb",
                "answered_at": "2022-03-16 12:50"
            }
        ]
    },
    {
        "title": "Using docker-compose to set containers timezones",
        "url": "https://stackoverflow.com/questions/39172652/using-docker-compose-to-set-containers-timezones",
        "votes": "75",
        "views": "199k",
        "author": "nobody",
        "issued_at": "2016-08-26 18:28",
        "tags": [
            "docker",
            "containers",
            "docker-compose",
            "dockerfile",
            "devops"
        ],
        "answers": [
            {
                "answer": "version \"2\"\n\nservices:\n  serviceA:\n    ...\n    environment:\n      TZ: \"America/Denver\"\n    command: >\n      sh -c \"ln -snf /usr/share/zoneinfo/$TZ /etc/localtime && \n      echo $TZ > /etc/timezone &&\n      exec my-main-application\"\nEdit: The question didn't ask for it but I've just added exec my-main-application to show how the main process would be specified. exec is important here to make sure that my-main-application receives Ctrl-C (SIGINT/SIGKILL).",
                "upvotes": 61,
                "answered_by": "Bernard",
                "answered_at": "2016-08-27 13:23"
            }
        ]
    },
    {
        "title": "Dockerfile CMD instruction will exit the container just after running it",
        "url": "https://stackoverflow.com/questions/42218957/dockerfile-cmd-instruction-will-exit-the-container-just-after-running-it",
        "votes": "73",
        "views": "216k",
        "author": "Anand Suthar",
        "issued_at": "2017-02-14 06:02",
        "tags": [
            "bash",
            "shell",
            "docker",
            "sh",
            "dockerfile"
        ],
        "answers": [
            {
                "answer": "A docker container will run as long as the CMD from your Dockerfile takes.\nIn your case your CMD consists of a shell script containing a single echo. So the container will exit after completing the echo.\nYou can override CMD, for example:\nsudo docker run -it --entrypoint=/bin/bash <imagename>\nThis will start an interactive shell in your container instead of executing your CMD. Your container will exit as soon as you exit that shell.\nIf you want your container to remain active, you have to ensure that your CMD keeps running. For instance, by adding the line while true; do sleep 1; done to your shell.sh file, your container will print your hello message and then do nothing any more until you stop it (using docker stop in another terminal).\nYou can open a shell in the running container using docker exec -it <containername> bash. If you then execute command ps ax, it will show you that your shell.sh is still running inside the container.",
                "upvotes": 52,
                "answered_by": "NZD",
                "answered_at": "2017-02-14 06:16"
            },
            {
                "answer": "Finally with some experiments I got my best result as below\nThere is nothing wrong with my Dockerfile as below it's correct.\nFROM ubuntu:14.04\n\nADD shell.sh /usr/local/bin/shell.sh\n\nRUN chmod 777 /usr/local/bin/shell.sh\n\nCMD /usr/local/bin/shell.sh\nWhat I do to get expected result is, I just add one more command(/bin/bash) in my shell script file as below and vola everything works in my best way.\n#!/bin/bash\n\necho \u201cHello-docker\u201d > /usr/hello.txt\n\n/bin/bash",
                "upvotes": 43,
                "answered_by": "Anand Suthar",
                "answered_at": "2017-02-14 07:47"
            },
            {
                "answer": "CMD bash -C '/path/to/start.sh';'bash'",
                "upvotes": 7,
                "answered_by": "lanni654321",
                "answered_at": "2018-05-30 10:24"
            }
        ]
    },
    {
        "title": "How to run kubectl commands inside a container?",
        "url": "https://stackoverflow.com/questions/42642170/how-to-run-kubectl-commands-inside-a-container",
        "votes": "73",
        "views": "91k",
        "author": "Dreams",
        "issued_at": "2017-03-07 07:02",
        "tags": [
            "docker",
            "kubernetes",
            "dockerfile"
        ],
        "answers": [
            {
                "answer": "I would use kubernetes api, you just need to install curl, instead of kubectl and the rest is restful.\ncurl http://localhost:8080/api/v1/namespaces/default/pods\nIm running above command on one of my apiservers. Change the localhost to apiserver ip address/dns name.\nDepending on your configuration you may need to use ssl or provide client certificate.\nIn order to find api endpoints, you can use --v=8 with kubectl.\nexample:\nkubectl get pods --v=8\nResources:\nKubernetes API documentation\nUpdate for RBAC:\nI assume you already configured rbac, created a service account for your pod and run using it. This service account should have list permissions on pods in required namespace. In order to do that, you need to create a role and role binding for that service account.\nEvery container in a cluster is populated with a token that can be used for authenticating to the API server. To verify, Inside the container run:\ncat /var/run/secrets/kubernetes.io/serviceaccount/token\nTo make request to apiserver, inside the container run:\ncurl -ik \\\n     -H \"Authorization: Bearer $(cat /var/run/secrets/kubernetes.io/serviceaccount/token)\" \\\n     https://kubernetes.default.svc.cluster.local/api/v1/namespaces/default/pods",
                "upvotes": 51,
                "answered_by": "Farhad Farahi",
                "answered_at": "2017-03-07 07:17"
            },
            {
                "answer": "Bit late to the party here, but this is my two cents:\nI've found using kubectl within a container much easier than calling the cluster's api\n(Why? Auto authentication!)\nSay you're deploying a Node.js project that needs kubectl usage.\nDownload & Build kubectl inside the container\nBuild your application, copying kubectl to your container\nVoila! kubectl provides a rich cli for managing your kubernetes cluster\nHelpful documentation\n--- EDITS ---\nAfter working with kubectl in my cluster pods, I found a more effective way to authenticate pods to be able to make k8s API calls. This method provides stricter authentication.\nCreate a ServiceAccount for your pod, and configure your pod to use said account. k8s Service Account docs\nConfigure a RoleBinding or ClusterRoleBinding to allow services to have the authorization to communicate with the k8s API. k8s Role Binding docs\nCall the API directly, or use a the k8s-client to manage API calls for you. I HIGHLY recommend using the client, it has automatic configuration for pods which removes the authentication token step required with normal requests.\nWhen you're done, you will have the following: ServiceAccount, ClusterRoleBinding, Deployment (your pods)\nFeel free to comment if you need some clearer direction, I'll try to help out as much as I can :)\nAll-in-on example\napiVersion: extensions/v1beta1\nkind: Deployment\nmetadata:\n  name: k8s-101\nspec:\n  replicas: 3\n  template:\n    metadata:\n      labels:\n        app: k8s-101\n    spec:\n      serviceAccountName: k8s-101-role\n      containers:\n      - name: k8s-101\n        imagePullPolicy: Always\n        image: salathielgenese/k8s-101\n        ports:\n        - name: app\n          containerPort: 3000\n---\nkind: ClusterRoleBinding\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: k8s-101-role\nsubjects:\n- kind: ServiceAccount\n  name: k8s-101-role\n  namespace: default\nroleRef:\n  kind: ClusterRole\n  name: cluster-admin\n  apiGroup: rbac.authorization.k8s.io\n---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: k8s-101-role\nThe salathielgenese/k8s-101 image contains kubectl. So one can just log into a pod container & execute kubectl as if he was running it on k8s host: kubectl exec -it pod-container-id -- kubectl get pods",
                "upvotes": 43,
                "answered_by": "mster",
                "answered_at": "2018-08-08 00:45"
            },
            {
                "answer": "First Question\n/usr/local/bin/kubectl: cannot execute binary file\nIt looks like you downloaded the OSX binary for kubectl. When running in Docker you probably need the Linux one:\nhttps://storage.googleapis.com/kubernetes-release/release/$(curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt)/bin/linux/amd64/kubectl\nSecond Question\nIf you run kubectl in a proper configured Kubernetes cluster, it should be able to connect to the apiserver.\nkubectl basically uses this code to find the apiserver and authenticate: github.com/kubernetes/client-go/rest.InClusterConfig\nThis means:\nThe host and port of the apiserver are stored in the environment variables KUBERNETES_SERVICE_HOST and KUBERNETES_SERVICE_PORT.\nThe access token is mounted to var/run/secrets/kubernetes.io/serviceaccount/token.\nThe server certificate is mounted to /var/run/secrets/kubernetes.io/serviceaccount/ca.crt.\nThis is all data kubectl needs to know to connect to the apiserver.\nSome thoughts why this might won't work:\nThe container doesn't run in Kubernetes.\nIt's not enough to use the same Docker host; the container needs to run as part of a pod definition.\nThe access is restricted by using an authorization plugin (which is not the default).\nThe service account credentials are overwritten by the pod definition (spec.serviceAccountName).",
                "upvotes": 20,
                "answered_by": "svenwltr",
                "answered_at": "2017-03-07 15:06"
            }
        ]
    },
    {
        "title": "Reuse inherited image's CMD or ENTRYPOINT",
        "url": "https://stackoverflow.com/questions/42280792/reuse-inherited-images-cmd-or-entrypoint",
        "votes": "73",
        "views": "44k",
        "author": "Johnathan Elmore",
        "issued_at": "2017-02-16 17:24",
        "tags": [
            "shell",
            "docker",
            "dockerfile"
        ],
        "answers": [
            {
                "answer": "If the base image is not yours, you unfortunately have to call the parent command manually.\nIf you own the parent image, you can try what the people at camptocamp suggest here.\nThey basically use a generic script as an entry point that calls run-parts on a directory. What that does is run all scripts in that directory in lexicographic order. So when you extend an image, you just have to put your new scripts in that same folder.\nHowever, that means you'll have to maintain order by prefixing your scripts which could potentially get out of hand. (Imagine the parent image decides to add a new script later...).\nAnyway, that could work.\nUpdate #1\nThere is a long discussion on this docker compose issue about provisioning after container run. One suggestion is to wrap you docker run or compose command in a shell script and then run docker exec on your other commands.\nIf you'd like to use that approach, you basically keep the parent CMD as the run command and you place yours as a docker exec after your docker run.",
                "upvotes": 12,
                "answered_by": "Morgan Kobeissi",
                "answered_at": "2017-05-01 20:48"
            },
            {
                "answer": "Using mysql image as an example\nDo docker inspect mysql/mysql-server:5.7 and see that:\nConfig.Cmd=\"mysqld\"\nConfig.Entrypoint=\"/entrypoint.sh\"\nwhich we put in bootstrap.sh (remember to chmod a+x):\n#!/bin/bash\n\necho $HOSTNAME\necho \"Start my initialization script...\"\n\n# docker inspect results used here\n/entrypoint.sh mysqld\nDockerfile is now:\nFROM mysql/mysql-server:5.7\n# put our script inside the image\nADD bootstrap.sh /etc/bootstrap.sh\n# set to run our script\nENTRYPOINT [\"/bin/sh\",\"-c\"]\nCMD [\"/etc/bootstrap.sh\"]\nBuild and run our new image:\ndocker build --rm -t sidazhou/tmp-mysql:5.7 .\ndocker run -it --rm sidazhou/tmp-mysql:5.7\nOutputs:\n6f5be7c6d587\nStart my initialization script...\n[Entrypoint] MySQL Docker Image 5.7.28-1.1.13\n[Entrypoint] No password option specified for new database.\n...\n...\nYou'll see this has the same output as the original image:\ndocker run -it --rm mysql/mysql-server:5.7\n[Entrypoint] MySQL Docker Image 5.7.28-1.1.13\n[Entrypoint] No password option specified for new database.\n...\n...",
                "upvotes": 6,
                "answered_by": "Sida Zhou",
                "answered_at": "2021-07-02 08:17"
            }
        ]
    },
    {
        "title": "Docker - What is proper way to rebuild and push updated image to docker cloud?",
        "url": "https://stackoverflow.com/questions/36714384/docker-what-is-proper-way-to-rebuild-and-push-updated-image-to-docker-cloud",
        "votes": "72",
        "views": "92k",
        "author": "Alex T",
        "issued_at": "2016-04-19 09:34",
        "tags": [
            "docker",
            "dockerfile",
            "docker-image",
            "docker-cloud"
        ],
        "answers": [
            {
                "answer": "Another solution, albeit bruteforce, is to rebuild with the --no-cache flag before pushing again.\ndocker rmi --force my-djnago-app:latest\n\ndocker build -t my-djnago-app:latest . --no-cache\n\ndocker push my-djnago-app:latest",
                "upvotes": 21,
                "answered_by": "Grey Perez",
                "answered_at": "2019-08-21 19:21"
            },
            {
                "answer": "I meet the problem as well(In my web application), like this:\n# when I push my contaimer to repo\n$ docker push <container>\nThe push refers to repository [docker.io/xx/getting-started]\nfd5aa641b308: Layer already exists\nd9c60c6f98e8: Layer already exists\nd9d14867f6d7: Layer already exists\n64ce166099ca: Layer already exists\n73b670e35c69: Layer already exists\n5f70bf18a086: Layer already exists\n9ea142d097a5: Layer already exists\n52f5845b1de0: Layer already exists\nI try my Solution, and it's works!\n# force remove image\n$ docker rmi --force <image-id>\n# tag for image\n$ docker tag <image-name> <your-dockerHub-username>/<image-name>\n# push image, just done!\n$ docker push <your-user-name>/<image-name>\nterminal output:\n# when I push my container to repo\n$ docker push <container>\nThe push refers to repository [docker.io/xx/getting-started]\n# it'll push your part of changes\nfd5aa641b308: Pushed\nd9c60c6f98e8: Pushed\nd9d14867f6d7: Layer already exists\n64ce166099ca: Layer already exists\n73b670e35c69: Layer already exists\n5f70bf18a086: Layer already exists\n9ea142d097a5: Layer already exists\n52f5845b1de0: Layer already exists\nThen, open my web appliction, it's updates the lastest version!",
                "upvotes": 1,
                "answered_by": "lidean",
                "answered_at": "2021-01-29 08:39"
            }
        ]
    },
    {
        "title": "Error in docker: network \"path\" declared as external, but could not be found",
        "url": "https://stackoverflow.com/questions/70307460/error-in-docker-network-path-declared-as-external-but-could-not-be-found",
        "votes": "71",
        "views": "132k",
        "author": "geekt",
        "issued_at": "2021-12-10 16:17",
        "tags": [
            "docker",
            "docker-compose",
            "dockerfile"
        ],
        "answers": [
            {
                "answer": "I solved the issue, finally. The issue came from the fact that I had in docker-compose.yml remaxmdcrm_remaxmd-network declared as external. The external network was not created during installation, thus I needed to create a bridging network. I ran the command docker network create \"name_of_network\"\nFor further details, here is the full documentation this",
                "upvotes": 154,
                "answered_by": "geekt",
                "answered_at": "2021-12-10 19:45"
            }
        ]
    },
    {
        "title": "Externalising Spring Boot properties when deploying to Docker",
        "url": "https://stackoverflow.com/questions/46057625/externalising-spring-boot-properties-when-deploying-to-docker",
        "votes": "70",
        "views": "181k",
        "author": "MeanwhileInHell",
        "issued_at": "2017-09-05 14:37",
        "tags": [
            "docker",
            "spring-boot",
            "dockerfile",
            "spring-boot-configuration"
        ],
        "answers": [
            {
                "answer": "DOCKER IMAGE CONFIGURATION\nIf you look to the way Spring recommends to launch a Spring Boot powered docker container, that's what you find:\nFROM openjdk:8-jdk-alpine\nVOLUME /tmp\nARG JAR_FILE\nCOPY ${JAR_FILE} app.jar\nENTRYPOINT [\"java\",\"-Djava.security.egd=file:/dev/./urandom\",\"-jar\",\"/app.jar\"]\nThat means your image extends openjdk and your container has its own environment. If you're doing like that, it would be enough to declare what you want to override as environment properties and Spring Boot will fetch them, since environment variables take precedence over the yml files.\nEnvironment variables can be passed in your docker command too, to launch the container with your desired configuration. If you want to set some limit for the JVM memory, see the link below.\nDOCKER COMPOSE SAMPLE\nHere you have an example of how I launch a simple app environment with docker compose. As you see, I declare the spring.datasource.url property here as an environment variable, so it overrides whatever you've got in your application.yml file.\nversion: '2'\nservices:\n    myapp:\n        image: mycompany/myapp:1.0.0\n        container_name: myapp\n        depends_on:\n        - mysql\n        environment:\n            - SPRING_DATASOURCE_URL=jdbc:mysql://mysql:3306/myapp?useUnicode=true&characterEncoding=utf8&useSSL=false\n        ports:\n            - 8080:8080\n\n    mysql:\n        image: mysql:5.7.19\n        container_name: mysql\n        volumes:\n            - /home/docker/volumes/myapp/mysql/:/var/lib/mysql/\n        environment:\n            - MYSQL_USER=root\n            - MYSQL_ALLOW_EMPTY_PASSWORD=yes\n            - MYSQL_DATABASE=myapp\n        command: mysqld --lower_case_table_names=1 --skip-ssl --character_set_server=utf8\nSee also:\nHow do I pass environment variables to Docker containers?\nLimit JVM memory consumption in a Docker container",
                "upvotes": 71,
                "answered_by": "Aritz",
                "answered_at": "2017-09-05 14:57"
            },
            {
                "answer": "I personally would consider two options:\nUsing an environment variable per config\napp:\n  image: my-app:latest\n  ports:\n    - \"8080:8080\"\n  environment:\n     SPRING_DATASOURCE_URL=jdbc:mysql://db:3306/table\nUsing SPRING_APPLICATION_JSON\napp:\n  image: my-app:latest\n  ports:\n    - \"8080:8080\"\n  environment:\n    SPRING_APPLICATION_JSON: '{\n      \"spring.datasource.url\": \"jdbc:mysql://db:3306/table\",\n    }'",
                "upvotes": 38,
                "answered_by": "Rafal Enden",
                "answered_at": "2019-02-18 17:17"
            }
        ]
    },
    {
        "title": "Install python package in docker file",
        "url": "https://stackoverflow.com/questions/50333650/install-python-package-in-docker-file",
        "votes": "67",
        "views": "283k",
        "author": "Mehdi",
        "issued_at": "2018-05-14 15:09",
        "tags": [
            "python",
            "docker",
            "dockerfile"
        ],
        "answers": [
            {
                "answer": "Recommended base image\nAs suggested in my comment, you could write a Dockerfile that looks like:\nFROM python:3\n\nRUN pip install --no-cache-dir --upgrade pip && \\\n    pip install --no-cache-dir nibabel pydicom matplotlib pillow med2image\n    # Note: we had to merge the two \"pip install\" package lists here, otherwise\n    # the last \"pip install\" command in the OP may break dependency resolution\u2026\n\nCMD [\"cat\", \"/etc/os-release\"]\nAnd the command example above could confirm at runtime (docker build --pull -t test . && docker run --rm -it test) that this image is based on the GNU/Linux distribution \"Debian stable\".\nGeneric Dockerfile template\nFinally to give a comprehensive answer, note that a good practice regarding Python dependencies consists in specifying them in a declarative way in a dedicated text file (in alphabetical order, to ease review and update) so that for your example, you may want to write the following file:\nrequirements.txt\nmatplotlib\nmed2image\nnibabel\npillow\npydicom\nand use the following generic Dockerfile\nFROM python:3\n\nWORKDIR /usr/src/app\n\nCOPY requirements.txt ./\n\nRUN pip install --no-cache-dir --upgrade pip \\\n  && pip install --no-cache-dir -r requirements.txt\n\nCOPY . .\n\nCMD [\"python\", \"./your-daemon-or-script.py\"]\nTo be more precise, this is the approach suggested in the documentation of the Docker official image python, \u00a7. How to use this image",
                "upvotes": 91,
                "answered_by": "ErikMD",
                "answered_at": "2018-05-14 21:14"
            },
            {
                "answer": "Some of the other answers/comments are suggesting to change your base image but if you want to keep your ubuntu 16.04 you can also simply specify your version of pip/python to use pip3 or pip3.5 like shown below.\nFROM ubuntu:16.04\n\nRUN apt-get update && apt-get install -y --no-install-recommends \\\n    python3.5 \\\n    python3-pip \\\n    && \\\n    apt-get clean && \\\n    rm -rf /var/lib/apt/lists/*\n\nRUN pip3 install nibabel pydicom matplotlib pillow\nRUN pip3 install med2image",
                "upvotes": 48,
                "answered_by": "mgc",
                "answered_at": "2018-05-14 21:20"
            },
            {
                "answer": "Try pip3\nRUN pip3 install nibabel pydicom matplotlib pillow\nRUN pip3 install med2image",
                "upvotes": 5,
                "answered_by": "d g",
                "answered_at": "2018-05-14 21:19"
            },
            {
                "answer": "If you have Jupyter Notebook in your docker container, you can install any python package by running a new Terminal in Jupyter by clicking the button shown here:\nand running: pip install <package-name>\nThe package stays in the docker container even after you exit the container.",
                "upvotes": 2,
                "answered_by": "Sayyor Y",
                "answered_at": "2021-02-08 18:12"
            }
        ]
    },
    {
        "title": "Is it redundant in a Dockfile to run USER root since you're already root?",
        "url": "https://stackoverflow.com/questions/43705442/is-it-redundant-in-a-dockfile-to-run-user-root-since-youre-already-root",
        "votes": "67",
        "views": "140k",
        "author": "hawkeye",
        "issued_at": "2017-04-30 10:31",
        "tags": [
            "docker",
            "dockerfile"
        ],
        "answers": [
            {
                "answer": "Yes, it's redundant, but there's almost no downside to leaving this redundancy in. This may have been done to develop against other images, or to support uses that may swap out the base image. It could be done to prevent future issues if the upstream image changes it's behavior. Or they may just want to be explicit so it's clear this container needs to run commands as root.\nThis assumes that you have verified that the base image is running as root (as the OP has done). For others with this question, if you run:\ndocker image inspect --format '{{.Config.User}}' $base_image_name\nand see anything other than an empty string or root, then you need USER root to change the user for tasks that require that access (e.g. installing packages, changing filesystem permissions, and writing files in folders not owned by your user). After performing those steps (in separate RUN lines) be sure to change back to the non-privileged user in your released image with another USER youruser line.",
                "upvotes": 52,
                "answered_by": "BMitch",
                "answered_at": "2017-04-30 11:18"
            },
            {
                "answer": "if you are already root, then it's redundant to use it.\nAs @BMitch also points out, you can use USER root to ensure you are not going to break things if the parent image changes the user in upcoming versions, among other things.\nIt actually depends on the image. In some images, such as grafana/grafana, the default user is not root and there is no sudo. Thus you must use USER root for any privileged task (e.g., updating and installing apps via apt).",
                "upvotes": 10,
                "answered_by": "ruasoliveira",
                "answered_at": "2019-01-29 15:09"
            }
        ]
    },
    {
        "title": "Installing Java in Docker image",
        "url": "https://stackoverflow.com/questions/31196567/installing-java-in-docker-image",
        "votes": "67",
        "views": "219k",
        "author": "Yana K.",
        "issued_at": "2015-07-03 00:11",
        "tags": [
            "dockerfile"
        ],
        "answers": [
            {
                "answer": "I was able to install OpenJDK 8 via the steps below (taken from here). My Dockerfile inherits from phusion/baseimage-docker, which is based on Ubuntu 16.04 LTS.\n# Install OpenJDK-8\nRUN apt-get update && \\\n    apt-get install -y openjdk-8-jdk && \\\n    apt-get install -y ant && \\\n    apt-get clean;\n    \n# Fix certificate issues\nRUN apt-get update && \\\n    apt-get install ca-certificates-java && \\\n    apt-get clean && \\\n    update-ca-certificates -f;\n\n# Setup JAVA_HOME -- useful for docker commandline\nENV JAVA_HOME /usr/lib/jvm/java-8-openjdk-amd64/\nRUN export JAVA_HOME\nTo install OpenJDK 7 instead, you may need to prepend\nadd-apt-repository ppa:openjdk-r/ppa\nsuch that the first step becomes\n# Install OpenJDK-7\nRUN add-apt-repository ppa:openjdk-r/ppa && \\\n    apt-get update && \\\n    apt-get install -y openjdk-7-jdk && \\\n    apt-get install -y ant && \\\n    apt-get clean;",
                "upvotes": 84,
                "answered_by": "laylaylom",
                "answered_at": "2017-05-18 21:39"
            },
            {
                "answer": "Here is how to install java 11 on any Debian/Debian slim based containers\nFROM python:3.7-slim-buster\n# Install JRE\nRUN mkdir -p /usr/share/man/man1 /usr/share/man/man2 && \\\n    apt-get update &&\\\n    apt-get install -y --no-install-recommends openjdk-11-jre && \\\n    apt-get install ca-certificates-java -y && \\\n    apt-get clean && \\\n    update-ca-certificates -f;\nENV JAVA_HOME /usr/lib/jvm/java-11-openjdk-amd64/\nCMD [\"sleep\", \"infinity\"]\nBuild the image, run and exec into the container.\ndocker build -t java11 .\ndocker run -d --name java-container java11\ndocker exec -it java-container /bin/bash\ncheck the version in the container\nroot@a9a0011f0ab6:/# java -version\nopenjdk version \"11.0.16\" 2022-07-19\nOpenJDK Runtime Environment (build 11.0.16+8-post-Debian-1deb10u1)\nOpenJDK 64-Bit Server VM (build 11.0.16+8-post-Debian-1deb10u1, mixed mode, sharing)\nIn Debian Slim based containers you might come across below error.\nerror creating symbolic link '/usr/share/man/man1/java.1.gz.dpkg-tmp': No such file or directory\nSetting up openjdk-11-jre-headless:amd64 (11.0.16+8-1~deb10u1) ...\n#11 66.48 update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/java to provide /usr/bin/java (java) in auto mode\n#11 66.48 update-alternatives: error: error creating symbolic link '/usr/share/man/man1/java.1.gz.dpkg-tmp': No such file or directory\n#11 66.48 dpkg: error processing package openjdk-11-jre-headless:amd64 (--configure):\n#11 66.48  installed openjdk-11-jre-headless:amd64 package post-installation script subprocess returned error exit status 2\nIn such scenarios simply include below line before installing java. This will create couple of directories in the container which is left out in slim variants to reduce the overall container size.\nmkdir -p /usr/share/man/man1 /usr/share/man/man2\nEDIT 1: Removed RUN export JAVA_HOME from the Dockerfile as its not required.",
                "upvotes": 3,
                "answered_by": "ns15",
                "answered_at": "2022-08-10 18:52"
            }
        ]
    },
    {
        "title": "Docker compose how to mount path from one to another container?",
        "url": "https://stackoverflow.com/questions/43559619/docker-compose-how-to-mount-path-from-one-to-another-container",
        "votes": "67",
        "views": "58k",
        "author": "dev.meghraj",
        "issued_at": "2017-04-22 13:10",
        "tags": [
            "docker",
            "docker-compose",
            "dockerfile"
        ],
        "answers": [
            {
                "answer": "What you want to do is use a volume, and then mount that volume into whatever containers you want it to appear in.\nCompletely within Docker\nYou can do this completely inside of Docker.\nHere is an example (stripped-down - your real file would have much more than this in it, of course).\nversion: '3'\nservices:\n  nginx:\n    volumes:\n      - asset-volume:/var/lib/assets\n  asset:\n    volumes:\n      - asset-volume:/var/lib/assets\n\nvolumes:\n  asset-volume:\nAt the bottom is a single volume defined, named \"asset-volume\".\nThen in each of your services, you tell Docker to mount that volume at a certain path. I show example paths inside the container, just adjust these to be whatever path you wish them to be in the container.\nThe volume is an independent entity not owned by any particular container. It is just mounted into each of them, and is shared. If one container modifies the contents, then they all see the changes.\nNote that if you prefer only one can make changes, you can always mount the volume as read-only in some services, by adding :ro to the end of the volume string.\nservices:\n  servicename:\n    volumes:\n      - asset-volume:/var/lib/assets:ro\nUsing a host directory\nAlternately you can use a directory on the host and mount that into the containers. This has the advantage of you being able to work directly on the files using your tools outside of Docker (such as your GUI text editor and other tools).\nIt's the same, except you don't define a volume in Docker, instead mounting the external directory.\nversion: '3'\nservices:\n  nginx:\n    volumes:\n      - ./assets:/var/lib/assets\n  asset:\n    volumes:\n      - ./assets:/var/lib/assets\nIn this example, the local directory \"assets\" is mounted into both containers using the relative path ./assets.\nUsing both depending on environment\nYou can also set it up for a different dev and production environment. Put everything in docker-compose.yml except the volume mounts. Then make two more files.\ndocker-compose.dev.yml\ndocker-compose.prod.yml\nIn these files put only the minimum config to define the volume mount. We'll mix this with the docker-compose.yml to get a final config.\nThen use this. It will use the config from docker-compose.yml, and use anything in the second file as an override or supplemental config.\ndocker-compose -f docker-compose.yml \\\n    -f docker-compose.dev.yml \\\n    up -d\nAnd for production, just use the prod file instead of the dev file.\nThe idea here is to keep most of the config in docker-compose.yml, and only the minimum set of differences in the alternative files.\nExample:\ndocker-compose.prod.yml\nversion: '3'\nservices:\n  nginx:\n    volumes:\n      - asset-volume:/var/lib/assets\ndocker-compose.dev.yml\nversion: '3'\nservices:\n  nginx:\n    volumes:\n      - ./assets:/var/lib/assets",
                "upvotes": 112,
                "answered_by": "Dan Lowe",
                "answered_at": "2017-04-22 13:56"
            }
        ]
    },
    {
        "title": "Differences Between Dockerfile Instructions in Shell and Exec Form",
        "url": "https://stackoverflow.com/questions/42805750/differences-between-dockerfile-instructions-in-shell-and-exec-form",
        "votes": "67",
        "views": "32k",
        "author": "Sheena",
        "issued_at": "2017-03-15 09:30",
        "tags": [
            "docker",
            "dockerfile"
        ],
        "answers": [
            {
                "answer": "There are two differences between the shell form and the exec form. According to the documentation, the exec form is the preferred form. These are the two differences:\nThe exec form is parsed as a JSON array, which means that you must use double-quotes (\u201c) around words not single-quotes (\u2018).\nUnlike the shell form, the exec form does not invoke a command shell. This means that normal shell processing does not happen. For example, CMD [ \"echo\", \"$HOME\" ] will not do variable substitution on $HOME. If you want shell processing then either use the shell form or execute a shell directly, for example: CMD [ \"sh\", \"-c\", \"echo $HOME\" ]. When using the exec form and executing a shell directly, as in the case for the shell form, it is the shell that is doing the environment variable expansion, not docker.\nSome additional subtleties here are:\nThe exec form makes it possible to avoid shell string munging, and to RUN commands using a base image that does not contain the specified shell executable.\nIn the shell form you can use a \\ (backslash) to continue a single RUN instruction onto the next line.\nThere is also a third form for CMD:\nCMD [\"param1\",\"param2\"] (as default parameters to ENTRYPOINT)\nAdditionally, the exec form is required for CMD if you are using it as parameters/arguments to ENTRYPOINT that are intended to be overwritten.",
                "upvotes": 72,
                "answered_by": "Matthew Schuchard",
                "answered_at": "2017-03-15 11:50"
            }
        ]
    },
    {
        "title": "Dockerfile replicate the host user UID and GID to the image",
        "url": "https://stackoverflow.com/questions/44683119/dockerfile-replicate-the-host-user-uid-and-gid-to-the-image",
        "votes": "66",
        "views": "109k",
        "author": "minghua",
        "issued_at": "2017-06-21 17:58",
        "tags": [
            "docker",
            "permissions",
            "dockerfile"
        ],
        "answers": [
            {
                "answer": "You can pass it as a build arg. Your Dockerfile can be static:\nFROM ubuntu:xenial-20170214\nARG UNAME=testuser\nARG UID=1000\nARG GID=1000\nRUN groupadd -g $GID -o $UNAME\nRUN useradd -m -u $UID -g $GID -o -s /bin/bash $UNAME\nUSER $UNAME\nCMD /bin/bash\nThen you'd pass the options on your build command:\ndocker build --build-arg UID=$(id -u) --build-arg GID=$(id -g) \\\n  -f bb.dockerfile -t testimg .\nNote that I've solved similar problems to this a different way, by running an entrypoint as root that looks a file/directory permissions of the host volume mount, and adjust the uid/gid of the users inside the container to match the volume uid/gid. After making that change, it drops access from the root user to the modified uid/gid user and runs the original command/entrypoint. The result is the image can be run unchanged on any developer machine. An example of this can be found in my jenkins-docker repo:\nhttps://github.com/sudo-bmitch/jenkins-docker",
                "upvotes": 120,
                "answered_by": "BMitch",
                "answered_at": "2017-06-21 18:05"
            }
        ]
    },
    {
        "title": "How to copy files from dockerfile to host?",
        "url": "https://stackoverflow.com/questions/33377022/how-to-copy-files-from-dockerfile-to-host",
        "votes": "65",
        "views": "60k",
        "author": "Allen",
        "issued_at": "2015-10-27 19:39",
        "tags": [
            "linux",
            "docker",
            "dockerfile"
        ],
        "answers": [
            {
                "answer": "This is now possible since Docker 19.03.0 in July 2019 introduced \"custom build outputs\". See the official docs about custom build outputs.\nTo enable custom build outputs from the build image into the host during the build process, you need to activate the BuildKit which is a newer recommended back-compatible way for the engine to do the build phase. See the official docs for enabling BuildKit.\nThis can be done in 2 ways:\nSet the environment variable DOCKER_BUILDKIT=1, or\nSet it in the docker engine by default by adding \"features\": { \"buildkit\": true } to the root of the config json.\nFrom the official docs about custom build outputs:\ncustom exporters allow you to export the build artifacts as files on the local filesystem instead of a Docker image, which can be useful for generating local binaries, code generation etc.\n...\nThe local exporter writes the resulting build files to a directory on the client side. The tar exporter is similar but writes the files as a single tarball (.tar).\nIf no type is specified, the value defaults to the output directory of the local exporter.\n...\nThe --output option exports all files from the target stage. A common pattern for exporting only specific files is to do multi-stage builds and to copy the desired files to a new scratch stage with COPY --from.\ne.g. an example Dockerfile\nFROM alpine:latest AS stage1\nWORKDIR /app\nRUN echo \"hello world\" > output.txt\n\nFROM scratch AS export-stage\nCOPY --from=stage1 /app/output.txt .\nRunning\nDOCKER_BUILDKIT=1 docker build --file Dockerfile --output out .\nThe tail of the output is:\n => [export-stage 1/1] COPY --from=stage1 /app/output.txt .\n0.0s\n => exporting to client\n0.1s\n => => copying files 45B\n0.1s\nThis produces a local file out/output.txt that was created by the RUN command.\n$ cat out/output.txt\nhello world\nAll files are output from the target stage\nThe --output option will export all files from the target stage. So using a non-scratch stage with COPY --from will cause extraneous files to be copied to the output. The recommendation is to use a scratch stage with COPY --from.\nWindows is not supported for now:\nhttps://docs.docker.com/build/buildkit/",
                "upvotes": 86,
                "answered_by": "Ben T",
                "answered_at": "2020-04-26 12:23"
            },
            {
                "answer": "Copying files \"from the Dockerfile\" to the host is not supported. The Dockerfile is just a recipe specifying how to build an image.\nWhen you build, you have the opportunity to copy files from host to the image you are building (with the COPY directive or ADD)\nYou can also copy files from a container (an image that has been docker run'd) to the host with docker cp (atually, the cp can copy from the host to the container as well)\nIf you want to get back to your host some files that might have been generated during the build (like for example calling a script that generates ssl), you can run a container, mounting a folder from your host and executing cp commands.\nSee for example this getcrt script.\ndocker run -u root --entrypoint=/bin/sh --rm -i -v ${HOME}/b2d/apache:/apache apache << COMMANDS\npwd\ncp crt /apache\ncp key /apache\necho Changing owner from \\$(id -u):\\$(id -g) to $(id -u):$(id -u)\nchown -R $(id -u):$(id -u) /apache/crt\nchown -R $(id -u):$(id -u) /apache/key\nCOMMANDS\nEverything between COMMANDS are commands executed on the container, including cp ones which are copying on the host ${HOME}/b2d/apache folder, mounted within the container as /apache with -v ${HOME}/b2d/apache:/apache.\nThat means each time you copy anything on /apache in the container, you are actually copying in ${HOME}/b2d/apache on the host!",
                "upvotes": 20,
                "answered_by": "VonC",
                "answered_at": "2015-10-27 20:16"
            },
            {
                "answer": "There is now a much easier way of doing that using Docker's plugin buildx, with its Local Exporter.\nIf you've followed Docker's tutorial for installing Docker Engine or Desktop, buildx should already be installed and activated as the default builder.\nIt exports everything from the root of the container to a local path on the host but you can use multi-stage build with the scratch Docker image to export only what you need:\n#Stage 1: Build\nFROM node:20-alpine AS builder\n\nWORKDIR /build\n\nCOPY . .\n\nRUN npm run build:doc\n\n#Stage 2: Copy files to scratch image\nFROM scratch AS export\n\nCOPY --from=builder /build/docs .\nThen all you need to do is build with --type=local, dest=<desired_path>:\ndocker buildx build --output type=local,dest=. .",
                "upvotes": 3,
                "answered_by": "Gabriel Amorim",
                "answered_at": "2024-03-08 19:37"
            }
        ]
    },
    {
        "title": "Docker-compose: deploying service in multiple hosts",
        "url": "https://stackoverflow.com/questions/40737389/docker-compose-deploying-service-in-multiple-hosts",
        "votes": "65",
        "views": "45k",
        "author": "Asier Gomez",
        "issued_at": "2016-11-22 08:51",
        "tags": [
            "docker",
            "docker-compose",
            "dockerfile",
            "docker-swarm"
        ],
        "answers": [
            {
                "answer": "With docker swarm mode, you can deploy a version 3 compose yml file using:\ndocker stack deploy -c docker-compose.yml $your_stack_name\nThe v3 syntax removes a few features that do not apply to swarm mode, like links and dependencies. You should also note that volumes are stored local to a node by default. Otherwise the v3 syntax is very similar to the v2 syntax you may already be using. See ether following for more details:\nhttps://docs.docker.com/compose/compose-file/\nhttps://docs.docker.com/engine/swarm/\n[ Original answer before v3 of the docker-compose.yml ]\nFor a single docker-compose.yml to deploy to multiple hosts, you need to use the standalone swarm (not the newer swarm mode, yet, this is rapidly changing). Spin up a swarm manager that has each host defined as members of its swarm, and then you can use constraints inside your docker-compose.yml to define which services run on which hosts.\nYou can also split up your docker-compose.yml into several files, one for each host, and then run multiple docker-compose up commands, with a different DOCKER_HOST value defined for each.\nIn both cases, you'll need to configure your docker installs to listen on the network, which should be done by configuring TLS on those sockets. This documentation describes what you need to do for that.",
                "upvotes": 23,
                "answered_by": "BMitch",
                "answered_at": "2016-11-22 12:39"
            }
        ]
    },
    {
        "title": "Docker COPY not updating files when rebuilding container",
        "url": "https://stackoverflow.com/questions/41498336/docker-copy-not-updating-files-when-rebuilding-container",
        "votes": "65",
        "views": "63k",
        "author": "Alex H",
        "issued_at": "2017-01-06 03:13",
        "tags": [
            "docker",
            "docker-compose",
            "dockerfile"
        ],
        "answers": [
            {
                "answer": "Just leaving this here for when I come back to this page in two weeks.\nYou may not want to use docker system prune -f in this block.\n    docker-compose down --rmi all -v \\\n    && docker-compose build --no-cache \\\n    && docker-compose -f docker-compose-staging.yml up -d --force-recreate",
                "upvotes": 7,
                "answered_by": "Grahame",
                "answered_at": "2020-04-17 02:03"
            }
        ]
    },
    {
        "title": "How to show the full (not truncated) \"CREATED BY\" commands in \"docker history\" output?",
        "url": "https://stackoverflow.com/questions/55737199/how-to-show-the-full-not-truncated-created-by-commands-in-docker-history-o",
        "votes": "64",
        "views": "42k",
        "author": "user674669",
        "issued_at": "2019-04-17 23:16",
        "tags": [
            "docker",
            "dockerfile"
        ],
        "answers": [
            {
                "answer": "Use the docker history --no-trunc option to show the full command.",
                "upvotes": 87,
                "answered_by": "Alassane Ndiaye",
                "answered_at": "2019-04-17 23:56"
            },
            {
                "answer": "I needed to see the full build history without truncated build steps in a tabular format for debugging purposes. The Docker Docs examples are often incomplete (or non-existent). The accepted answer by @Alassane_Hdiaye can work, but throws an error if used in the wrong place. After a couple tries, I got the GoLang template table formatter to produce the result I needed.\nThe Docker GoLang template --format option works with docker history, but must be entered AFTER --format, because it modifies --format, NOT docker history. The docker history command takes only 1 argument. The --format option changes the output of docker history, and --no-trunc modifies the --format option.\nThe command sequence must be\ndocker history <image-id> --format <GoLang template spec> --no-trunc\nThe --no-trunc option follows --format on the command line, because it modifies the --format option.\nSo for the fully history in a tabular format:\ndocker history <image-id> --format \"table{{.ID}}, {{.CreatedBy}}\" --no-trunc\nThe output is messy, because Dockerfile RUN commands can wrap many lines.\nSo pipe the result to a CSV file and open it in Excel (for lack of a better table viewer).\ndocker history <image-id> --format \"table{{.ID}}, {{.CreatedBy}}\" --no-trunc > image-id-history.csv\nSo now I can see complete build instructions in Excel without truncation.\nAs a side note, Docker history does not output RUN and other dockerfile build keywords, but these can be inferred. But at least full commands from the complete history are available.",
                "upvotes": 21,
                "answered_by": "Rich Lysakowski PhD",
                "answered_at": "2020-03-15 19:54"
            }
        ]
    },
    {
        "title": "Is Docker Compose suitable for production?",
        "url": "https://stackoverflow.com/questions/37521440/is-docker-compose-suitable-for-production",
        "votes": "64",
        "views": "29k",
        "author": "Lukasz Dynowski",
        "issued_at": "2016-05-30 08:51",
        "tags": [
            "docker",
            "docker-compose",
            "dockerfile"
        ],
        "answers": [
            {
                "answer": "Just to extend what @ChrisSainty already mentioned, compose is just an orchestration tool, you can use your own images built with your own Dockerfiles with your compose settings in a single host. But note that it is possible to compose against a swarm cluster as it exposes the same API as a single Docker host.\nIn my opinion it is an easy way to implement a microservice architecture using containers to tailor services with high efficient availability. In addition to that I recommend checking this official documentation on good practices on using compose in production environments.",
                "upvotes": 19,
                "answered_by": "Rog\u00e9rio Peixoto",
                "answered_at": "2016-06-12 23:15"
            }
        ]
    },
    {
        "title": "Cache Rust dependencies with Docker build",
        "url": "https://stackoverflow.com/questions/58473606/cache-rust-dependencies-with-docker-build",
        "votes": "63",
        "views": "31k",
        "author": "Arek C.",
        "issued_at": "2019-10-20 13:42",
        "tags": [
            "docker",
            "rust",
            "dockerfile",
            "actix-web"
        ],
        "answers": [
            {
                "answer": "Seems like you are not alone in your endeavor to cache rust dependencies via the docker build process. Here is a great article that helps you along the way.\nThe gist of it is you need a dummy.rs and your Cargo.toml first, then build it to cache the dependencies and then copy your application source later in order to not invalidate the cache with every build.\nDockerfile\nFROM rust\nWORKDIR /var/www/app\nCOPY dummy.rs .\nCOPY Cargo.toml .\nRUN sed -i 's#src/main.rs#dummy.rs#' Cargo.toml\nRUN cargo build --release\nRUN sed -i 's#dummy.rs#src/main.rs#' Cargo.toml\nCOPY . .\nRUN cargo build --release\nCMD [\"target/release/app\"]\nCMD application name \"app\" is based on what you have specified in your Cargo.toml for your binary.\ndummy.rs\nfn main() {}\nCargo.toml\n[package]\nname = \"app\"\nversion = \"0.1.0\"\nauthors = [\"...\"]\n[[bin]]\nname = \"app\"\npath = \"src/main.rs\"\n\n[dependencies]\nactix-web = \"1.0.0\"\nsrc/main.rs\nextern crate actix_web;\n\nuse actix_web::{web, App, HttpServer, Responder};\n\nfn index() -> impl Responder {\n    \"Hello world\"\n}\n\nfn main() -> std::io::Result<()> {\n    HttpServer::new(|| App::new().service(web::resource(\"/\").to(index)))\n        .bind(\"0.0.0.0:8080\")?\n        .run()\n}",
                "upvotes": 58,
                "answered_by": "ckaserer",
                "answered_at": "2019-10-20 15:44"
            },
            {
                "answer": "You can use cargo-chef (unrelated to the Chef DevOps tool) to leverage Docker layer caching using a multi-stage build.\nFROM rust as planner\nWORKDIR app\n# We only pay the installation cost once, \n# it will be cached from the second build onwards\nRUN cargo install cargo-chef \nCOPY . .\nRUN cargo chef prepare  --recipe-path recipe.json\n\nFROM rust as cacher\nWORKDIR app\nRUN cargo install cargo-chef\nCOPY --from=planner /app/recipe.json recipe.json\nRUN cargo chef cook --release --recipe-path recipe.json\n\nFROM rust as builder\nWORKDIR app\nCOPY . .\n# Copy over the cached dependencies\nCOPY --from=cacher /app/target target\nRUN cargo build --release --bin app\n\nFROM rust as runtime\nWORKDIR app\nCOPY --from=builder /app/target/release/app /usr/local/bin\nENTRYPOINT [\"./usr/local/bin/app\"]\nIt does not require Buildkit and works for both simple projects and workspaces. You can find more details in the release announcement.",
                "upvotes": 16,
                "answered_by": "LukeMathWalker",
                "answered_at": "2020-10-25 20:29"
            },
            {
                "answer": "While electronix384128 answer is excellent. I would like to expand on it by adding cache for .cargo/git which is needed for any dependency using git and by adding a multistage docker example.\nUsing rust-musl-builder and Docker Buildkit feature, which is now default in Docker Desktop 2.4. On other versions you may still need to enable it via: DOCKER_BUILDKIT=1 docker build .\nrusl-musl-builder's working directory is /home/rust/src\nTried setting uid/gid on --mount but failed to compile rust due to permission issue in target.\n# syntax=docker/dockerfile:1.2\nFROM ekidd/rust-musl-builder:stable AS builder\n\nCOPY . .\nRUN --mount=type=cache,target=/home/rust/.cargo/git \\\n    --mount=type=cache,target=/home/rust/.cargo/registry \\\n    --mount=type=cache,sharing=private,target=/home/rust/src/target \\\n    sudo chown -R rust: target /home/rust/.cargo && \\\n    cargo build --release && \\\n    # Copy executable out of the cache so it is available in the final image.\n    cp target/x86_64-unknown-linux-musl/release/my-executable ./my-executable\n\nFROM alpine\nCOPY --from=builder /home/rust/src/my-executable .\nUSER 1000\nCMD [\"./my-executable\"]",
                "upvotes": 10,
                "answered_by": "jetersen",
                "answered_at": "2020-09-30 15:38"
            }
        ]
    },
    {
        "title": "Docker parallel operations limit",
        "url": "https://stackoverflow.com/questions/43479614/docker-parallel-operations-limit",
        "votes": "63",
        "views": "37k",
        "author": "rmoh21",
        "issued_at": "2017-04-18 18:25",
        "tags": [
            "docker",
            "concurrency",
            "dockerfile",
            "docker-registry"
        ],
        "answers": [
            {
                "answer": "The options are set in the configuration file (Linux-based OS it is located in the path: /etc/docker/daemon.json and C:\\ProgramData\\docker\\config\\daemon.json on Windows)\nOpen /etc/docker/daemon.json (If doesn't exist, create it)\nAdd the values(for push/pulls) and set parallel operations limit\n{\n    \"max-concurrent-uploads\": 1,\n    \"max-concurrent-downloads\": 1\n}\nRestart daemon: sudo service docker restart",
                "upvotes": 97,
                "answered_by": "Daniel I. Cruz",
                "answered_at": "2018-12-24 16:19"
            },
            {
                "answer": "For anyone using Docker for Windows and WSL2: You can (and should) set the options on the Settings tab:",
                "upvotes": 6,
                "answered_by": "Emrah Kaya",
                "answered_at": "2021-05-20 14:27"
            }
        ]
    },
    {
        "title": "What is --from, as used in COPY command in Dockerfile?",
        "url": "https://stackoverflow.com/questions/66353510/what-is-from-as-used-in-copy-command-in-dockerfile",
        "votes": "62",
        "views": "104k",
        "author": "yogihosting",
        "issued_at": "2021-02-24 15:04",
        "tags": [
            "docker",
            "dockerfile"
        ],
        "answers": [
            {
                "answer": "This is a multi-stage build. This is used to keep the running docker container small while still be able to build/compile things needing a lot of dependencies.\nFor example a go application could be built by using:\nFROM golang:1.7.3 AS builder\nWORKDIR /go/src/github.com/alexellis/href-counter/\nRUN go get -d -v golang.org/x/net/html  \nCOPY app.go .\nRUN CGO_ENABLED=0 GOOS=linux go build -a -installsuffix cgo -o app .\n\nFROM alpine:latest  \nRUN apk --no-cache add ca-certificates\nWORKDIR /root/\nCOPY --from=builder /go/src/github.com/alexellis/href-counter/app .\nCMD [\"./app\"]  \nSo in the first part we need a complete go environment to compile our software. Notice the name for the first part and the alias builder\nFROM golang:1.7.3 AS builder\nIn the second part beginning from the second FROM we only need the compiled app and no other go dependencies anymore. So we can change the base image to using a much smaller alpine Linux. But the compiled files are still in our builder image and not part of the image we want to start. So we need to copy files from the builder image via\nCOPY --from=builder\nYou can have as many stages as you want. The last one is the one defining the image which will be the template for the docker container.\nYou can read more about it in the official documentation: https://docs.docker.com/develop/develop-images/multistage-build/",
                "upvotes": 94,
                "answered_by": "mszalbach",
                "answered_at": "2021-02-24 15:15"
            }
        ]
    },
    {
        "title": "How to push Docker containers managed by Docker-compose to Heroku?",
        "url": "https://stackoverflow.com/questions/46904060/how-to-push-docker-containers-managed-by-docker-compose-to-heroku",
        "votes": "62",
        "views": "45k",
        "author": "AspiringMat",
        "issued_at": "2017-10-24 06:58",
        "tags": [
            "docker",
            "heroku",
            "docker-compose",
            "dockerfile"
        ],
        "answers": [
            {
                "answer": "The more accurate heroku documentation for what you are looking to do is here: https://devcenter.heroku.com/articles/container-registry-and-runtime\nThe above will walk you through setting up the heroku container plugin and logging into the registry. You can even migrate an image to a Dockerfile with the following line in your dockerfile:\nFROM \"<insert Dockerfile tag here>\"\nTo easily set this up, you will name your Dockerfiles with different suffixes, such as Dockerfile.mongo, Dockerfile.node, Dockerfile.flask, and Dockerfile.javamvc. The suffix tells heroku the dyno name used for your web app. When you need to push all of your containers, you can do so with the following command, which will recursively build all dockerfiles as long as all of them have unique suffixes:\nheroku container:push --recursive\nAs Heroku doesn't read docker-compose files, any environment variable setup/port exposure/etc will need to be migrated to the Dockerfile. Also as I can't find how to do persistent storage/volume mounting with containers on Heroku, I would recommend using a Heroku add-on for your mongo database.\nOn Heroku, you will see your app running as one dyno per Dockerfile, with each dyno's name as the suffix of each Dockerfile.\nUPDATE:\nTravis brings up a good point. Make sure to have a CMD statement in your Dockerfile, otherwise heroku will throw an error.\nHeroku recently added a step to the process, you will need to run heroku container:release <your dyno name> for each dyno that you want to update.",
                "upvotes": 13,
                "answered_by": "eliotn",
                "answered_at": "2017-12-21 21:44"
            }
        ]
    },
    {
        "title": "What is \"/app\" working directory for a Dockerfile?",
        "url": "https://stackoverflow.com/questions/55108649/what-is-app-working-directory-for-a-dockerfile",
        "votes": "61",
        "views": "100k",
        "author": "Intrastellar Explorer",
        "issued_at": "2019-03-11 19:02",
        "tags": [
            "docker",
            "dockerfile"
        ],
        "answers": [
            {
                "answer": "There are two important directories when building a docker image:\nthe build context directory.\nthe WORKDIR directory.\nBuild context directory\nIt's the directory on the host machine where docker will get the files to build the image. It is passed to the docker build command as the last argument. (Instead of a PATH on the host machine it can be a URL). Simple example:\ndocker build -t myimage .\nHere the current dir (.) is the build context dir. In this case, docker build will use Dockerfile located in that dir. All files from that dir will be visible to docker build.\nThe build context dir is not necessarily where the Dockerfile is located. Dockerfile location defaults to current dir and is otherwise indicated by the -f otpion. Example:\ndocker build -t myimage -f ./rest-adapter/docker/Dockerfile ./rest-adapter\nHere build context dir is ./rest-adapter, a subdirectory of where you call docker build; the Dokerfile location is indicated by -f.\nWORKDIR\nIt's a directory inside your container image that can be set with the WORKDIR instruction in the Dockerfile. It is optional (default is /, but the parent image might have set it), but setting it is considered a good practice. Subsequent instructions in the Dockerfile, such as RUN, CMD and ENTRYPOINT will operate in this dir. As for COPY and ADD, they use both...\nCOPY and ADD use both dirs\nThese two commands have <src> and <dest>.\n<src> is relative to the build context directory.\n<dest> is relative to the WORKDIR directory.\nFor example, if your Dockerfile contains...\nWORKDIR /myapp\nCOPY . .\nADD data/mydata.csv /usr/share/mydata.csv\nthen...\nthe COPY command will copy the contents of your build context directory to the /myapp dir inside your docker image.\nthe ADD command will copy file data/mydata.csv under your build context dir to the /usr/share dir at the docker image filesystem root level (<dest> in this case starts with /, so it's an absolute path).",
                "upvotes": 136,
                "answered_by": "Paulo Merson",
                "answered_at": "2019-10-17 14:04"
            },
            {
                "answer": "WORKDIR is a good practice because you can set a directory as the main directory, then you can work on it using COPY, ENTRYPOINT, CMD commands, because them will execute pointing to this PATH.\nDocker documentation: https://docs.docker.com/engine/reference/builder/\nThe WORKDIR instruction sets the working directory for any RUN, CMD, ENTRYPOINT, COPY and ADD instructions that follow it in the Dockerfile. If the WORKDIR doesn\u2019t exist, it will be created even if it\u2019s not used in any subsequent Dockerfile instruction.\nThe WORKDIR instruction can be used multiple times in a Dockerfile. If a relative path is provided, it will be relative to the path of the previous WORKDIR instruction.\nDockerfile Example:\nFROM node:alpine\nWORKDIR '/app'\nCOPY ./package.json ./\nRUN npm install\nCOPY . .\nCMD [\"npm\", \"run\", \"start\"]\nA alpine node.js was created and the workdir is /app, then al files are copied them into /app\nFinally npm run start command is running into /app folder inside the container.\nYou should exec the following command in the case you have sh or bash tty:\ndocker exec -it <container-id> sh\nor\ndocker exec -it <container-id> bash\nAfter that you can do ls command and you will can see the WORKDIR folder.\nI hope it may help you",
                "upvotes": 26,
                "answered_by": "Hugo Lesta",
                "answered_at": "2019-03-11 19:30"
            },
            {
                "answer": "You need to declare a working directory and move your code into it, because your code has to live somewhere. Otherwise your code wouldn't be present and your app wouldn't run. Then when commands like RUN, CMD, ENTRYPOINT, COPY, and ADD are used, they are executed in the context of WORKDIR.\n/app is an arbitrary choice of working directory. You could use anything you like (foo, bar, or baz), but app is nice since it's self-descriptive and commonly used.",
                "upvotes": 12,
                "answered_by": "Gavin Miller",
                "answered_at": "2019-03-11 19:21"
            }
        ]
    },
    {
        "title": "docker-compose change name of main container",
        "url": "https://stackoverflow.com/questions/68357205/docker-compose-change-name-of-main-container",
        "votes": "61",
        "views": "107k",
        "author": "Michaelo",
        "issued_at": "2021-07-13 06:02",
        "tags": [
            "docker",
            "docker-compose",
            "dockerfile"
        ],
        "answers": [
            {
                "answer": "When refering to your main container, you are probably refering to the project name, which you could usually set via the -p flag. (See other answers)\nFor docker-compose, you can set the top level variable name to your desired project name.\ndocker-compose.yml file:\nversion: \"3.9\"\nname: my-project-name\nservices:\n  myService:\n    ...\nIf you are using Docker Desktop, make sure Use Docker Compose V2 is enabled there.",
                "upvotes": 82,
                "answered_by": "Tigerware",
                "answered_at": "2022-05-28 00:07"
            },
            {
                "answer": "I think that your docker compose file is right and to change the co you can use the containe_name instruction but I think you should run this command when you want to run your application :\ndocker-compose up --build",
                "upvotes": 3,
                "answered_by": "rassakra",
                "answered_at": "2021-07-13 06:59"
            }
        ]
    },
    {
        "title": "Run jar file in docker image",
        "url": "https://stackoverflow.com/questions/35061746/run-jar-file-in-docker-image",
        "votes": "61",
        "views": "172k",
        "author": "Svetoslav Angelov",
        "issued_at": "2016-01-28 12:39",
        "tags": [
            "docker",
            "dockerfile"
        ],
        "answers": [
            {
                "answer": "There is a difference between images and containers.\nImages will be built ONCE\nYou can start containers from Images\nIn your case:\nChange your image:\nFROM anapsix/alpine-java\nMAINTAINER myNAME \nCOPY testprj-1.0-SNAPSHOT.jar /home/testprj-1.0-SNAPSHOT.jar\nCMD [\"java\",\"-jar\",\"/home/testprj-1.0-SNAPSHOT.jar\"]\nBuild your image:\ndocker build -t imageName .\nNow invoke your program inside a container:\ndocker run --name myProgram imageName\nNow restart your program by restarting the container:\ndocker restart myProgram\nYour program changed? Rebuild the image!:\ndocker rmi imageName\ndocker build -t imageName .",
                "upvotes": 112,
                "answered_by": "blacklabelops",
                "answered_at": "2016-01-28 12:54"
            }
        ]
    },
    {
        "title": "Docker RabbitMQ persistency",
        "url": "https://stackoverflow.com/questions/41330514/docker-rabbitmq-persistency",
        "votes": "61",
        "views": "56k",
        "author": "Rinat Mukhamedgaliev",
        "issued_at": "2016-12-26 11:24",
        "tags": [
            "docker",
            "rabbitmq",
            "containers",
            "dockerfile"
        ],
        "answers": [
            {
                "answer": "TL;DR\nDidn't do too much digging on this, but it appears that the simplest way to do this is to change the hostname as Pedro mentions above.\n\nMORE INFO:\nUsing RABBITMQ_NODENAME\nIf you want to edit the RABBITMQ_NODENAME variable via Docker, it looks like you need to add a hostname as well since the Docker hostnames are generated as random hashes.\nIf you change the RABBITMQ_NODENAME var to something static like my-rabbit, RabbitMQ will throw something like an \"nxdomain not found\" error because it's looking for something like\nmy-rabbit@<docker_hostname_hash>. If you know the Docker hostname and can automate pulling it into your RABBITMQ_NODENAME value like so, my-rabbit@<docker_hostname_hash> I believe it would work.\nUPDATE\nI previously said,\nIf you know the Docker hostname and can automate pulling it into your RABBITMQ_NODENAME value like so, my-rabbit@<docker_hostname_hash> I believe it would work.\nThis would not work as described precisely because the default docker host name is randomly generated at launch, if it is not assigned explicitly. The hurdle would actually be to make sure you use the EXACT SAME <docker_hostname_hash> as your originating run so that the data directory gets picked up correctly. This would be a pain to implement dynamically/robustly. It would be easiest to use an explicit hostname as described below.\nThe alternative would be to set the hostname to a value you choose -- say, app-messaging -- AND ALSO set the RABBITMQ_NODENAME var to something like rabbit@app-messaging. This way you are controlling the full node name that will be used in the data directory.\nUsing Hostname\n(Recommended)\nThat said, unless you have a reason NOT to change the hostname, changing the hostname alone is the simplest way to ensure that your data will be mounted to and from the same point every time.\nI'm using the following Docker Compose file to successfully persist my setup between launches.\nversion: '3'\nservices:\n  rabbitmq:\n    hostname: 'mabbit'\n    image: \"${ARTIFACTORY}/rabbitmq:3-management\"\n    ports:\n      - \"15672:15672\"\n      - \"5672:5672\"\n    volumes:\n      - \"./data:/var/lib/rabbitmq/mnesia/\"\n    networks:\n      - rabbitmq\n\nnetworks:\n  rabbitmq:\n    driver: bridge\nThis creates a data directory next to my compose file and persists the RabbitMQ setup like so:\n./data/\n  rabbit@mabbit/\n  rabbit@mabbit-plugins-expand/\n  rabbit@mabbit.pid\n  rabbit@mabbit-feature_flags",
                "upvotes": 47,
                "answered_by": "wileymab",
                "answered_at": "2019-08-22 15:22"
            }
        ]
    },
    {
        "title": "How to install docker in docker container?",
        "url": "https://stackoverflow.com/questions/44451859/how-to-install-docker-in-docker-container",
        "votes": "61",
        "views": "134k",
        "author": "Jolly23",
        "issued_at": "2017-06-09 07:30",
        "tags": [
            "docker",
            "debian",
            "dockerfile"
        ],
        "answers": [
            {
                "answer": "I had a similar problem trying to install Docker inside a Bamboo Server image. To solve this:\nfirst remove the line: RUN docker run hello-world from your Dockerfile\nThe simplest way is to just expose the Docker socket, by bind-mounting it with the -v flag or mounting a volume using Docker Compose:\ndocker run -v /var/run/docker.sock:/var/run/docker.sock ...",
                "upvotes": 20,
                "answered_by": "Felipe Desiderati",
                "answered_at": "2019-02-04 02:22"
            }
        ]
    },
    {
        "title": "How to install Go in alpine linux",
        "url": "https://stackoverflow.com/questions/52056387/how-to-install-go-in-alpine-linux",
        "votes": "60",
        "views": "139k",
        "author": "Yogesh Jilhawar",
        "issued_at": "2018-08-28 11:06",
        "tags": [
            "docker",
            "go",
            "dockerfile",
            "tar",
            "alpine-linux"
        ],
        "answers": [
            {
                "answer": "I just copied it over using multi stage builds, seems to be ok so far\nFROM XXX\n \nCOPY --from=golang:1.13-alpine /usr/local/go/ /usr/local/go/\n \nENV PATH=\"/usr/local/go/bin:${PATH}\"",
                "upvotes": 104,
                "answered_by": "Chad Grant",
                "answered_at": "2019-12-26 08:43"
            },
            {
                "answer": "The following Dockerfile worked for me. Simpler and more abstract.\nFROM alpine:latest\n\nRUN apk add --no-cache git make musl-dev go\n\n# Configure Go\nENV GOROOT /usr/lib/go\nENV GOPATH /go\nENV PATH /go/bin:$PATH\n\nRUN mkdir -p ${GOPATH}/src ${GOPATH}/bin\n\n# Install Glide\nRUN go get -u github.com/Masterminds/glide/...\n\nWORKDIR $GOPATH\n\nCMD [\"make\"]\nsource: https://raw.githubusercontent.com/mickep76/alpine-golang/master/Dockerfile",
                "upvotes": 45,
                "answered_by": "Bevilaqua",
                "answered_at": "2018-11-21 03:49"
            },
            {
                "answer": "Thanks BMitch.\nI compiled the go source code and performed the below steps inside alpine image container.\necho \"installing go version 1.10.3...\" \napk add --no-cache --virtual .build-deps bash gcc musl-dev openssl go \n\n# download go tar \nwget -O go.tgz https://dl.google.com/go/go1.10.3.src.tar.gz \ntar -C /usr/local -xzf go.tgz \ncd /usr/local/go/src/ \n\n# compile code\n./make.bash \nexport PATH=\"/usr/local/go/bin:$PATH\"\nexport GOPATH=/opt/go/ \nexport PATH=$PATH:$GOPATH/bin \napk del .build-deps \ngo version",
                "upvotes": 29,
                "answered_by": "Yogesh Jilhawar",
                "answered_at": "2018-09-24 07:21"
            }
        ]
    },
    {
        "title": "How do I reduce a python (docker) image size using a multi-stage build?",
        "url": "https://stackoverflow.com/questions/48543834/how-do-i-reduce-a-python-docker-image-size-using-a-multi-stage-build",
        "votes": "60",
        "views": "51k",
        "author": "gCoh",
        "issued_at": "2018-01-31 13:54",
        "tags": [
            "python",
            "docker",
            "dockerfile",
            "docker-multi-stage-build"
        ],
        "answers": [
            {
                "answer": "ok so my solution is using wheel, it lets us compile on first image, create wheel files for all dependencies and install them in the second image, without installing the compilers\nFROM python:2.7-alpine as base\n\nRUN mkdir /svc\nCOPY . /svc\nWORKDIR /svc\n\nRUN apk add --update \\\n    postgresql-dev \\\n    gcc \\\n    musl-dev \\\n    linux-headers\n\nRUN pip install wheel && pip wheel . --wheel-dir=/svc/wheels\n\nFROM python:2.7-alpine\n\nCOPY --from=base /svc /svc\n\nWORKDIR /svc\n\nRUN pip install --no-index --find-links=/svc/wheels -r requirements.txt\nYou can see my answer regarding this in the following blog post\nhttps://www.blogfoobar.com/post/2018/02/10/python-and-docker-multistage-build",
                "upvotes": 57,
                "answered_by": "gCoh",
                "answered_at": "2018-02-09 21:42"
            }
        ]
    },
    {
        "title": "Build postgres docker container with initial schema",
        "url": "https://stackoverflow.com/questions/34751814/build-postgres-docker-container-with-initial-schema",
        "votes": "59",
        "views": "89k",
        "author": "Jono",
        "issued_at": "2016-01-12 19:04",
        "tags": [
            "postgresql",
            "docker",
            "dockerfile",
            "docker-compose"
        ],
        "answers": [
            {
                "answer": "As said in the comments, @Thomasleveil answer is great and simple if your schema recreation is fast. But in my case it's slow, and I wanted to use docker volumes, so here is what I did\nFirst use docker image as in @Thomasleveil answer to create a container with postgres with all the schema initialization\nDockerfile:\nFROM postgres\nWORKDIR /docker-entrypoint-initdb.d\nADD psql_dump.sql /docker-entrypoint-initdb.d\nEXPOSE 5432\nthen run it and create new local dir which contains the postgres data after its populated from the \u201cpsql_dump.sql\u201d file: docker cp mypg:/var/lib/postgresql/data ./postgres-data\nCopy the data to a temp data folder, and start a new postgres docker-compose container whose volume is at the new temp data folder:\nstartPostgres.sh:\nrm -r ./temp-postgres-data/data\nmkdir -p ./temp-postgres-data/data\ncp -r ./postgres-data/data ./temp-postgres-data/\ndocker-compose -p mini-postgres-project up\nand the docker-compose.yml file is:\nversion: '3'\nservices:\n  postgres:\n    container_name: mini-postgres\n    image: postgres:9.5\n    ports:\n    - \"5432:5432\"\n    volumes:\n      - ./temp-postgres-data/data:/var/lib/postgresql/data\nNow you can run steps #1 and #2 on a new machine or if your psql_dump.sql changes. And each time you want a new clean (but already initialized) db, you can only run startPostgres.sh from step #3. And it still uses docker volumes.",
                "upvotes": 9,
                "answered_by": "yishaiz",
                "answered_at": "2017-11-27 14:01"
            }
        ]
    },
    {
        "title": "How to unset \"ENV\" in dockerfile?",
        "url": "https://stackoverflow.com/questions/55789409/how-to-unset-env-in-dockerfile",
        "votes": "59",
        "views": "89k",
        "author": "Kim",
        "issued_at": "2019-04-22 05:10",
        "tags": [
            "docker",
            "dockerfile"
        ],
        "answers": [
            {
                "answer": "Short-answer:\nTry to avoid unnecessary environment variables, so you don't need to unset them.\nIn case you have to unset for a command you can do the following:\nRUN unset http_proxy https_proxy no_proxy \\\n    && execute_your_command_here  \nIn case you have to unset for the built image you can do the following:\nFROM ubuntu_with_http_proxy\n\nENV http_proxy= \\\n    https_proxy= \\\n    no_proxy=\nOnce environment variables are set using the ENV instruction we can't really unset them as it is detailed:\nEach ENV line creates a new intermediate layer, just like RUN commands. This means that even if you unset the environment variable in a future layer, it still persists in this layer and its value can be dumped.\nSee: Best practices for writing Dockerfiles\nDetails:\nI prefer to define http_proxy as an argument during build like the following:\nFROM ubuntu:20.04\n\nARG http_proxy=http://host.docker.internal:3128 \nARG https_proxy=http://host.docker.internal:3128 \nARG no_proxy=.your.domain,localhost,127.0.0.1,.docker.internal\nOn corporate proxy we need authentication anyways, so we need to configure local proxy server listening on 127.0.0.1:3128 witch is accessible over host.docker.internal:3128 from containers. This way it also works on docker desktop if we connect to corporate network over VPN (with local/home network blocked).\nSetting no_proxy is also important to avoid flooding the proxy server.\nSee the following article for more details on no_proxy related topics:\nCan we standardize NO_PROXY?\nSometimes it is also good to read the related documentation:\nENV\nARG\nIn case we need to configure those environment variables we can use the following command:\nduring build (link):\ndocker build ... --build-arg http_proxy='http://alternative.proxy:3128/' ...\nduring runs (link):\ndocker run ... -env http_proxy='http://alternative.proxy:3128/' ...\nAlso note that we don't even need to define proxy related arguments since those are already predefine according to the following section:\nDockerfile reference - Predefined ARGs",
                "upvotes": 9,
                "answered_by": "minus one",
                "answered_at": "2021-02-25 12:18"
            }
        ]
    },
    {
        "title": "How to start apache2 automatically in a ubuntu docker container?",
        "url": "https://stackoverflow.com/questions/44376852/how-to-start-apache2-automatically-in-a-ubuntu-docker-container",
        "votes": "58",
        "views": "131k",
        "author": "Rayhan Muktader",
        "issued_at": "2017-06-05 20:07",
        "tags": [
            "apache",
            "ubuntu",
            "docker",
            "dockerfile"
        ],
        "answers": [
            {
                "answer": "The issue is here: CMD service apache2 start When you execute this command process apache2 will be detached from the shell. But Docker works only while main process is alive.\nThe solution is to run Apache in the foreground. Dockerfile must look like this: (only last line changed).\nFROM ubuntu\n\n# File Author / Maintainer\nMAINTAINER rmuktader\n\n# Update the repository sources list\nRUN apt-get update\n\n# Install and run apache\nRUN apt-get install -y apache2 && apt-get clean\n\n#ENTRYPOINT [\"/usr/sbin/apache2\", \"-k\", \"start\"]\n\n\n#ENV APACHE_RUN_USER www-data\n#ENV APACHE_RUN_GROUP www-data\n#ENV APACHE_LOG_DIR /var/log/apache2\n\nEXPOSE 80\nCMD apachectl -D FOREGROUND",
                "upvotes": 108,
                "answered_by": "Bukharov Sergey",
                "answered_at": "2017-06-05 20:56"
            },
            {
                "answer": "My project was slightly different where I installed a bunch of other stuff, but the apache start portion matched above. Once I built this image and used it, my server started fine.\nFROM ubuntu:latest\n\n#install all the tools you might want to use in your container\nRUN apt-get update\nRUN apt-get install curl -y\nRUN apt-get install vim -y\n#the following ARG turns off the questions normally asked for location and timezone for Apache\nARG DEBIAN_FRONTEND=noninteractive\nRUN apt-get install apache2 -y\n\n#change working directory to root of apache webhost\nWORKDIR var/www/html\n\n#copy your files, if you want to copy all use COPY . .\nCOPY index.html index.html\n\n#now start the server\nCMD [\"apachectl\", \"-D\", \"FOREGROUND\"]",
                "upvotes": 8,
                "answered_by": "jbailey",
                "answered_at": "2020-09-11 17:06"
            }
        ]
    },
    {
        "title": "Docker-compose volume mount before run",
        "url": "https://stackoverflow.com/questions/37089162/docker-compose-volume-mount-before-run",
        "votes": "58",
        "views": "51k",
        "author": "DanielM",
        "issued_at": "2016-05-07 13:29",
        "tags": [
            "docker",
            "docker-compose",
            "dockerfile"
        ],
        "answers": [
            {
                "answer": "You can't mount host folders or volumes during a Docker build. Allowing that would compromise build repeatability. The only way to access local data during a Docker build is the build context, which is everything in the PATH or URL you passed to the build command. Note that the Dockerfile needs to exist somewhere in context. See https://docs.docker.com/engine/reference/commandline/build/ for more details.",
                "upvotes": 24,
                "answered_by": "Erik Dannenberg",
                "answered_at": "2016-05-07 14:35"
            }
        ]
    },
    {
        "title": "How to enable/disable buildkit in docker?",
        "url": "https://stackoverflow.com/questions/66839443/how-to-enable-disable-buildkit-in-docker",
        "votes": "58",
        "views": "128k",
        "author": "Code Eagle",
        "issued_at": "2021-03-28 07:59",
        "tags": [
            "docker",
            "windows-10",
            "dockerfile"
        ],
        "answers": [
            {
                "answer": "You must adjust the Docker Engine's daemon settings, stored in the daemon.json, and restart the engine. As @Zeitounator suggests, you should be able to temporarily disable the buildkit with DOCKER_BUILDKIT=0 docker build .. Docker CLI will parse that environment variable and should honor it as that checking is done here in the docker/cli source code.\nTo adjust the Docker daemon's buildkit settings, you can follow the instructions below.\nFrom these docs. Partially on the command line, you can do that this way in Powershell:\nOpen the file, on the command line the easiest way to do this is:\nnotepad \"$env:USERPROFILE\\.docker\\daemon.json\"\nChange the value of \"buildkit\" to false so it looks like this:\n{\n  \"registry-mirrors\": [],\n  \"insecure-registries\": [],\n  \"debug\": true,\n  \"experimental\": false,\n  \"features\": {\n    \"buildkit\": false\n  }\n}\nRestart the Docker service:\nRestart-Service *docker*\nAlternatively, on Docker Desktop for Windows app:\nOpen the Dashboard > Settings:\nSelect Docker Engine and edit the json \"features\" field to read false if it's not already:",
                "upvotes": 55,
                "answered_by": "cam",
                "answered_at": "2021-03-28 08:35"
            },
            {
                "answer": "On Windows 10 x64 and Docker Desktop:\n# Start DOS cmd prompt\nset DOCKER_BUILDKIT=0\ndocker ...\nNo need to edit any files, restart service, etc. It just works as Docker checks the this environment variable when it runs.\nCase study\nDocker gave an error on the command line when I was logged into a remote PC using MobaXTerm on the command line. This fixed it.",
                "upvotes": 8,
                "answered_by": "Contango",
                "answered_at": "2022-05-12 10:56"
            },
            {
                "answer": "Open your Docker desktop application\nFind settings\nFind Docker Engine\nYou can find below json:\n{\n  \"builder\": {\n    \"gc\": {\n      \"defaultKeepStorage\": \"20GB\",\n      \"enabled\": true\n    }\n  },\n  \"experimental\": false,\n  \"features\": {\n    \"buildkit\": false\n  }\n}\nDefault value of \"buildkit\":true -> change to false\nRestart the engine",
                "upvotes": 4,
                "answered_by": "Reshnu chandran",
                "answered_at": "2022-10-28 23:13"
            }
        ]
    },
    {
        "title": "Multiline comments in Dockerfiles",
        "url": "https://stackoverflow.com/questions/55165989/multiline-comments-in-dockerfiles",
        "votes": "58",
        "views": "42k",
        "author": "user1460043",
        "issued_at": "2019-03-14 15:10",
        "tags": [
            "docker",
            "dockerfile"
        ],
        "answers": [
            {
                "answer": "There is no mentioning of multiline comments in Docker documentation\nI also paste here the relevant part for simplicity:\nDocker treats lines that begin with # as a comment, unless the line is a valid parser directive.\nA # marker anywhere else in a line is treated as an argument.\nThis allows statements like:\n# Comment  \nRUN echo 'we are running some # of cool things'  \nLine continuation characters are not supported in comments.\nOn the other hand you can achieve the requested result easily with any modern IDE / Text Editor.\nThis is an example using Sublime Text (Select text and then control + /).\nYou can achieve the same result with VsCode, Notepad++, JetBrains products (IntelliJ, PyCharm, PHPStorm etc.) and almost 100% of the IDEs / Text Editors I know and use.",
                "upvotes": 20,
                "answered_by": "Pitto",
                "answered_at": "2019-03-14 15:23"
            }
        ]
    },
    {
        "title": "Dockerfile strategies for Git",
        "url": "https://stackoverflow.com/questions/33682123/dockerfile-strategies-for-git",
        "votes": "58",
        "views": "67k",
        "author": "Hemerson Varela",
        "issued_at": "2015-11-12 21:51",
        "tags": [
            "git",
            "docker",
            "git-clone",
            "dockerfile"
        ],
        "answers": [
            {
                "answer": "From Ryan Baumann's blog post \u201cGit strategies for Docker\u201d\nThere are different strategies for getting your Git source code into a Docker build. Many of these have different ways of interacting with Docker\u2019s caching mechanisms, and may be more or less appropriately suited to your project and how you intend to use Docker.\nRUN git clone\nIf you\u2019re like me, this is the approach that first springs to mind when you see the commands available to you in a Dockerfile. The trouble with this is that it can interact in several unintuitive ways with Docker\u2019s build caching mechanisms. For example, if you make an update to your git repository, and then re-run the docker build which has a RUN git clone command, you may or may not get the new commit(s) depending on if the preceding Dockerfile commands have invalidated the cache.\nOne way to get around this is to use docker build --no-cache, but then if there are any time-intensive commands preceding the clone they\u2019ll have to run again too.\nAnother issue is that you (or someone you\u2019ve distributed your Dockerfile to) may unexpectedly come back to a broken build later on when the upstream git repository updates.\nA two-birds-one-stone approach to this while still using RUN git clone is to put it on one line1 with a specific revision checkout, e.g.:\nRUN git clone https://github.com/example/example.git && cd example && git checkout 0123abcdef\nThen updating the revision to check out in the Dockerfile will invalidate the cache at that line and cause the clone/checkout to run.\nOne possible drawback to this approach in general is that you have to have git installed in your container.\nRUN curl or ADD a tag/commit tarball URL\nThis avoids having to have git installed in your container environment, and can benefit from being explicit about when the cache will break (i.e. if the tag/revision is part of the URL, that URL change will bust the cache). Note that if you use the Dockerfile ADD command to copy from a remote URL, the file will be downloaded every time you run the build, and the HTTP Last-Modified header will also be used to invalidate the cache.\nYou can see this approach used in the golang Dockerfile.\nGit submodules inside Dockerfile repository\nIf you keep your Dockerfile and Docker build in a separate repository from your source code, or your Docker build requires multiple source repositories, using git submodules (or git subtrees) in this repository may be a valid way to get your source repos into your build context. This avoids some concerns with Docker caching and upstream updating, as you lock the upstream revision in your submodule/subtree specification. Updating them will break your Docker cache as it changes the build context.\nNote that this only gets the files into your Docker build context, you still need to use ADD commands in your Dockerfile to copy those paths to where you expect them in the container.\nYou can see this approach used in the here\nDockerfile inside git repository\nHere, you just have your Dockerfile in the same git repository alongside the code you want to build/test/deploy, so it automatically gets sent as part of the build context, so you can e.g. ADD . /project to copy the context into the container. The advantage to this is that you can test changes without having to potentially commit/push them to get them into a test docker build; the disadvantage is that every time you modify any files in your working directory it will invalidate the cache at the ADD command. Sending the build context for a large source/data directory can also be time-consuming. So if you use this approach, you may also want to make judicious use of the .dockerignore file, including doing things like ignoring everything in your .gitignore and possibly the .git directory itself.\nVolume mapping\nIf you\u2019re using Docker to set up a dev/test environment that you want to share among a wide variety of source repos on your host machine, mounting a host directory as a data volume may be a viable strategy. This gives you the ability to specify which directories you want to include at docker run-time, and avoids concerns about docker build caching, but none of this will be shared among other users of your Dockerfile or container image.",
                "upvotes": 39,
                "answered_by": "Hemerson Varela",
                "answered_at": "2015-11-30 16:17"
            },
            {
                "answer": "You have generally two approaches:\nreferencing a vault where you get your secret data necessary to access what you need to put in your image (here, your ssh keys to access your private repo)\nUpdate 2018: see \"How to keep your container secrets secure\", which includes:\nUse volume mounts to pass secrets to a container at runtime\nHave a plan for rotating secrets\nMake sure your secrets are encrypted\nor a squashing technique (not recommended, see comment)\nFor the second approach, see \"Pulling Git into a Docker image without leaving SSH keys behind\"\nAdd the private key to the Dockerfile\nAdd it to the ssh-agent\nRun the commands that require SSH authentication\nRemove the private key\nDockerfile:\nADD ~/.ssh/mykey /tmp/  \nRUN ssh-agent /tmp  \n# RUN bundle install or similar command\nRUN rm /tmp/mykey  \nLet\u2019s build the image now:\n$ docker build -t original .\nSquash the layers:\ndocker save original | sudo docker-squash -t squashed | docker load",
                "upvotes": 10,
                "answered_by": "VonC",
                "answered_at": "2015-11-12 21:54"
            },
            {
                "answer": "ADD\nADD https://github.com/youraccount/myscript.git#main .\nThis will copy the whole repo into your container. Here's a working example below:\nFROM node:latest\nWORKDIR /usr/src/app\nADD https://github.com/youraccount/myscript.git#main .\nCOPY package*.json ./\nRUN npm install\nCOPY . .\nEXPOSE 8080\nYour .dockerignore file in your repo determines which folders/files are filtered out / ignored. ADD can grab a single file as well.\nIf it's a private repo, the documentation says:\nAdding a private git repository\nTo add a private repo via SSH, create a Dockerfile with the following form:\n# syntax=docker/dockerfile:1-labs\nFROM alpine\nADD git@git.example.com:foo/bar.git /bar",
                "upvotes": 1,
                "answered_by": "Ronnie Smith",
                "answered_at": "2023-04-30 21:12"
            }
        ]
    },
    {
        "title": "How to install packages with miniconda in Dockerfile?",
        "url": "https://stackoverflow.com/questions/58269375/how-to-install-packages-with-miniconda-in-dockerfile",
        "votes": "57",
        "views": "75k",
        "author": "maciek",
        "issued_at": "2019-10-07 12:16",
        "tags": [
            "python",
            "docker",
            "anaconda",
            "dockerfile",
            "miniconda"
        ],
        "answers": [
            {
                "answer": "This will work using ARG and ENV:\nFROM ubuntu:18.04\n\nENV PATH=\"/root/miniconda3/bin:${PATH}\"\nARG PATH=\"/root/miniconda3/bin:${PATH}\"\n\n# Install wget to fetch Miniconda\nRUN apt-get update && \\\n    apt-get install -y wget && \\\n    apt-get clean && \\\n    rm -rf /var/lib/apt/lists/*\n\n# Install Miniconda on x86 or ARM platforms\nRUN arch=$(uname -m) && \\\n    if [ \"$arch\" = \"x86_64\" ]; then \\\n    MINICONDA_URL=\"https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh\"; \\\n    elif [ \"$arch\" = \"aarch64\" ]; then \\\n    MINICONDA_URL=\"https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-aarch64.sh\"; \\\n    else \\\n    echo \"Unsupported architecture: $arch\"; \\\n    exit 1; \\\n    fi && \\\n    wget $MINICONDA_URL -O miniconda.sh && \\\n    mkdir -p /root/.conda && \\\n    bash miniconda.sh -b -p /root/miniconda3 && \\\n    rm -f miniconda.sh\n\nRUN conda --version",
                "upvotes": 109,
                "answered_by": "LinPy",
                "answered_at": "2019-10-07 12:33"
            }
        ]
    },
    {
        "title": "`docker pull` returns `denied: access forbidden` from private gitlab registry",
        "url": "https://stackoverflow.com/questions/46418652/docker-pull-returns-denied-access-forbidden-from-private-gitlab-registry",
        "votes": "57",
        "views": "129k",
        "author": "Zeinab Abbasimazar",
        "issued_at": "2017-09-26 05:46",
        "tags": [
            "docker",
            "docker-compose",
            "gitlab",
            "dockerfile"
        ],
        "answers": [
            {
                "answer": "If this is an authenticated registry, then you need to run docker login <registryurl> on the machine where you are building this.\nThis only needs to be done once per host. The command then caches the auth in a file\n$ cat ~/.docker/config.json\n{\n    \"auths\": {\n        \"https://index.docker.io/v1/\": {\n            \"auth\": \"......=\"\n        }\n    }\n}",
                "upvotes": 80,
                "answered_by": "Tarun Lalwani",
                "answered_at": "2017-09-26 09:06"
            },
            {
                "answer": "A login did not fix the problem for me. This may be specific to Mac, but just in case here is the Git issue\nMy comment on it:\nAlso experiencing this issue.\nDockerfile:\nFROM <insert_private_registry>/test-image:latest\nCLI\nBoth commands fail without a login to the private registry (expected)\n    $ docker-compose up\n    Building app\n    Step 1/2 : FROM <insert_private_registry>/test-image:latest\n    ERROR: Service 'app' failed to build: Get https://<insert_private_registry>/v2/test-image/manifests/latest: denied: access forbidden\n\n    $ docker pull <insert_private_registry>/test-image:latest\n    Error response from daemon: Get https://<insert_private_registry>/test-image/manifests/latest: denied: access forbidden\nAfter logging in, a docker pull ... works while the docker-compose up fails to pull the image:\n    $ docker login <insert_private_registry>\n    Username: <insert>\n    Password: <insert>\n    Login Succeeded\n\n    $ docker-compose up\n    Building app\n    Step 1/2 : FROM <insert_private_registry>/test-image:latest\n    ERROR: Service 'app' failed to build: Get https://<insert_private_registry>/v2/test-image/manifests/latest: denied: access forbidden\n\n    $ docker pull <insert_private_registry>/test-image:latest\n    latest: Pulling from <insert_private_image_path>/test-image\n    ...\n    Status: Downloaded newer image for <insert_private_registry>/test-image:latest\nCurrent Solution\nOur current workaround is to explicitly pull the image prior to running the docker-compose containers:\n    docker pull <insert_private_registry>/test-image:latest\n    latest: Pulling from <insert_private_image_path>/test-image\n    ...\n    Status: Downloaded newer image for <insert_private_registry>/test-image:latest\n\n    $ docker-compose up\n    Building app\n    Step 1/2 : FROM <insert_private_registry>/test-image:latest\n    ...",
                "upvotes": 12,
                "answered_by": "Isaiah",
                "answered_at": "2019-09-12 21:50"
            },
            {
                "answer": "I notice your URL scheme uses the http protocol - Docker needs to be configured to allow insecure registries.\nCreate or modify your daemon.json (required in one of the following locations):\nLinux: /etc/docker/\nWindows: C:\\ProgramData\\Docker\\config\\\nWith the contents:\n{\n    \"insecure-registries\" : [ \"my.private.gitlab.registry:port\" ]\n}\nThen restart Docker (not just the terminal session) and try again.\nOnce you've logged in with:\ndocker login my.private.gitlab.registry:port\nAs per tarun-lalwani's answer, this should then add the auth into the config, for future use (docker pull's etc.).",
                "upvotes": 6,
                "answered_by": "Michael",
                "answered_at": "2017-09-26 09:22"
            }
        ]
    },
    {
        "title": "How to build a docker container for a Java application",
        "url": "https://stackoverflow.com/questions/31696439/how-to-build-a-docker-container-for-a-java-application",
        "votes": "56",
        "views": "74k",
        "author": "Tobias Kremer",
        "issued_at": "2015-07-29 09:28",
        "tags": [
            "java",
            "maven",
            "gradle",
            "docker",
            "dockerfile"
        ],
        "answers": [
            {
                "answer": "The docker registry hub has a Maven image that can be used to create java containers.\nUsing this approach the build machine does not need to have either Java or Maven pre-installed, Docker controls the entire build process.\nExample\n\u251c\u2500\u2500 Dockerfile\n\u251c\u2500\u2500 pom.xml\n\u2514\u2500\u2500 src\n    \u251c\u2500\u2500 main\n    \u2502   \u251c\u2500\u2500 java\n    \u2502   \u2502   \u2514\u2500\u2500 org\n    \u2502   \u2502       \u2514\u2500\u2500 demo\n    \u2502   \u2502           \u2514\u2500\u2500 App.java\n    \u2502   \u2514\u2500\u2500 resources\n    \u2502       \u2514\u2500\u2500 log4j.properties\n    \u2514\u2500\u2500 test\n        \u2514\u2500\u2500 java\n            \u2514\u2500\u2500 org\n                \u2514\u2500\u2500 demo\n                    \u2514\u2500\u2500 AppTest.java\nImage is built as follows:\ndocker build -t my-maven .\nAnd run as follows:\n$ docker run -it --rm my-maven\n0    [main] INFO  org.demo.App  - hello world\nDockerfile\nFROM maven:3.3-jdk-8-onbuild\nCMD [\"java\",\"-jar\",\"/usr/src/app/target/demo-1.0-SNAPSHOT-jar-with-dependencies.jar\"]\nUpdate\nIf you wanted to optimize your image to exclude the source you could create a Dockerfile that only includes the built jar:\nFROM java:8\nADD target/demo-1.0-SNAPSHOT-jar-with-dependencies.jar /opt/demo/demo-1.0-SNAPSHOT-jar-with-dependencies.jar\nCMD [\"java\",\"-jar\",\"/opt/demo/demo-1.0-SNAPSHOT-jar-with-dependencies.jar\"]\nAnd build the image in two steps:\ndocker run -it --rm -w /opt/maven \\\n   -v $PWD:/opt/maven \\\n   -v $HOME/.m2:/root/.m2 \\\n   maven:3.3-jdk-8 \\\n   mvn clean install\n\ndocker build -t my-app .\n__\nUpdate (2017-07-27)\nDocker now has a multi-stage build capability. This enables Docker to build an image containing the build tools but only the runtime dependencies.\nThe following example demonstrates this concept, note how the jar is copied from target directory of the first build phase\nFROM maven:3.3-jdk-8-onbuild \n\nFROM java:8\nCOPY --from=0 /usr/src/app/target/demo-1.0-SNAPSHOT.jar /opt/demo.jar\nCMD [\"java\",\"-jar\",\"/opt/demo.jar\"]",
                "upvotes": 38,
                "answered_by": "Mark O'Connor",
                "answered_at": "2015-07-29 20:11"
            },
            {
                "answer": "Structure of java aplication\nDemo\n\u2514\u2500\u2500 src\n|    \u251c\u2500\u2500 main\n|    \u2502   \u251c\u2500\u2500 java\n|    \u2502   \u2502   \u2514\u2500\u2500 org\n|    \u2502   \u2502       \u2514\u2500\u2500 demo\n|    \u2502   \u2502           \u2514\u2500\u2500 App.java\n|    \u2502   \u2514\u2500\u2500 resources\n|    \u2502       \u2514\u2500\u2500 application.properties\n|    \u2514\u2500\u2500 test\n|         \u2514\u2500\u2500 java\n|               \u2514\u2500\u2500 org\n|                   \u2514\u2500\u2500 demo\n|                         \u2514\u2500\u2500 App.java  \n\u251c\u2500\u2500\u2500\u2500 Dockerfile\n\u251c\u2500\u2500\u2500\u2500 pom.xml\nContent of Dockerfile\nFROM java:8\nEXPOSE 8080\nADD /target/demo.jar demo.jar\nENTRYPOINT [\"java\",\"-jar\",\"demo.jar\"]\nCommands to build and run image\nGo to the directory of project.Lets say D:/Demo\n$ cd D/demo\n$ mvn clean install\n$ docker build demo .\n$ docker run -p 8080:8080 -t demo\nCheck that container is running or not\n$ docker ps\nThe output will be\nCONTAINER ID        IMAGE               COMMAND                CREATED             STATUS              PORTS                    NAMES\n55c11a464f5a        demo1               \"java -jar demo.jar\"   21 seconds ago      Up About a minute   0.0.0.0:8080->8080/tcp   cranky_mayer",
                "upvotes": 6,
                "answered_by": "Riddhi Gohil",
                "answered_at": "2016-04-01 12:25"
            }
        ]
    },
    {
        "title": "Docker build: failed to fetch oauth token for openjdk?",
        "url": "https://stackoverflow.com/questions/65361083/docker-build-failed-to-fetch-oauth-token-for-openjdk",
        "votes": "55",
        "views": "108k",
        "author": "Kespoco",
        "issued_at": "2020-12-18 17:07",
        "tags": [
            "docker",
            "dockerfile"
        ],
        "answers": [
            {
                "answer": "It looks like you have BuildKit enabled in your docker configuration. BuildKit can cause these type of problems. Please try it again with BuildKit disabled.\nIn Linux, using environment variables:\nexport DOCKER_BUILDKIT=0\nexport COMPOSE_DOCKER_CLI_BUILD=0\nIn Windows and macOS, start the Docker Desktop application, go to Settings, select Docker Engine and look for the existing entry:\n\"buildkit\": true\nChange this entry to disable buildkit:\n\"buildkit\": false\nThen click on Apply & Restart and try it again.",
                "upvotes": 136,
                "answered_by": "McPringle",
                "answered_at": "2020-12-18 18:31"
            },
            {
                "answer": "I have also faced this problem after updating to a new Docker version on my Mac. However I have solved the problem after logging in again from the terminal.\nThe command was:\ndocker login\nAfter that I had to provide username and password for Docker Hub. The problem was fixed.\nNote that if you use a custom registry, you have to run docker login <registry> instead.",
                "upvotes": 52,
                "answered_by": "Zakaria",
                "answered_at": "2021-09-19 05:46"
            },
            {
                "answer": "If you are facing this issue after the subscription changes to docker on August 31st 2021, then it means you need to sign in to docker hub to perform the operations.\nEither use docker login from the docker desktop app or use the docker login command from terminal.\nIf you have not created a docker account before then you can sign up for a personal (free) plan here - https://hub.docker.com/ or use the docker account which your organisation has given you (if you have one).\nChanges to the subscription was mentioned here - https://docs.docker.com/subscription/",
                "upvotes": 6,
                "answered_by": "giri-sh",
                "answered_at": "2021-09-05 06:41"
            }
        ]
    },
    {
        "title": "Error: \"error creating aufs mount to\" when building dockerfile",
        "url": "https://stackoverflow.com/questions/30984569/error-error-creating-aufs-mount-to-when-building-dockerfile",
        "votes": "52",
        "views": "54k",
        "author": "lars",
        "issued_at": "2015-06-22 15:58",
        "tags": [
            "docker",
            "dockerfile"
        ],
        "answers": [
            {
                "answer": "I had some unresolved errors after removing /var/lib/docker/aufs, which a couple extra steps cleared up.\nTo add to @benwalther answer, since I lack the reputation to comment:\n# Cleaning up through docker avoids these errors\n#   ERROR: Service 'master' failed to build:\n#     open /var/lib/docker/aufs/layers/<container_id>: no such file or directory\n#   ERROR: Service 'master' failed to build: failed to register layer:\n#     open /var/lib/docker/aufs/layers/<container_id>: no such file or directory\ndocker rm -f $(docker ps -a -q)\ndocker rmi -f $(docker images -a -q)\n\n# As per @BenWalther's answer above\nsudo service docker stop\nsudo rm -rf /var/lib/docker/aufs\n\n# Removing the linkgraph.db fixed this error:\n#   Conflict. The name \"/jenkins_data_1\" is already in use by container <container_id>.\n#   You have to remove (or rename) that container to be able to reuse that name.\nsudo rm -f /var/lib/docker/linkgraph.db\n\n\nsudo service docker start",
                "upvotes": 61,
                "answered_by": "AManOfScience",
                "answered_at": "2016-02-25 14:43"
            },
            {
                "answer": "I have removed /var/lib/docker/aufs/diff and got the same problem:\nerror creating aufs mount to /var/lib/docker/aufs/mnt/blah-blah-init: invalid argument\nIt solved by running the following commands:\ndocker stop $(docker ps -a -q);\ndocker rm $(docker ps -a -q);\ndocker rmi -f $(docker images -a -q)",
                "upvotes": 15,
                "answered_by": "Art",
                "answered_at": "2017-08-25 06:38"
            }
        ]
    },
    {
        "title": "What is the most light weight base image I can use to build a Dockerfile?",
        "url": "https://stackoverflow.com/questions/42964839/what-is-the-most-light-weight-base-image-i-can-use-to-build-a-dockerfile",
        "votes": "52",
        "views": "74k",
        "author": "Jayabalan Bala",
        "issued_at": "2017-03-22 23:57",
        "tags": [
            "docker",
            "dockerfile",
            "docker-build"
        ],
        "answers": [
            {
                "answer": "This really depends on your requirements:\nFROM scratch: if you are able to statically compile your application and don't need any other binaries (libraries, shells, or any other command period), then you can use the completely empty \"scratch\". You'll see this used as the starting point for the other base images, and it's also found in a lot of pre-compiled Go commands.\nDistroless: these images are built for a handful of use cases, and ship without a package manager or even shell (excluding their developer images). If you fit in their specific use case, these can be very small, but like with scratch images, difficult to debug.\nBusybox: I consider this less of a base image and more of a convenient utility container. You get a lot of common commands in a very small size. Busybox is a single binary with various commands linked to it, and that binary implements each of the commands depending on the CLI. What you don't get is the general package manager to easily install other components.\nAlpine: This is a minimal distribution, based on busybox, but with the apk package manager. The small size comes at a cost, things like glibc are not included, preferring the musl libc implementation instead. You will find that many of the official images are based on Alpine, so inside of the container ecosystem, this is a very popular option.\nDebian, Ubuntu, and CentOS: These are less of the lightweight base images. But what they lose with size they gain with a large collection of packages you can pull from and lots of people that are testing, fixing bugs, and contributing to things upstream. They also come with a collection of libraries that some applications may expect to be preinstalled.\nWhile that last option is a bit larger, keep in mind that base images should only be pushed over the wire and stored on disk once. After that, unless you change them, any images built on top of them only need to send the manifest that references layers in that base image and the docker engine will see that it already has those layers downloaded. And with the union fs, those layers never need to be copied even if you run 100 containers all pointing back to that image, they each use the same read-only layer on disk for all the image layers and write their changes to the their container specific RW layer.\nIf you find yourself installing a set of common tools on many of your images, the better option is to build your own base image, extending an upstream base image with your common tools. That way those tools only get packaged into a layer once and reused by multiple images.",
                "upvotes": 88,
                "answered_by": "BMitch",
                "answered_at": "2017-03-23 00:28"
            }
        ]
    },
    {
        "title": "/bin/sh: 1: sudo: not found when running dockerfile",
        "url": "https://stackoverflow.com/questions/56249601/bin-sh-1-sudo-not-found-when-running-dockerfile",
        "votes": "52",
        "views": "76k",
        "author": "mspms",
        "issued_at": "2019-05-22 05:02",
        "tags": [
            "docker",
            "dockerfile"
        ],
        "answers": [
            {
                "answer": "by default docker container runs as root user\nremove the sudo from Dockerfile and run again.",
                "upvotes": 56,
                "answered_by": "Abhishek D K",
                "answered_at": "2019-05-22 05:11"
            }
        ]
    },
    {
        "title": "Can I run an intermediate layer of a Docker image?",
        "url": "https://stackoverflow.com/questions/42602731/can-i-run-an-intermediate-layer-of-a-docker-image",
        "votes": "52",
        "views": "26k",
        "author": "achabahe",
        "issued_at": "2017-03-04 23:48",
        "tags": [
            "docker",
            "dockerfile"
        ],
        "answers": [
            {
                "answer": "Well, you can run an associated image if the final image was built locally on the same host machine. Each layer would have an intermediate image associated with it. You can show these layers and their associated images by running docker history <image-id>\nImages that are pulled from a registry won't have that. In that case, docker history will show the value <missing> to indicate that there are no images associated with these layers. Hence there is nothing to run.\nThe digests on the output of docker pull are digests of the layers not intermediate images and can't be run.\nHere is a typical scenario:\nYou build an image on a host: An intermediate image will be created for each layer (this is done for docker build cache)\nYou push the image to a registry: Only the final image is pushed (which references all the layers), but none of the intermediate images are pushed.\nYou pull the image on another host: Only the final image is available now on that other Docker host.\nYou can read more about layers at explaining-docker-image-ids.",
                "upvotes": 10,
                "answered_by": "Ahmad Abdelghany",
                "answered_at": "2017-06-08 13:02"
            },
            {
                "answer": "Yes, this is possible, but if you are using Docker 1.10 or later, only with self build images. E.g., docker history ruby will output <missing> for all but the topmost layer, because they doesn't have a tag.\nIf you build it yourself they will have a tag and you can start them like normal.",
                "upvotes": 3,
                "answered_by": "Julian",
                "answered_at": "2017-03-05 00:29"
            }
        ]
    },
    {
        "title": "Dockerfile and docker-compose not updating with new instructions",
        "url": "https://stackoverflow.com/questions/35231362/dockerfile-and-docker-compose-not-updating-with-new-instructions",
        "votes": "51",
        "views": "49k",
        "author": "Leon",
        "issued_at": "2016-02-05 18:46",
        "tags": [
            "docker",
            "containers",
            "dockerfile",
            "docker-compose"
        ],
        "answers": [
            {
                "answer": "I had the same issue and a one liner that does it for me is :\ndocker-compose up --build --remove-orphans --force-recreate\n--build does the biggest part of the job and triggers the build.\n--remove-orphans is useful if you have changed the name of one of your services. Otherwise, you might have a warning leftover telling you about the old, now wrongly named service dangling around.\n--force-recreate is a little drastic but will force the recreation of the containers.\nReference: https://docs.docker.com/compose/reference/up/\nWarning I could do this on my project because I was toying around with really small container images. Recreating everything, everytime, could take significant time depending on your situation.",
                "upvotes": 27,
                "answered_by": "Philippe Boulanger",
                "answered_at": "2021-05-04 13:42"
            }
        ]
    },
    {
        "title": "Multiple images, one Dockerfile",
        "url": "https://stackoverflow.com/questions/49754286/multiple-images-one-dockerfile",
        "votes": "51",
        "views": "72k",
        "author": "John",
        "issued_at": "2018-04-10 12:54",
        "tags": [
            "docker",
            "dockerfile"
        ],
        "answers": [
            {
                "answer": "You can use a docker-compose file using the target option:\nversion: '3.4'\nservices:\n  img1:\n    build:\n      context: .\n      target: img1\n  img2:\n    build:\n      context: .\n      target: img2\nusing your Dockerfile with the following content:\nFROM alpine as img1\nCOPY file1.txt .\n\nFROM alpine as img2\nCOPY file2.txt .",
                "upvotes": 56,
                "answered_by": "Sebastian Brosch",
                "answered_at": "2018-04-10 13:31"
            }
        ]
    },
    {
        "title": "Syntax highlighting for Dockerfile in Sublime Text?",
        "url": "https://stackoverflow.com/questions/39042988/syntax-highlighting-for-dockerfile-in-sublime-text",
        "votes": "51",
        "views": "19k",
        "author": "Ben Harrison",
        "issued_at": "2016-08-19 15:39",
        "tags": [
            "docker",
            "syntax-highlighting",
            "sublimetext",
            "dockerfile"
        ],
        "answers": [
            {
                "answer": "Of course you can, by installing this package from Package Control:\nDockerfile Syntax Highlighting, https://packagecontrol.io/packages/Dockerfile%20Syntax%20Highlighting",
                "upvotes": 42,
                "answered_by": "alebianco",
                "answered_at": "2016-08-19 16:15"
            },
            {
                "answer": "4 Steps\nPress command + shift + p to open the 'Command Palette'.\nSearch: package control: install package (you may have to install it first if you haven't yet)\nThen search: dockerfile syntax highlighting\nClose and re-open your Dockferfile, it will be syntax highlighted.",
                "upvotes": 11,
                "answered_by": "stevec",
                "answered_at": "2022-04-05 01:24"
            },
            {
                "answer": "For Mac, Dockerfile extension for sublime Text editor:-\nSteps:-\nPress cmd(\u2318)+shift+p and type \"add repo\".\nPaste in the GitHub address for this package: https://github.com/jaytaylor/Dockerfile.sublime-syntax\nPress cmd(\u2318)+shift+p and type \"install\".\nEnter \"Dockerfile.sublime-syntax\" and press enter.",
                "upvotes": 0,
                "answered_by": "Fateh Singh",
                "answered_at": "2022-09-25 12:18"
            }
        ]
    },
    {
        "title": "Cannot \"pip install cryptography\" in Docker Alpine Linux 3.3 with OpenSSL 1.0.2g and Python 2.7",
        "url": "https://stackoverflow.com/questions/35736598/cannot-pip-install-cryptography-in-docker-alpine-linux-3-3-with-openssl-1-0-2g",
        "votes": "50",
        "views": "51k",
        "author": "Daniel F",
        "issued_at": "2016-03-02 01:03",
        "tags": [
            "python",
            "linux",
            "openssl",
            "dockerfile",
            "alpine-linux"
        ],
        "answers": [
            {
                "answer": "For those who are still experiencing problems installing cryptography==2.1.4 in Alpine 3.7 like this:\nwriting manifest file 'src/cryptography.egg-info/SOURCES.txt'\nrunning build_ext\ngenerating cffi module 'build/temp.linux-x86_64-2.7/_padding.c'\ncreating build/temp.linux-x86_64-2.7\ngenerating cffi module 'build/temp.linux-x86_64-2.7/_constant_time.c'\ngenerating cffi module 'build/temp.linux-x86_64-2.7/_openssl.c'\nbuilding '_openssl' extension\ncreating build/temp.linux-x86_64-2.7/build\ncreating build/temp.linux-x86_64-2.7/build/temp.linux-x86_64-2.7\ngcc -fno-strict-aliasing -Os -fomit-frame-pointer -g -DNDEBUG -Os -fomit-frame-pointer -g -DTHREAD_STACK_SIZE=0x100000 -fPIC -I/usr/include/python2.7 -c build/temp.linux-x86_64-2.7/_openssl.c -o build/temp.linux-x86_64-2.7/build/temp.linux-x86_64-2.7/_openssl.o -Wconversion -Wno-error=sign-conversion\nbuild/temp.linux-x86_64-2.7/_openssl.c:493:30: fatal error: openssl/opensslv.h: No such file or directory\n #include <openssl/opensslv.h>\n                              ^\ncompilation terminated.\nerror: command 'gcc' failed with exit status 1\nSolution\nInstall these dependencies in the Alpine container:\n$ apk add --no-cache libressl-dev musl-dev libffi-dev\nTo install these dependencies using a Dockerfile:\nRUN apk add --no-cache \\\n        libressl-dev \\\n        musl-dev \\\n        libffi-dev && \\\n    pip install --no-cache-dir cryptography==2.1.4 && \\\n    apk del \\\n        libressl-dev \\\n        musl-dev \\\n        libffi-dev\nReference\nInstallation instructions for cryptography on Alpine can be found here:\nhttps://cryptography.io/en/latest/installation/#building-cryptography-on-linux\nA version from the time of writing is available on github\nHere is the relevant portion:\nBuilding cryptography on Linux\n[skipping over the part for non-Alpine Linux] \u2026\n$ pip install cryptography\nIf you are on Alpine or just want to compile it yourself then cryptography requires a compiler, headers for Python (if you're not using pypy), and headers for the OpenSSL and libffi libraries available on your system.\nAlpine\nReplace python3-dev with python-dev if you're using Python 2.\n$ sudo apk add gcc musl-dev python3-dev libffi-dev openssl-dev\nIf you get an error with openssl-dev you may have to use libressl-dev.",
                "upvotes": 80,
                "answered_by": "Manoj Kasyap",
                "answered_at": "2018-11-30 17:43"
            },
            {
                "answer": "If it fails because of Rust version, then following is recommended in cryptography's docs:\nThe Rust available by default in Alpine < 3.12 is older than the \nminimum supported version. See the Rust installation instructions\n for information about installing a newer Rust.\n$ sudo apk add gcc musl-dev python3-dev libffi-dev openssl-dev cargo\nin my case, python3.8-alpine, adding cargo resolved.",
                "upvotes": 14,
                "answered_by": "muon",
                "answered_at": "2021-04-30 18:52"
            }
        ]
    },
    {
        "title": "Why is ARG in a DOCKERFILE not recommended for passing secrets?",
        "url": "https://stackoverflow.com/questions/33621242/why-is-arg-in-a-dockerfile-not-recommended-for-passing-secrets",
        "votes": "50",
        "views": "51k",
        "author": "Roger Lam",
        "issued_at": "2015-11-10 01:14",
        "tags": [
            "docker",
            "credentials",
            "dockerfile"
        ],
        "answers": [
            {
                "answer": "Update August 2018:\nYou now have docker build --secret id=mysecret,src=/secret/file.\nSee \"safe way to use build-time argument in Docker\".\nUpdate January 2017:\nDocker (swarm) 1.13 has docker secret.\nHowever, as commented by Steve Hoffman (bacoboy):\n[...]The secret command only helps swarm users is not a more general solution (like they did with attaching persistent volumes).\nHow you manage your secrets (what they are and who has access to them) is very system dependent and depends on which bits of paid and/or OSS you cobble together to make your \"platform\".\nWith Docker the company moving into providing a platform, I'm not surprised that their first implementation is swarm based just as Hashicorp is integrating Vault into Atlas -- it makes sense.\nReally how the secrets are passed falls outside the space of docker run.\nAWS does this kind of thing with roles and policies to grant/deny permissions plus an SDK.\nChef does it using encrypted databags and crypto \"bootstrapping\" to auth.\nK8S has their own version of what just got released in 1.13.\nI'm sure mesos will add a similar implementation in time.\nThese implementations seem to fall into 2 camps.\npass the secret via volume mount that the \"platform\" provides or (chef/docker secret/k8s\npass credentials to talk to an external service to get things at boot (iam/credstash/etc)\nOriginal answer: Nov. 2015\nThis was introduced in commit 54240f8 (docker 1.9, Nov 2015), from PR 15182,\nThe build environment is prepended to the intermediate continer's command string for aiding cache lookups.\nIt also helps with build traceability. But this also makes the feature less secure from point of view of passing build time secrets.\nissue 13490 reiterates:\nBuild-time environment variables: The build-time environment variables were not designed to handle secrets. By lack of other options, people are planning to use them for this. To prevent giving the impression that they are suitable for secrets, it's been decided to deliberately not encrypt those variables in the process.\nAs mentioned in 9176 comments:\nenv variables are the wrong way to pass secrets around. We shouldn't be trying to reinvent the wheel and provide a crippled security distribution mechanism right out of the box.\nWhen you store your secret keys in the environment, you are prone to accidentally expose them -- exactly what we want to avoid:\nGiven that the environment is implicitly available to the process, it's incredibly hard, if not impossible, to track access and how the contents get exposed\nIt is incredibly common having applications grabbing the whole environment and print it out, since it can be useful for debugging, or even send it as part of an error report. So many secrets get leaked to PagerDuty that they have a well-greased internal process to scrub them from their infrastructure.\nEnvironment variables are passed down to child processes, which allows unintended access and breaks the principle of least privilege. Imagine that as part of your application you call to third-party tool to perform some action, all of a sudden that third-party tool has access to your environment, and god knows what it will do with it.\nIt is very common for applications that crash to store the environment variables in log-files for later debugging. This means secrets in plain-text on disk.\nPutting secrets in env variables quickly turns into tribal knowledge. New engineers don't know they are there, and are not aware they should be careful when handling environment variables (filtering them to sub-processes, etc).\nOverall, secrets in env variables break the principle of least surprise, are a bad practice and will lead to the eventual leak of secrets.",
                "upvotes": 49,
                "answered_by": "VonC",
                "answered_at": "2015-11-10 05:40"
            },
            {
                "answer": "The simple reason is that the value of the secret is visible to anyone with the image by simply running history on it.\nTake this sample docker file:\nFROM alpine\n\nARG secret\n\nRUN echo \"${secret}\"\n(Nice and simple, just to illustrate how you might use a secret.)\nthen we build it $ docker build --build-arg secret=S3CR3T - < Dockerfile\nSending build context to Docker daemon 2.048 kB\nStep 1 : FROM alpine\n ---> 13e1761bf172\nStep 2 : ARG secret\n ---> Running in 695b7a931445\n ---> 5414c15a1cb6\nRemoving intermediate container 695b7a931445\nStep 3 : RUN echo \"${secret}\"\n ---> Running in c90cf0d1414b\ns3cr3t\n ---> f2bcff49ac09\nRemoving intermediate container c90cf0d1414b\nSuccessfully built f2bcff49ac09\nAnd an example of how to get the \"secret\" back out (look for |1 secret= on the first line):\n$ docker history f2bcff49ac09\nIMAGE               CREATED             CREATED BY                                      SIZE                COMMENT\nf2bcff49ac09        8 seconds ago       |1 secret=S3CR3T /bin/sh -c echo \"${secret}\"    0 B\n5414c15a1cb6        8 seconds ago       /bin/sh -c #(nop) ARG secret                    0 B\n13e1761bf172        6 months ago        /bin/sh -c #(nop) ADD file:614a9122187935fccf   4.797 MB\nThis is the case if you have built the image locally or pulled it from a registry.\nIf your goal is to keep the build-time secret out of the running container then using ARG does help you - consider this:\n$ docker run --rm -ti f2bcff49ac09 sh\n/ # env\nHOSTNAME=7bc772fd0f56\nSHLVL=1\nHOME=/root\nTERM=xterm\nPATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\nPWD=/\n$ # Note no secret in the above output",
                "upvotes": 23,
                "answered_by": "Ash Berlin-Taylor",
                "answered_at": "2016-11-23 10:35"
            }
        ]
    },
    {
        "title": "How can I install lxml in docker",
        "url": "https://stackoverflow.com/questions/35931579/how-can-i-install-lxml-in-docker",
        "votes": "49",
        "views": "41k",
        "author": "thiiiiiking",
        "issued_at": "2016-03-11 03:19",
        "tags": [
            "python",
            "docker",
            "lxml",
            "dockerfile"
        ],
        "answers": [
            {
                "answer": "I added RUN apk add --update --no-cache g++ gcc libxslt-dev before RUN pip install -r requirements.txt and it worked.",
                "upvotes": 83,
                "answered_by": "Cintia Sestelo",
                "answered_at": "2017-01-27 21:03"
            }
        ]
    },
    {
        "title": "SIGTERM not received by java process using 'docker stop' and the official java image",
        "url": "https://stackoverflow.com/questions/31836498/sigterm-not-received-by-java-process-using-docker-stop-and-the-official-java-i",
        "votes": "49",
        "views": "17k",
        "author": "Sunil Kumar",
        "issued_at": "2015-08-05 15:18",
        "tags": [
            "java",
            "docker",
            "dropwizard",
            "dockerfile"
        ],
        "answers": [
            {
                "answer": "Assuming you launch a Java service by defining the following in your Dockerfile:\nCMD java -jar ...\nWhen you now enter the container and list the processes e.g. by docker exec -it <containerName> ps AHf (I did not try that with the java but with the ubuntu image) you see that your Java process is not the root process (not the process with PID 1) but a child process of a /bin/sh process:\nUID        PID  PPID  C STIME TTY          TIME CMD\nroot         1     0  0 18:27 ?        00:00:00 /bin/sh -c java -jar ...\nroot         8     1  0 18:27 ?        00:00:00   java -jar ...\nSo basically you have a Linux shell that is the main process with PID 1 which has a child process (Java) with PID 8.\nTo get signal handling working properly you should avoid those shell parent process. That can be done by using the builtin shell command exec. That will make the child process taking over the parent process. So at the end the former parent process does not exist any more. And the child process becomes the process with the PID 1. Try the following in your Dockerfile:\nCMD exec java -jar ...\nThe process listing then should show something like:\nUID        PID  PPID  C STIME TTY          TIME CMD\nroot         1     0  0 18:30 ?        00:00:00 java -jar ...\nNow you only have that one process with PID 1. Generally a good practice is to have docker containers only contain one process - the one with PID 1 (or if you really need more processes then you should use e.g. supervisord as PID 1 which itself takes care of signal handling for its child processes).\nWith that setup the SIGTERM will be treated directly by the Java process. There is no shell process any more in between which could break signal handling.\nEDIT:\nThe same exec effect could be achieved by using a different CMD syntax that does it implicitly (thanks to Andy for his comment):\nCMD [\"java\", \"-jar\", \"...\"]",
                "upvotes": 75,
                "answered_by": "Henrik Sachse",
                "answered_at": "2015-08-05 18:38"
            }
        ]
    },
    {
        "title": "Difference between VOLUME declaration in Dockerfile and -v as docker run parameter",
        "url": "https://stackoverflow.com/questions/40163036/difference-between-volume-declaration-in-dockerfile-and-v-as-docker-run-paramet",
        "votes": "49",
        "views": "27k",
        "author": "Mohammed Noureldin",
        "issued_at": "2016-10-20 19:34",
        "tags": [
            "docker",
            "dockerfile"
        ],
        "answers": [
            {
                "answer": "The -v parameter and VOLUME keyword are almost the same. You can use -v to have the same behavior as VOLUME.\ndocker run -v /data\nSame as\nVOLUME /data\nBut also -v have more uses, one of them is where map to the volume:\ndocker run -v data:/data # Named volumes\ndocker run -v /var/data:/data # Host mounted volumes, this is what you refer to -v use, but as you can see there are more uses,\nSo the question is: what is the use of VOLUME in a Dockerfile?\nThe container filesystem is made of layers so writing there, is slower and limited (because the fixed number of layers) than the plain filesystem.\nYou declare VOLUME in your Dockerfile to denote where your container will write application data. For example a database container, its data will go in a volume regardless what you put in your docker run.\nIf you create a docker container for JBoss and you want to use fast filesystem access with libaio yo need to declare the data directory as a VOLUME or JBoss will crash on startup.\nIn summary VOLUME declares a volume regardless what you do in docker run. In fact in docker run you cannot undo a VOLUME declaration made in Dockerfile.\nRegards",
                "upvotes": 46,
                "answered_by": "Carlos Rafael Ramirez",
                "answered_at": "2016-10-20 20:16"
            },
            {
                "answer": "In a nutshell\nThe VOLUME [PATH] instruction inside a Dockerfile is equivalent to\n$ docker run -v $(docker volume create):[PATH] [IMAGE_NAME]\nDetailed explanation\nThe container filesystem is made of layers so writing there is slower and limited (because the fixed number of layers) than the plain filesystem.\nUsing volumes in Docker is primarily less a matter of speed than a matter of data persistance independet from a container's life cycle. Mounting volumes from a faster disk will obviously improve performance, but Docker's default behavior for VOLUME is to create a named volume on the host system with little to no speed improvements compared to the container's writable layer.\n-v parameter is for me clear, it simply exposes a directory from the host to the container and vice versa\nWhile this is partly true, -v can also be used to mount named volumes into your Docker container instead of a directory. This little detail is important in order to understand what VOLUME does. An example:\n$ docker volume create my_volume\n$ docker run -v my_volume:[PATH] [IMAGE_NAME]\nHere a volume named my_volume was created. It behaves as would expect from a 'normal' mount. Any changes to [PATH] inside the container will be persisted in this volume. The difference is that Docker manages the volume's location, so that you don't need to worry (it is /var/lib/docker/volumes/my_volume/_data in case you're interested). Why would you want this? You could have a test database. While you don't need direct access to the files, you might want to save the current state to mount it into other database containers.\nThe VOLUME [PATH] instruction basically saves the above instructions into the image's metainformation. So everytime you start a container from this image, Docker knows that you want to persist [PATH] in a volume and takes care of that.",
                "upvotes": 17,
                "answered_by": "stepf",
                "answered_at": "2016-10-21 10:18"
            }
        ]
    },
    {
        "title": "Error in anyjson setup command: use_2to3 is invalid",
        "url": "https://stackoverflow.com/questions/72414481/error-in-anyjson-setup-command-use-2to3-is-invalid",
        "votes": "48",
        "views": "56k",
        "author": "Richard McCormick",
        "issued_at": "2022-05-28 09:29",
        "tags": [
            "python",
            "python-3.x",
            "docker",
            "dockerfile"
        ],
        "answers": [
            {
                "answer": "If keep getting the error after downgrading setuptools, make sure to have installed wheel:\npip install \"setuptools<58.0.0\" wheel\nUPDATE (2023-11-02): anyjson was removed thanks to Brian Peterson and Aaron recommendation.",
                "upvotes": 9,
                "answered_by": "iDevPy",
                "answered_at": "2023-05-30 15:57"
            },
            {
                "answer": "Installing a version of setuptools lower than 58.0.0 solved the issue.\npip install \"setuptools<58.0.0\" \nIt happens because Use_2to3 has been removed from setuptools >=58.0.0 versions",
                "upvotes": 6,
                "answered_by": "gigi",
                "answered_at": "2022-11-01 07:27"
            }
        ]
    },
    {
        "title": "How to build Docker Images with Dockerfile behind HTTP_PROXY by Jenkins?",
        "url": "https://stackoverflow.com/questions/27749193/how-to-build-docker-images-with-dockerfile-behind-http-proxy-by-jenkins",
        "votes": "48",
        "views": "112k",
        "author": "Marcello DeSales",
        "issued_at": "2015-01-02 22:08",
        "tags": [
            "git",
            "maven",
            "npm",
            "docker",
            "dockerfile"
        ],
        "answers": [
            {
                "answer": "Docker has multiple ways to set proxies that take effect at different times.\nIf your docker build has to retrieve a base image through a proxy, you'll want to specify build-args:\ndocker build --build-arg HTTP_PROXY=$http_proxy \\\n--build-arg HTTPS_PROXY=$http_proxy --build-arg NO_PROXY=\"$no_proxy\" \\\n--build-arg http_proxy=$http_proxy --build-arg https_proxy=$http_proxy \\\n--build-arg no_proxy=\"$no_proxy\" -t myContainer /path/to/Dockerfile/directory\nwhere $http_proxy and $no_proxy were set in my bashrc. I used both HTTP_PROXY and http_proxy because different utilities will check different variables (curl checks both, wget only checks the lowercase ones, etc).\nIf your docker build has a RUN curl/wget/etc command that has to go through the proxy, you'll need to specify an environment variable inside your docker image:\nENV https_proxy=http://proxy-us02.company.com:8080\nENV http_proxy=http://proxy-us02.company.com:8080\nENV HTTP_PROXY=http://proxy-us02.company.com:8080\nENV HTTPS_PROXY=http://proxy-us02.company.com:8080\nENV no_proxy=\"localhost,localdomain,127.0.0.1,etc\"\nENV NO_PROXY=\"localhost,localdomain,127.0.0.1,etc\"\nIf you don't want this environment variable inside your image at runtime, you can remove all these at the end:\nRUN unset http_proxy https_proxy no_proxy HTTP_PROXY HTTPS_PROXY NO_PROXY",
                "upvotes": 40,
                "answered_by": "jeremysprofile",
                "answered_at": "2017-10-20 19:40"
            }
        ]
    },
    {
        "title": "Dockerfile define multiple ARG arguments in a single line",
        "url": "https://stackoverflow.com/questions/59248868/dockerfile-define-multiple-arg-arguments-in-a-single-line",
        "votes": "48",
        "views": "42k",
        "author": "Muhammad Abdul Raheem",
        "issued_at": "2019-12-09 12:22",
        "tags": [
            "docker",
            "dockerfile"
        ],
        "answers": [
            {
                "answer": "YES, After release 1.2.0\nPreviously it was not possible and it threw the following error\nARG requires exactly one argument definition\nBut after release 1.2.0 it is now possible to define ARG like following\nARG CDN_ENDPOINT \\\nAWS_S3_BUCKET",
                "upvotes": 75,
                "answered_by": "Muhammad Abdul Raheem",
                "answered_at": "2019-12-09 12:22"
            },
            {
                "answer": "There is a way to write a one-liner, but it seems to require specifying default values, like so:\nARG CDN_ENDPOINT=endpoint \\\n    AWS_S3_BUCKET=bucket\nOne could then override these values by specifying build arguments in the docker build command:\ndocker build -t <name:tag> --build-arg CDN_ENDPOINT=overriddenValue1 --build-arg AWS_S3_BUCKET=overriddenValue2 <context>\nTo test layering, I created two Dockerfiles:\nDockerfile-1:\nFROM alpine\n\nARG CDN_ENDPOINT=endpoint \\\n    AWS_S3_BUCKET=bucket\nDockerfile-2:\nFROM alpine\n\nARG CDN_ENDPOINT=endpoint\nARG AWS_S3_BUCKET=bucket\nAfter building each and running the command docker history <image-id>, here were the results showing the layers. It does seems like image-1 kept the ARG one-liner to one layer while image-2 used two layers:\nimage-1:\nIMAGE          CREATED        CREATED BY                                      SIZE      COMMENT\nca690c8c2cfc   3 months ago   ARG CDN_ENDPOINT=endpoint AWS_S3_BUCKET=buck\u2026   0B        buildkit.dockerfile.v0\n<missing>      3 months ago   /bin/sh -c #(nop)  CMD [\"/bin/sh\"]              0B\n<missing>      3 months ago   /bin/sh -c #(nop) ADD file:2a949686d9886ac7c\u2026   5.54MB\nimage-2:\nIMAGE          CREATED        CREATED BY                                      SIZE      COMMENT\ne4bee80eb809   3 months ago   ARG AWS_S3_BUCKET=bucket                        0B        buildkit.dockerfile.v0\n<missing>      3 months ago   ARG CDN_ENDPOINT=endpoint                       0B        buildkit.dockerfile.v0\n<missing>      3 months ago   /bin/sh -c #(nop)  CMD [\"/bin/sh\"]              0B\n<missing>      3 months ago   /bin/sh -c #(nop) ADD file:2a949686d9886ac7c\u2026   5.54MB",
                "upvotes": 2,
                "answered_by": "adamg",
                "answered_at": "2022-11-11 22:48"
            }
        ]
    },
    {
        "title": "Building common dependencies with docker-compose",
        "url": "https://stackoverflow.com/questions/37933204/building-common-dependencies-with-docker-compose",
        "votes": "48",
        "views": "25k",
        "author": "f5e7805f",
        "issued_at": "2016-06-20 23:10",
        "tags": [
            "docker",
            "docker-compose",
            "dockerfile"
        ],
        "answers": [
            {
                "answer": "Use a Makefile. docker-compose is not designed to build chains of images, it's designed for running containers.\nYou might also be interested in dobi which is a build-automation tool (like make) designed to work with docker images and containers.\nDisclaimer: I'm the author of dobi",
                "upvotes": 23,
                "answered_by": "dnephin",
                "answered_at": "2016-06-21 13:12"
            }
        ]
    },
    {
        "title": "What does working_dir tag mean in a docker-compose yml file",
        "url": "https://stackoverflow.com/questions/40341952/what-does-working-dir-tag-mean-in-a-docker-compose-yml-file",
        "votes": "48",
        "views": "55k",
        "author": "apil.tamang",
        "issued_at": "2016-10-31 12:12",
        "tags": [
            "docker-compose",
            "dockerfile"
        ],
        "answers": [
            {
                "answer": "I know this has been answered, but I wanted to add an important detail about working_dir in the docker-compose.yaml and a serious gotcha when also using WORKDIR in the dockerfile.\nIf you set working_dirin your docker-compose.yaml for a service, it not only overrides the first dockercompose WORKDIR declaration, but all of the WORKDIR declarations. Consider this docker-compose.yaml service declaration.\n  my-demo-svc:\n    build:\n      dockerfile: ./apps/my-demo-svc/dockerfile\n      working_dir: /other-dir # <- Note the /other-dir here.\nNow consider a Dockerfile like the following for ./apps/my-demo-svc/Dockerfile.\nFROM busybox\n\n# Let's say we set the working dir at the start of the script.\nWORKDIR /src\n\n# Maybe we add some source code of tooling here?\n\n# Set the working directory within the project folder\nWORKDIR /src/apps\n\n# Print out the working directory when the container starts.\nCMD [\"sh\", \"-c\", \"pwd\"]\nNow if I run the following docker-compose command, what do we think it will print out?\ndocker compose up\n    --force-recreate \\\n    --build my-demo-svc \\\n    my-demo-svc\nIf you answered /src/apps, you're in for a shock. It actually prints out:\n \u2714 Container my-demo-svc Recreated  0.0s \n   Attaching to my-demo-svc\n   my-demo-svc  | /other-dir\nAccording to our pwd command, we're in /other-dir!\nNow let's try the same command, but without the working_dir: /other-dir in the docker-compose.yaml this time.\n \u2714 Container my-demo-svc Recreated  0.0s \n   Attaching to my-demo-svc\n   my-demo-svc  | /src/apps\nThat's better. To summarise, understand the behaviour before using the working_dir attribute in your docker-compose.yaml. Otherwise you can end up with some very bizarre outcomes.",
                "upvotes": 6,
                "answered_by": "Aaron Newton",
                "answered_at": "2024-04-13 06:32"
            }
        ]
    },
    {
        "title": "Container command '/start.sh' not found or does not exist, entrypoint to container is shell script",
        "url": "https://stackoverflow.com/questions/37419042/container-command-start-sh-not-found-or-does-not-exist-entrypoint-to-contain",
        "votes": "47",
        "views": "104k",
        "author": "bobbyrne01",
        "issued_at": "2016-05-24 16:24",
        "tags": [
            "docker",
            "dockerfile"
        ],
        "answers": [
            {
                "answer": "On windows, while building the docker image, i also used to get the same error after building the image, that shell script is missing.. the path and the shebang was also correct.\nLater on, i read some where that it may be due to the encoding issue. I just opened the file in sublime editor and then..VIEW->Line Endings-> UNIX and just save the file, and rebuilded the image. Everything worked fine.\nI was getting this error, when i was building a image from windows.\nOther Option:\nSometime, we forgot to manually change the line format. So,what we can do is add this Run statement before the EntryPoint in dockerfile. It will encode the file in LF format.\n RUN sed -i 's/\\r$//' $app/filename.sh  && \\  \n        chmod +x $app/filename.sh\n\nENTRYPOINT $app/filename.sh",
                "upvotes": 126,
                "answered_by": "Utkarsh Yeolekar",
                "answered_at": "2017-08-31 05:26"
            }
        ]
    },
    {
        "title": "Load Postgres dump after docker-compose up",
        "url": "https://stackoverflow.com/questions/36781984/load-postgres-dump-after-docker-compose-up",
        "votes": "47",
        "views": "68k",
        "author": "jood",
        "issued_at": "2016-04-21 23:10",
        "tags": [
            "postgresql",
            "docker",
            "docker-compose",
            "dockerfile"
        ],
        "answers": [
            {
                "answer": "Reading https://hub.docker.com/_/postgres/, the section 'Extend this image' explains that any .sql in /docker-entrypoint-initdb.d will be executed after build.\nI just needed to change my Dockerfile.db to:\nFROM postgres\n\nADD ./devops/db/dummy_dump.sql /docker-entrypoint-initdb.d\nAnd it works!",
                "upvotes": 80,
                "answered_by": "jood",
                "answered_at": "2016-04-22 00:41"
            },
            {
                "answer": "Another option that doesn't require a Dockerfile would be to mount your sql file into the docker-entrypoint-initdb.d folder using the volumes attribute of docker-compose. The official postgres image https://hub.docker.com/_/postgres/ will import and execute all SQL files placed in that folder. So something like\nservices:\n  postgres:\n    environment:\n      POSTGRES_DB: my_db_name\n      POSTGRES_USER: my_name\n      POSTGRES_PASSWORD: my_password\n  volumes:\n    - ./devops/db/dummy_dump.sql:/docker-entrypoint-initdb.d/dummy_dump.sql\nThis will automatically populate the specified POSTGRES_DB for you.",
                "upvotes": 32,
                "answered_by": "emilaz",
                "answered_at": "2021-12-20 12:13"
            },
            {
                "answer": "sudo docker exec postgres psql -U postgres my_db_name < dump.sql",
                "upvotes": 18,
                "answered_by": "Valentin Kantor",
                "answered_at": "2018-06-03 03:50"
            },
            {
                "answer": "After the docker-compose up, do docker ps it will give you a list of active docker containers. From that, you can get the container ID.\nThen,\ndocker exec -i ${CONTAINER_ID} psql -U ${USER} < ${SQL_FILE}",
                "upvotes": 5,
                "answered_by": "Prashantkumar K B",
                "answered_at": "2019-12-13 07:14"
            }
        ]
    },
    {
        "title": "How to run my python script on docker?",
        "url": "https://stackoverflow.com/questions/47202705/how-to-run-my-python-script-on-docker",
        "votes": "47",
        "views": "189k",
        "author": "Pulkit",
        "issued_at": "2017-11-09 13:09",
        "tags": [
            "python",
            "docker",
            "dockerfile",
            "docker-swarm"
        ],
        "answers": [
            {
                "answer": "Alright, first create a specific project directory for your docker image. For example:\nmkdir /home/pi/Desktop/teasr/capturing\nCopy your dockerfile and script in there and change the current context to this directory.\ncp /home/pi/Desktop/teasr/capturing.py /home/pi/Desktop/teasr/dockerfile /home/pi/Desktop/teasr/capturing/\n\ncd /home/pi/Desktop/teasr/capturing\nThis is for best practice, as the first thing the docker-engine does on build, is read the whole current context.\nNext we'll take a look at your dockerfile. It should look something like this now:\nFROM python:latest\n\nWORKDIR /usr/local/bin\n\nCOPY capturing.py .\n\nCMD [\"capturing.py\", \"-OPTIONAL_FLAG\"]\nThe next thing you need to do is build it with a smart name. Using dots is generally discouraged.\ndocker build -t pulkit/capturing:1.0 .\nNext thing is to just run the image like you've done.\ndocker run -ti --name capturing pulkit/capturing:1.0\nThe script now get executed inside the container and will probably exit upon completion.\nEdit after finding the problem that created the following error:\nstandard_init_linux.go:195: exec user process caused \"exec format error\"\nThere's a different architecture beneath raspberry pi's (ARM instead of x86_64), which COULD'VE BEEN the problem, but wasn't. If that would've been the problem, a switch of the parent image to FROM armhf/python would've been enough.\nSource\nBUT! The error kept occurring.\nSo the solution to this problem is a simple missing Sha-Bang on top of the python script. The first line in the script needs to be #!/usr/bin/env python and that should solve the problem.\nSource",
                "upvotes": 34,
                "answered_by": "samprog",
                "answered_at": "2017-11-14 07:53"
            },
            {
                "answer": "I Followed @samprog (most accepted) answer on my machine running on UBUNTU VERSION=\"14.04.6\". and was getting \"standard_init_linux.go:195: exec user process caused \"exec format error\"\nNone of the solution worked for me mentioned above.\nFixed the error after changing my Dockerfile as follows\nFROM python:latest\nCOPY capturing.py ./capturing.py\nCMD [\"python\",\"capturing.py\"]\nNote: If your script import some other module then you need to modify COPY statement in your Dockerfile as follows - COPY *.py ./\nHope this will be useful for others.",
                "upvotes": 7,
                "answered_by": "Nitendra",
                "answered_at": "2020-02-12 07:46"
            }
        ]
    },
    {
        "title": "Redis Docker connection refused",
        "url": "https://stackoverflow.com/questions/33675914/redis-docker-connection-refused",
        "votes": "47",
        "views": "69k",
        "author": "Ikenna",
        "issued_at": "2015-11-12 16:05",
        "tags": [
            "redis",
            "docker",
            "boot2docker",
            "dockerfile",
            "docker-compose"
        ],
        "answers": [
            {
                "answer": "Find the discovered port of the redis and use that to connect\ndocker inspect --format='{{range .NetworkSettings.Networks}}{{.IPAddress}}{{end}}' redis-server",
                "upvotes": 16,
                "answered_by": "Rajesh Guptan",
                "answered_at": "2017-05-11 00:59"
            },
            {
                "answer": "If you're using the Redis 6.2 configuration example as a starting place (like I was when I encountered this issue), the combination of the bind 127.0.0.1 directive and protected-mode yes was not allowing for communication between Docker containers. For my purposes I commented out the bind directive and set the protected-mode directive to no -- but heed the provided warnings about not exposing Redis to the open internet.",
                "upvotes": 8,
                "answered_by": "john-goldsmith",
                "answered_at": "2020-01-08 01:15"
            }
        ]
    },
    {
        "title": "How do I declare multiple maintainers in my Dockerfile?",
        "url": "https://stackoverflow.com/questions/38899977/how-do-i-declare-multiple-maintainers-in-my-dockerfile",
        "votes": "47",
        "views": "19k",
        "author": "amacleod",
        "issued_at": "2016-08-11 15:02",
        "tags": [
            "docker",
            "dockerfile"
        ],
        "answers": [
            {
                "answer": "You can only specify one MAINTAINER instruction in a Dockerfile.\nFurthermore, MAINTAINER will be deprecated in the upcoming 1.13.0 release, see deprecations and this pull request.\nThe recommended solution is to use LABEL instead, e.g.\nLABEL authors=\"first author,second author\"\nLabels have a key=value syntax. This means you cannot assign the same label more than once and you cannot assign multiple values to a given label. But you can combine multiple values into one with a syntax of your choice as illustrated in the example..",
                "upvotes": 87,
                "answered_by": "Harald Albers",
                "answered_at": "2016-09-29 10:35"
            }
        ]
    },
    {
        "title": "What is the purpose of the Docker build context?",
        "url": "https://stackoverflow.com/questions/44465703/what-is-the-purpose-of-the-docker-build-context",
        "votes": "47",
        "views": "16k",
        "author": "frogstarr78",
        "issued_at": "2017-06-09 19:49",
        "tags": [
            "docker",
            "build",
            "dockerfile"
        ],
        "answers": [
            {
                "answer": "TL;DR: \"because the client and daemon may not even run on the same machine\"\nThe docker command is the docker client of the dockerd that is the service that can run directly in your PC (linux) or under a Linux VM under OSX or Windows.\nQ: What is the purpose of the Docker build context?\nFrom here:\nWould probably be good to also mention that this has to happen this way because the client and daemon may not even run on the same machine, so without this \"context\" the daemon machine wouldn't have any other way to get files for ADD or otherwise\nQ: If the build process compresses the current directory contents and sends it to the daemon, where does it go?\nThe docker daemon receives the compressed directory and process it on the fly; does not matter where it is stored in that moment.\nQ: Why doesn't it make that content available for use in the image?\nThink this: How can docker know where do you want to put each file/directory in the target image? With COPY/ADD directives you can control where put each one. The case that you've mentioned is only a trivial example where you have a single directory and a single target.",
                "upvotes": 35,
                "answered_by": "Robert",
                "answered_at": "2017-06-09 21:03"
            }
        ]
    },
    {
        "title": "How to set timezone inside alpine base docker image?",
        "url": "https://stackoverflow.com/questions/68996420/how-to-set-timezone-inside-alpine-base-docker-image",
        "votes": "46",
        "views": "77k",
        "author": "kavindu",
        "issued_at": "2021-08-31 09:41",
        "tags": [
            "docker",
            "dockerfile",
            "timezone"
        ],
        "answers": [
            {
                "answer": "You need to install the tzdata package and then set the enviroment variable TZ to a timezone. (List with all the timezones)\nFROM alpine:latest\nRUN apk add --no-cache tzdata\nENV TZ=Europe/Copenhagen\nOutput\n$ docker run --rm alpine date\nTue Aug 31 09:52:08 UTC 2021\n\n$ docker run --rm myimage date\nTue Aug 31 11:52:13 CEST 2021",
                "upvotes": 131,
                "answered_by": "Hans Kilian",
                "answered_at": "2021-08-31 09:50"
            },
            {
                "answer": "This is my Dockerfile and it works:\nFROM alpine:latest\n\n# Essentials\nRUN apk add -U tzdata\nENV TZ=America/Santiago\nRUN cp /usr/share/zoneinfo/America/Santiago /etc/localtime\nyou have to replace 'America/Santiago' with your timezone",
                "upvotes": 21,
                "answered_by": "edocollado",
                "answered_at": "2022-03-15 13:23"
            }
        ]
    },
    {
        "title": "SSH agent forwarding during docker build",
        "url": "https://stackoverflow.com/questions/43418188/ssh-agent-forwarding-during-docker-build",
        "votes": "46",
        "views": "50k",
        "author": "Anand",
        "issued_at": "2017-04-14 19:41",
        "tags": [
            "docker",
            "docker-compose",
            "dockerfile",
            "ssh-keys",
            "docker-image"
        ],
        "answers": [
            {
                "answer": "This may be solved using an alternative build script. For example you may create a bash script and put it in ~/usr/local/bin/docker-compose or your favourite location:\n#!/bin/bash\n\ntrap 'kill $(jobs -p)' EXIT\nsocat TCP-LISTEN:56789,reuseaddr,fork UNIX-CLIENT:${SSH_AUTH_SOCK} &\n\n/usr/bin/docker-compose $@\nThen in your Dockerfile you would use your existing ssh socket:\n...\nENV SSH_AUTH_SOCK /tmp/auth.sock\n...\n  && apk add --no-cache socat openssh \\\n  && /bin/sh -c \"socat -v UNIX-LISTEN:${SSH_AUTH_SOCK},unlink-early,mode=777,fork TCP:172.22.1.11:56789 &> /dev/null &\" \\\n  && bundle install \\\n...\nor any other ssh commands will works\nNow you can call our custom docker-compose build. It would call the actual docker script with a shared ssh socket.",
                "upvotes": 5,
                "answered_by": "Samoilenko Yuri",
                "answered_at": "2017-11-30 07:22"
            },
            {
                "answer": "This one is also interesting:\nhttps://github.com/docker/for-mac/issues/483#issuecomment-344901087\nIt looks like:\nOn the host\nmkfifo myfifo\nnc -lk 12345 <myfifo | nc -U $SSH_AUTH_SOCK >myfifo\nIn the dockerfile\nRUN mkfifo myfifo\nRUN while true; do \\\n  nc 172.17.0.1 12345 <myfifo | nc -Ul /tmp/ssh-agent.sock >myfifo \\\ndone &\n\nRUN export SSH_AUTH_SOCK=/tmp/ssh-agent.sock\n\nRUN ssh ...",
                "upvotes": 1,
                "answered_by": "Dan Pav",
                "answered_at": "2019-04-29 07:27"
            }
        ]
    },
    {
        "title": "How to tag an image in a Dockerfile? [duplicate]",
        "url": "https://stackoverflow.com/questions/38993182/how-to-tag-an-image-in-a-dockerfile",
        "votes": "46",
        "views": "72k",
        "author": "J\u00e9r\u00f4me Verstrynge",
        "issued_at": "2016-08-17 09:46",
        "tags": [
            "docker",
            "tags",
            "dockerfile",
            "docker-image"
        ],
        "answers": [
            {
                "answer": "Unfortunately it is not possible. You can use build.sh script, which contains like this:\n#!/usr/bin/env bash\nif [ $# -eq 0 ]\n  then\n    tag='latest'\n  else\n    tag=$1\nfi\n\ndocker build -t project:$tag .\nRun ./build.sh for creating image project:latest or run ./build.sh your_tag to specify image tag.",
                "upvotes": 46,
                "answered_by": "Bukharov Sergey",
                "answered_at": "2016-08-17 10:23"
            }
        ]
    },
    {
        "title": "installing `lightdm` in Dockerfile raises interactive keyboard layout menu",
        "url": "https://stackoverflow.com/questions/38165407/installing-lightdm-in-dockerfile-raises-interactive-keyboard-layout-menu",
        "votes": "45",
        "views": "16k",
        "author": "lurscher",
        "issued_at": "2016-07-02 23:41",
        "tags": [
            "ubuntu",
            "docker",
            "dockerfile",
            "interactive",
            "apt-get"
        ],
        "answers": [
            {
                "answer": "copy a working /etc/default/keyboard file to the directory that contains the Dockerfile\nAdd the following line to your Dockerfile before installing xdm or lightdm\nCOPY ./keyboard /etc/default/keyboard\nExample keyboard file:\n# Check /usr/share/doc/keyboard-configuration/README.Debian for\n# documentation on what to do after having modified this file.\n\n# The following variables describe your keyboard and can have the same\n# values as the XkbModel, XkbLayout, XkbVariant and XkbOptions options\n# in /etc/X11/xorg.conf.\n\nXKBMODEL=\"pc105\"\nXKBLAYOUT=\"us\"\nXKBVARIANT=\"intl\"\nXKBOPTIONS=\"\"\n\n# If you don't want to use the XKB layout on the console, you can\n# specify an alternative keymap.  Make sure it will be accessible\n# before /usr is mounted.\n# KMAP=/etc/console-setup/defkeymap.kmap.gz\nBACKSPACE=\"guess\"",
                "upvotes": 18,
                "answered_by": "anneb",
                "answered_at": "2016-08-09 12:19"
            }
        ]
    },
    {
        "title": "Docker and .bash_history",
        "url": "https://stackoverflow.com/questions/28279862/docker-and-bash-history",
        "votes": "45",
        "views": "24k",
        "author": "tzenderman",
        "issued_at": "2015-02-02 14:23",
        "tags": [
            "bash",
            "docker",
            "ipython",
            "dockerfile"
        ],
        "answers": [
            {
                "answer": "It is the example from the documentation about volume: Mount a host file as a data volume:\ndocker run --rm -it -v ~/.bash_history:/root/.bash_history ubuntu /bin/bash\nThis will drop you into a bash shell in a new container, you will have your bash history from the host and when you exit the container, the host will have the history of the commands typed while in the container.",
                "upvotes": 32,
                "answered_by": "user2915097",
                "answered_at": "2015-02-02 14:38"
            },
            {
                "answer": "In your docker-compose.override.yml:\nversion: '2'\nservices:\n  whatever:\n    \u2026\n    volumes:\n      - \u2026\n      - ~/.bash_history:/root/.bash_history",
                "upvotes": 12,
                "answered_by": "\u00c9douard Lopez",
                "answered_at": "2016-08-20 11:43"
            }
        ]
    },
    {
        "title": "How to use Docker's COPY/ADD instructions to copy a single file to an image",
        "url": "https://stackoverflow.com/questions/31640660/how-to-use-dockers-copy-add-instructions-to-copy-a-single-file-to-an-image",
        "votes": "45",
        "views": "70k",
        "author": "Andrew Butler",
        "issued_at": "2015-07-26 18:47",
        "tags": [
            "docker",
            "dockerfile"
        ],
        "answers": [
            {
                "answer": "As stated in the Dockerfile documentation:\nIf <src> is any other kind of file [besides a local tar archive], it is copied individually along with its metadata. In this case, if <dest> ends with a trailing slash /, it will be considered a directory and the contents of <src> will be written at <dest>/base(<src>).\nIf <dest> does not end with a trailing slash, it will be considered a regular file and the contents of <src> will be written at <dest>.\nThus, you have to write COPY test.txt /usr/src/app/ with a trailing /.",
                "upvotes": 51,
                "answered_by": "jwodder",
                "answered_at": "2015-07-26 18:53"
            }
        ]
    },
    {
        "title": "Commands to execute background process in Docker CMD",
        "url": "https://stackoverflow.com/questions/31559132/commands-to-execute-background-process-in-docker-cmd",
        "votes": "45",
        "views": "79k",
        "author": "Jeevitha G",
        "issued_at": "2015-07-22 09:27",
        "tags": [
            "bash",
            "docker",
            "shell",
            "dockerfile",
            "docker-cmd"
        ],
        "answers": [
            {
                "answer": "Of course there is also the official Docker documentation of how to start multiple services, again using a script file not the CMD. The docker documentation also states how to use supervisord as a process manager:\nFROM ubuntu:latest\nRUN apt-get update && apt-get install -y supervisor\nRUN mkdir -p /var/log/supervisor\nCOPY supervisord.conf /etc/supervisor/conf.d/supervisord.conf\nCOPY my_first_process my_first_process\nCOPY my_second_process my_second_process\nCMD [\"/usr/bin/supervisord\"]\nIf it is an option you could use a phusion base images which allows running multiple processes in one container. Thus you can run system services such as cron or other processes using a service supervisor like runit.\nMore information about whether or not a phusion base image is a good choice in your use case can be found here\nA ruby focused description of how to avoid running more processes in your container except for your app you can find here. The elaborations are too detailed to repeat on SO.",
                "upvotes": 1,
                "answered_by": "sebisnow",
                "answered_at": "2021-03-04 10:01"
            }
        ]
    },
    {
        "title": "How to measure Docker build steps duration?",
        "url": "https://stackoverflow.com/questions/46166293/how-to-measure-docker-build-steps-duration",
        "votes": "44",
        "views": "20k",
        "author": "Ryan Wilson-Perkin",
        "issued_at": "2017-09-12 00:50",
        "tags": [
            "docker",
            "dockerfile"
        ],
        "answers": [
            {
                "answer": "BuildKit, which was experimental in 18.06 and generally available in 18.09, has this functionality built in. To configure the dockerd daemon with experimental mode, you can setup the daemon.json:\n$ cat /etc/docker/daemon.json\n{\n  \"experimental\": true\n}\nThen you can enable BuildKit from the client side with an environment variable:\n$ export DOCKER_BUILDKIT=1\n$ docker build -t java-test:latest .\n[+] Building 421.6s (13/13) FINISHED\n => local://context (.dockerignore)                                                                           1.6s\n => => transferring context: 56B                                                                              0.3s\n => local://dockerfile (Dockerfile)                                                                           2.0s\n => => transferring dockerfile: 895B                                                                          0.4s\n => CACHED docker-image://docker.io/tonistiigi/copy:v0.1.3@sha256:e57a3b4d6240f55bac26b655d2cfb751f8b9412d6f  0.1s\n => docker-image://docker.io/library/openjdk:8-jdk-alpine                                                     1.0s\n => => resolve docker.io/library/openjdk:8-jdk-alpine                                                         0.0s\n => local://context                                                                                           1.7s\n => => transferring context: 6.20kB                                                                           0.4s\n => docker-image://docker.io/library/openjdk:8-jre-alpine                                                     1.3s\n => => resolve docker.io/library/openjdk:8-jre-alpine                                                         0.0s\n => /bin/sh -c apk add --no-cache maven                                                                      61.0s\n => copy /src-0/pom.xml java/pom.xml                                                                          1.3s\n => /bin/sh -c mvn dependency:go-offline                                                                    339.4s\n => copy /src-0 java                                                                                          0.9s\n => /bin/sh -c mvn package -Dmaven.test.skip=true                                                            10.2s\n => copy /src-0/gs-spring-boot-docker-0.1.0.jar java/app.jar                                                  0.8s\n => exporting to image                                                                                        1.2s\n => => exporting layers                                                                                       1.0s\n => => writing image sha256:d57028743ca10bb4d0527a294d5c83dd941aeb1033d4fe08949a135677846179                  0.1s\n => => naming to docker.io/library/java-test:latest                                                           0.1s\nThere's also an option to disable the tty console output which generates output more suitable for scripting with each section having a start, stop, and duration:\n$ docker build -t java-test:latest --progress plain .                                                                                                                         \n\n#1 local://dockerfile (Dockerfile)                                                      \n#1       digest: sha256:da721b637ea85add6e26070a48520675cefc2bed947c626f392be9890236d11b\n#1         name: \"local://dockerfile (Dockerfile)\"      \n#1      started: 2018-09-05 19:30:53.899809093 +0000 UTC\n#1    completed: 2018-09-05 19:30:53.899903348 +0000 UTC\n#1     duration: 94.255\u00b5s\n#1      started: 2018-09-05 19:30:53.900069076 +0000 UTC\n#1 transferring dockerfile: 38B done\n#2 ...              \n\n#2 local://context (.dockerignore)  \n#2       digest: sha256:cbf55954659905f4d7bd2fc3e5e52d566055eecd94fd7503565315022d834c21\n#2         name: \"local://context (.dockerignore)\"       \n#2      started: 2018-09-05 19:30:53.899624016 +0000 UTC\n#2    completed: 2018-09-05 19:30:53.899695455 +0000 UTC\n#2     duration: 71.439\u00b5s\n#2      started: 2018-09-05 19:30:53.899839335 +0000 UTC\n#2    completed: 2018-09-05 19:30:54.359527504 +0000 UTC\n#2     duration: 459.688169ms                                                            \n#2 transferring context: 34B done                                \n\n\n#1 local://dockerfile (Dockerfile)\n#1    completed: 2018-09-05 19:30:54.592304408 +0000 UTC\n#1     duration: 692.235332ms\n\n\n#3 docker-image://docker.io/tonistiigi/copy:v0.1.3@sha256:e57a3b4d6240f55ba...           \n#3       digest: sha256:39386c91e9f27ee70b2eefdee12fc8a029bf5edac621b91eb5f3e6001d41dd4f\n#3         name: \"docker-image://docker.io/tonistiigi/copy:v0.1.3@sha256:e57a3b4d6240f55bac26b655d2cfb751f8b9412d6f7bb1f787e946391fb4b21b\"\n#3      started: 2018-09-05 19:30:54.731749377 +0000 UTC \n#3    completed: 2018-09-05 19:30:54.732013326 +0000 UTC\n#3     duration: 263.949\u00b5s\n\n\n#5 docker-image://docker.io/library/openjdk:8-jdk-alpine\n#5       digest: sha256:d680c6a82813d080081fbc3c024d21ddfa7ff995981cc7b4bfafe55edf80a319\n#5         name: \"docker-image://docker.io/library/openjdk:8-jdk-alpine\"\n#5      started: 2018-09-05 19:30:54.731483638 +0000 UTC\n#5    completed: 2018-09-05 19:30:54.732480345 +0000 UTC\n#5     duration: 996.707\u00b5s\n\n\n#4 docker-image://docker.io/library/openjdk:8-jre-alpine\n#4       digest: sha256:9ed31df4e6731a1718ea93bfa77354ad1ea2d1625c1cb16e2087d16d0b84bd00\n#4         name: \"docker-image://docker.io/library/openjdk:8-jre-alpine\"                \n#4      started: 2018-09-05 19:30:54.73176516 +0000 UTC\n#4    completed: 2018-09-05 19:30:54.732603067 +0000 UTC\n#4     duration: 837.907\u00b5s                              \n\n\n#7 local://context\n#7       digest: sha256:efe765161a29e2bf7a41439cd2e6656fcf6fa6bc97da825ac9b5a0d8adecf1ac\n#7         name: \"local://context\"\n#7      started: 2018-09-05 19:30:54.73178732 +0000 UTC\n#7    completed: 2018-09-05 19:30:54.731880943 +0000 UTC\n#7     duration: 93.623\u00b5s\n#7      started: 2018-09-05 19:30:54.792740019 +0000 UTC\n#7 transferring context: 473B done\n#7    completed: 2018-09-05 19:30:55.059008345 +0000 UTC\n#7     duration: 266.268326ms\n\n\n#9 /bin/sh -c mvn dependency:go-offline\n#9       digest: sha256:2197672cd7a44d93e0dba40aa00d7ef41f8680226d91f469d1c925646bdc8d6d\n#9         name: \"/bin/sh -c mvn dependency:go-offline\"\n#9      started: 2018-09-05 19:30:55.203449147 +0000 UTC\n#9    completed: 2018-09-05 19:30:55.203449147 +0000 UTC\n#9     duration: 0s\n#9       cached: true\n\n\n#10 copy /src-0 java\n#10       digest: sha256:36cf252c34be098731bd8c5fb3f273f9c1437a5f74a65a3555d71150c2092fa7\n#10         name: \"copy /src-0 java\"\n#10      started: 2018-09-05 19:30:55.203449147 +0000 UTC\n#10    completed: 2018-09-05 19:30:55.203449147 +0000 UTC\n#10     duration: 0s\n#10       cached: true\n\n#11 /bin/sh -c mvn package -Dmaven.test.skip=true\n#11       digest: sha256:390464b1fdc7a4c833b3476033d95b7714e22bcbfd018469e97b04781cb41532\n#11         name: \"/bin/sh -c mvn package -Dmaven.test.skip=true\"\n#11      started: 2018-09-05 19:30:55.203449147 +0000 UTC\n#11    completed: 2018-09-05 19:30:55.203449147 +0000 UTC\n#11     duration: 0s\n#11       cached: true\n\n\n#12 copy /src-0/gs-spring-boot-docker-0.1.0.jar java/app.jar\n#12       digest: sha256:a7d60191a720f80de72a77ebe0d4bd1b0fd55d44e623661e80916b7fd1952076\n#12         name: \"copy /src-0/gs-spring-boot-docker-0.1.0.jar java/app.jar\"\n#12      started: 2018-09-05 19:30:55.203449147 +0000 UTC\n#12    completed: 2018-09-05 19:30:55.203555216 +0000 UTC\n#12     duration: 106.069\u00b5s\n#12       cached: true\n\n\n#6 /bin/sh -c apk add --no-cache maven\n#6       digest: sha256:db505db5e418f195c7bad3a710ad40bec3d91d47ff11a6f464b3ae37af744e7d\n#6         name: \"/bin/sh -c apk add --no-cache maven\"\n#6      started: 2018-09-05 19:30:55.203449147 +0000 UTC\n#6    completed: 2018-09-05 19:30:55.203449147 +0000 UTC\n#6     duration: 0s\n#6       cached: true\n\n\n#8 copy /src-0/pom.xml java/pom.xml\n#8       digest: sha256:f032d4ff111c6ab0efef1a4e37d2467fffe43f48a529b8d56291ec81f96296ab\n#8         name: \"copy /src-0/pom.xml java/pom.xml\"\n#8      started: 2018-09-05 19:30:55.203449147 +0000 UTC\n#8    completed: 2018-09-05 19:30:55.203449147 +0000 UTC\n#8     duration: 0s\n#8       cached: true\n\n\n#13 exporting to image\n#13       digest: sha256:d536dc2895c30fbde898bb4635581350a87c21f3695913ba21850a73d31422d9\n#13         name: \"exporting to image\"\n#13      started: 2018-09-05 19:30:55.203674127 +0000 UTC\n#13 exporting layers done\n#13 writing image sha256:d57028743ca10bb4d0527a294d5c83dd941aeb1033d4fe08949a135677846179 0.1s done\n#13 naming to docker.io/library/java-test:latest\n#13    completed: 2018-09-05 19:30:55.341300051 +0000 UTC\n#13     duration: 137.625924ms\n#13 naming to docker.io/library/java-test:latest 0.0s done",
                "upvotes": 64,
                "answered_by": "BMitch",
                "answered_at": "2018-09-05 19:38"
            },
            {
                "answer": "You could use the tool time to measure the build times. E.g.\ntime docker build .\nFor individual build steps it's getting more difficult. You could add a RUN date command after each step, but this would add another layer to the image. So it's getting a bit messy.",
                "upvotes": 4,
                "answered_by": "tlo",
                "answered_at": "2017-09-12 14:19"
            }
        ]
    },
    {
        "title": "FIle could not be opened in append mode: failed to open stream: Permission denied laradock",
        "url": "https://stackoverflow.com/questions/63203144/file-could-not-be-opened-in-append-mode-failed-to-open-stream-permission-denie",
        "votes": "44",
        "views": "192k",
        "author": "Pawan Rai",
        "issued_at": "2020-08-01 08:12",
        "tags": [
            "laravel",
            "docker",
            "docker-compose",
            "dockerfile",
            "laradock"
        ],
        "answers": [
            {
                "answer": "This worked for me:\nchown -R www-data:www-data \"project foldername\"",
                "upvotes": 70,
                "answered_by": "MARIO MORENO",
                "answered_at": "2021-02-06 21:37"
            },
            {
                "answer": "If you aren't running your application as root on your web server, then it wont have write access based on the permissions you've provided.\nChecked from workspace container bash. storage/logs/ directory has drwxr-xr-x 2 root root 4096 Aug 1 07:37 logs\nThe error is complaining about permission denial for opening in append mode - it doesn't have permission to write to the file, only root does.\nWhat you need to do is make your web server group the owner of the storage directory:\nchown -R www-data:www-data /var/www/laravel-api/storage/\nThe www-data can be switched out for whatever group your web server is associated with.\nTo avoid completely repeating an amazing and complete answer, I recommend you give this answer a read:\nHow to set up file permissions for Laravel?",
                "upvotes": 11,
                "answered_by": "ChewySalmon",
                "answered_at": "2020-08-01 10:34"
            },
            {
                "answer": "you have to enter the workspace first by typing \"docker-compose exec workspace bash\"(without quotes)\nTo give a write permission to a single file,\nchmod -R 777 /var/www/laravel-api/storage/logs/laravel.log\nor\nchmod -R 777 /var/www/laravel-api/storage/logs\nor\nsudo chmod -R 777 /var/www/laravel-api/storage/logs/laravel.log\nwhen the same error appears but different folders or files, do the same thing only change the folder name\nexample :\nchmod -R 777 /var/www/laravel-api/storage/logs\n\nchmod -R 777 /var/www/laravel-api/storage/framework/views\n\nchmod -R 777 /var/www/laravel-api/storage/framework/sessions",
                "upvotes": 8,
                "answered_by": "Anjana Silva",
                "answered_at": "2021-04-12 10:23"
            }
        ]
    },
    {
        "title": "Connect docker python to SQL server with pyodbc",
        "url": "https://stackoverflow.com/questions/46405777/connect-docker-python-to-sql-server-with-pyodbc",
        "votes": "44",
        "views": "62k",
        "author": "K\u00e5re Rasmussen",
        "issued_at": "2017-09-25 12:53",
        "tags": [
            "docker",
            "dockerfile",
            "pyodbc"
        ],
        "answers": [
            {
                "answer": "Running through this recently I found it was necessary to additionally include the following line (note that it did not build without this step):\nRUN apt-get install --reinstall build-essential -y\nThe full Dockerfile looks as follows:\n# parent image\nFROM python:3.7-slim\n\n# install FreeTDS and dependencies\nRUN apt-get update \\\n && apt-get install unixodbc -y \\\n && apt-get install unixodbc-dev -y \\\n && apt-get install freetds-dev -y \\\n && apt-get install freetds-bin -y \\\n && apt-get install tdsodbc -y \\\n && apt-get install --reinstall build-essential -y\n\n# populate \"ocbcinst.ini\"\nRUN echo \"[FreeTDS]\\n\\\nDescription = FreeTDS unixODBC Driver\\n\\\nDriver = /usr/lib/x86_64-linux-gnu/odbc/libtdsodbc.so\\n\\\nSetup = /usr/lib/x86_64-linux-gnu/odbc/libtdsS.so\" >> /etc/odbcinst.ini\n\n# install pyodbc (and, optionally, sqlalchemy)\nRUN pip install --trusted-host pypi.python.org pyodbc==4.0.26 sqlalchemy==1.3.5\n\n# run app.py upon container launch\nCMD [\"python\", \"app.py\"]\nHere's one way to then actually establish the connection inside app.py, via sqlalchemy (and assuming port 1433):\nimport sqlalchemy as sa\nargs = (username, password, server, database)\nconnstr = \"mssql+pyodbc://{}:{}@{}/{}?driver=FreeTDS&port=1433&odbc_options='TDS_Version=8.0'\"\nengine = sa.create_engine(connstr.format(*args))",
                "upvotes": 33,
                "answered_by": "Mark Richardson",
                "answered_at": "2019-10-17 16:02"
            },
            {
                "answer": "Based on K\u00e5re Rasmussen's answer, here's a complete dockerfile for further use.\nMake sure to edit the last two lines according to your architecture! They should reflect the actual paths to libtdsodbc.so and libtdsS.so.\nIf you're not sure about the paths to libtdsodbc.so and libtdsS.so, try dpkg --search libtdsodbc.so and dpkg --search libtdsS.so.\nFROM python:3\n\n#Install FreeTDS and dependencies for PyODBC\nRUN apt-get update && apt-get install -y tdsodbc unixodbc-dev \\\n && apt install unixodbc-bin -y  \\\n && apt-get clean -y\n\nRUN echo \"[FreeTDS]\\n\\\nDescription = FreeTDS unixODBC Driver\\n\\\nDriver = /usr/lib/arm-linux-gnueabi/odbc/libtdsodbc.so\\n\\\nSetup = /usr/lib/arm-linux-gnueabi/odbc/libtdsS.so\" >> /etc/odbcinst.ini\nAfterwards, install PyODBC, COPY your app and run it.",
                "upvotes": 10,
                "answered_by": "Phonolog",
                "answered_at": "2018-02-25 16:01"
            }
        ]
    },
    {
        "title": "Docker: failed to export image: failed to create image: failed to get layer",
        "url": "https://stackoverflow.com/questions/51115856/docker-failed-to-export-image-failed-to-create-image-failed-to-get-layer",
        "votes": "44",
        "views": "37k",
        "author": "Lewik",
        "issued_at": "2018-06-30 14:34",
        "tags": [
            "docker",
            "dockerfile"
        ],
        "answers": [
            {
                "answer": "This problem occurs with a specific sequence of COPY commands in a multistage build.\nMore precisely, the bug triggers when there is a COPY instruction producing null effect (for example if the content copied is already present in the destination, with 0 diff), followed immediately by another COPY instruction.\nA workaround could be to add RUN true between COPY statements:\nCOPY ./lib/ /usr/src/app/BOOT-INF/lib/\nRUN true\nCOPY ./lib/entities-1.0-SNAPSHOT.jar /usr/src/app/BOOT-INF/lib/entities-1.0-SNAPSHOT.jar\nRUN true\nCOPY ./app/ /usr/src/app/\nAnother way that seems to work is to launch the build using BUILDKIT, like that:\nDOCKER_BUILDKIT=1 docker build --tag app:test .\nSee: https://github.com/moby/moby/issues/37965",
                "upvotes": 105,
                "answered_by": "veben",
                "answered_at": "2020-06-16 13:21"
            }
        ]
    },
    {
        "title": "connecting to local mongodb from docker container",
        "url": "https://stackoverflow.com/questions/43800164/connecting-to-local-mongodb-from-docker-container",
        "votes": "44",
        "views": "50k",
        "author": "chaudharyp",
        "issued_at": "2017-05-05 08:32",
        "tags": [
            "node.js",
            "mongodb",
            "docker",
            "mongoose",
            "dockerfile"
        ],
        "answers": [
            {
                "answer": "On Docker for Mac, you can use host.docker.internal if your mongo is running on your localhost. You could have your code read in an env variable for the mongo host and set it in the Dockerfile like so:\nENV MONGO_HOST \"host.docker.internal\"\nSee here for more details on https://docs.docker.com/docker-for-mac/networking/#use-cases-and-workarounds",
                "upvotes": 39,
                "answered_by": "airpower44",
                "answered_at": "2018-07-11 19:19"
            },
            {
                "answer": "A docker container is a separate from your computer,\nSo you will not be able to connect to your localhost.\nYou should run the mongo server on a container and the connect with the image name.\nmongodb://CONTAINER-NAME:port",
                "upvotes": 14,
                "answered_by": "Daniel Taub",
                "answered_at": "2017-05-05 22:44"
            },
            {
                "answer": "Change the mongoose connect url to \"mongodb://host.docker.internal:27017/db-name\", you will be able to access the mongoDB from the container. Refrence https://docs.docker.com/desktop/networking/#use-cases-and-workarounds",
                "upvotes": 4,
                "answered_by": "Suraj Gudaji",
                "answered_at": "2023-03-12 20:13"
            }
        ]
    },
    {
        "title": "Failed to create endpoint on network nat: hnsCall failed in Win32: The process cannot access the file",
        "url": "https://stackoverflow.com/questions/53836103/failed-to-create-endpoint-on-network-nat-hnscall-failed-in-win32-the-process-c",
        "votes": "43",
        "views": "53k",
        "author": "user3365017",
        "issued_at": "2018-12-18 15:19",
        "tags": [
            "docker",
            "dockerfile"
        ],
        "answers": [
            {
                "answer": "Not sure how wise this is, but I checked the port wasn't in use with another app and still got the error.\nThis has fixed the issue a couple of times for me. In an Administrative PowerShell console, run the following:\nStop-Service docker\nStop-service hns\nStart-service hns\nStart-Service docker\ndocker network prune\nPartially sourced from this post.",
                "upvotes": 53,
                "answered_by": "Andy Joiner",
                "answered_at": "2019-10-16 22:10"
            }
        ]
    },
    {
        "title": "npm ERR! code ENOTEMPTY while npm install",
        "url": "https://stackoverflow.com/questions/43778883/npm-err-code-enotempty-while-npm-install",
        "votes": "43",
        "views": "110k",
        "author": "Sandy",
        "issued_at": "2017-05-04 09:23",
        "tags": [
            "npm",
            "dockerfile"
        ],
        "answers": [
            {
                "answer": "I had the same error/issue, and I removed the directory.\nrm -r node_modules/MODULE\nIt simply worked!",
                "upvotes": 18,
                "answered_by": "X 47 48 - IR",
                "answered_at": "2021-08-13 13:27"
            }
        ]
    },
    {
        "title": ".dockerignore mentioned files are not ignored",
        "url": "https://stackoverflow.com/questions/36295418/dockerignore-mentioned-files-are-not-ignored",
        "votes": "43",
        "views": "33k",
        "author": "Peter Butkovic",
        "issued_at": "2016-03-29 21:28",
        "tags": [
            "docker",
            "dockerfile"
        ],
        "answers": [
            {
                "answer": "The .dockerignore rules follow the filepath/#Match.\nTry (for testing) Gemfile.lock instead of /Gemfile.lock.\nAnd check that the eol (end of line) characters are unix-style, not Windows style in your .dockerignore file.\nApparently, (docker 1.10, March 2016) using rule starting with / like /xxx ( or /.*) is not well supported.",
                "upvotes": 27,
                "answered_by": "VonC",
                "answered_at": "2016-03-30 05:12"
            }
        ]
    },
    {
        "title": "How to avoid question during the Docker build?",
        "url": "https://stackoverflow.com/questions/61388002/how-to-avoid-question-during-the-docker-build",
        "votes": "42",
        "views": "24k",
        "author": "ZedZip",
        "issued_at": "2020-04-23 13:06",
        "tags": [
            "docker",
            "ubuntu",
            "dockerfile"
        ],
        "answers": [
            {
                "answer": "I had the same issue in Dockerfile, then I used ARG DEBIAN_FRONTEND=noninteractive after base image and it works for me:\nExample Dockerfile:\nFROM ubuntu\nARG DEBIAN_FRONTEND=noninteractive",
                "upvotes": 53,
                "answered_by": "Parth Shah",
                "answered_at": "2020-10-01 16:54"
            },
            {
                "answer": "This worked for me.\nENV TZ=Asia/Kolkata \\\n    DEBIAN_FRONTEND=noninteractive\n\nRUN apt-get update && \\\n    apt-get install tzdata",
                "upvotes": 26,
                "answered_by": "devarajbh",
                "answered_at": "2021-07-21 03:57"
            }
        ]
    }
]